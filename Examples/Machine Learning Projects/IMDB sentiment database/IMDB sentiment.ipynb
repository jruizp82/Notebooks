{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RUIZ\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "#For this real world use case we tackle a problem from the field of Natural Language Processing (NLP). \n",
    "#The task is to classify movie reviews into classes expressing positive sentiment about a movie or negative sentiment. \n",
    "#To perform a task like this, the model must be able to understand natural language, that is it must know \n",
    "#the meaning of an entire sentence as expressed by its class prediction. \n",
    "#Recurrent Neural Networks (RNNs) are usually well suited for tasks involving sequential data like sentences however, \n",
    "#we would apply a 1-dimensional Convolutional Neural Network (CNN) model to this task as it is easier to train \n",
    "#and produces comparable results.\n",
    "\n",
    "#The dataset we would use is the IMDB sentiment database which contains 25,000 movie reviews in the training set \n",
    "#and 25,000 reviews in the test set. TFLearn bundles this dataset alongside others so we would access it from the datasets module.\n",
    "\n",
    "#First we import the IMDB sentiment dataset module and other relevant components from TFLearn such as convolutional layers, \n",
    "#fully connected layers, data utilities etc.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tflearn\n",
    "\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "\n",
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "\n",
    "from tflearn.layers.merge_ops import merge\n",
    "\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "\n",
    "from tflearn.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.iro.umontreal.ca/~lisa/deep/data/imdb.pkl\n"
     ]
    }
   ],
   "source": [
    "#The next step is to actually load the dataset into the train and test splits\n",
    "\n",
    "# load IMDB dataset\n",
    "\n",
    "train, test, _ = imdb.load_data(path='imdb.pkl', n_words=10000, valid_portion=0.1)\n",
    "\n",
    "trainX, trainY = train\n",
    "\n",
    "testX, testY = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next phase involves preprocessing the data where we pad sequences which means we set a maximum sentence length \n",
    "#and for sentences less than the maximum sentence length we add zeros to them. The reason is to make sure that all sentences \n",
    "#are of the same length before they are passed to the neural network model. The labels in the train and test sets \n",
    "#are also converted to categorical values.\n",
    "\n",
    "# data preprocessing\n",
    "\n",
    "# sequence padding\n",
    "\n",
    "trainX = pad_sequences(trainX, maxlen=100, value=0.)\n",
    "\n",
    "testX = pad_sequences(testX, maxlen=100, value=0.)\n",
    "\n",
    "# converting labels to binary vectors\n",
    "\n",
    "trainY = to_categorical(trainY, nb_classes=2)\n",
    "\n",
    "testY = to_categorical(testY, nb_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\RUIZ\\Anaconda3\\lib\\site-packages\\tflearn\\initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From C:\\Users\\RUIZ\\Anaconda3\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "#the next step is to describe a 1-dimensional Convolutional NeuralÂ  Network model using the building blocks provided to us by TFLearn.\n",
    "\n",
    "# building the convolutional network\n",
    "\n",
    "#The network contains 3 1-dimensional convolutional layers, a global max pooling layer used to reduce \n",
    "#the dimension of convolutions, a dropout layer used for regularization and a fully connected layer.\n",
    "\n",
    "network = input_data(shape=[None, 100], name='input')\n",
    "\n",
    "network = tflearn.embedding(network, input_dim=10000, output_dim=128)\n",
    "\n",
    "branch1 = conv_1d(network, 128, 3, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "\n",
    "branch2 = conv_1d(network, 128, 4, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "\n",
    "branch3 = conv_1d(network, 128, 5, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "\n",
    "network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "\n",
    "network = tf.expand_dims(network, 2)\n",
    "\n",
    "network = global_max_pool(network)\n",
    "\n",
    "network = dropout(network, 0.5)\n",
    "\n",
    "network = fully_connected(network, 2, activation='softmax')\n",
    "\n",
    "network = regression(network, optimizer='adam', learning_rate=0.001, loss='categorical_crossentropy', name='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3519  | total loss: \u001b[1m\u001b[32m0.09039\u001b[0m\u001b[0m | time: 159.175s\n",
      "| Adam | epoch: 005 | loss: 0.09039 - acc: 0.9656 -- iter: 22496/22500\n",
      "Training Step: 3520  | total loss: \u001b[1m\u001b[32m0.10540\u001b[0m\u001b[0m | time: 163.064s\n",
      "| Adam | epoch: 005 | loss: 0.10540 - acc: 0.9628 | val_loss: 0.56382 - val_acc: 0.7992 -- iter: 22500/22500\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "#Finally, we declare the model and call fit method on it to begin training.\n",
    "\n",
    "# training\n",
    "\n",
    "#The trained model achieves an accuracy of 0.7992 on the test set which is to say it correctly classified \n",
    "#the sentiment expressed in 79% of sentences.\n",
    "\n",
    "model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "\n",
    "model.fit(trainX, trainY, n_epoch=5, shuffle=True, validation_set=(testX, testY), show_metric=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
