{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Supervised Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary depencencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import model_evaluation_utils as meu\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(r'data\\movie_reviews.csv')\n",
    "\n",
    "# take a peek at the data\n",
    "print(dataset.head())\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "\n",
    "# build train and test datasets\n",
    "train_reviews = reviews[:35000]\n",
    "train_sentiments = sentiments[:35000]\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]\n",
    "\n",
    "# normalize datasets\n",
    "norm_train_reviews = tn.normalize_corpus(train_reviews)\n",
    "norm_test_reviews = tn.normalize_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize train & test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will first tokenize these datasets such that each text review is decomposed into its corresponding tokens (workflow Step 2)\n",
    "tokenized_train = [tn.tokenizer.tokenize(text) for text in norm_train_reviews]\n",
    "tokenized_test = [tn.tokenizer.tokenize(text) for text in norm_test_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary Mapping (word to index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For feature engineering (Step 3), we will be creating word embeddings. However, we will create them\n",
    "#ourselves using keras instead of using pre-built ones like word2vec or GloVe.\n",
    "#Word embeddings tend to vectorize text documents into fixed sized vectors such that these vectors try to capture\n",
    "#contextual and semantic information.\n",
    "\n",
    "#For generating embeddings, we will use the Embedding layer from keras, which requires documents\n",
    "#to be represented as tokenized and numeric vectors. We already have tokenized text vectors in our\n",
    "#tokenized_train and tokenized_text variables. However we would need to convert them into numeric\n",
    "#representations. Besides this, we would also need the vectors to be of uniform size even though the\n",
    "#tokenized text reviews will be of variable length due to the difference in number of tokens in each review. For\n",
    "#this, one strategy could be to take the length of the longest review (with maximum number of tokens\\words) \n",
    "#and set it as the vector size, letâ€™s call this max_len. Reviews of shorter length can be padded with a PAD term \n",
    "#in the beginning to increase their length to max_len.\n",
    "\n",
    "#We would need to create a word to index vocabulary mapping for representing each tokenized text \n",
    "#review in a numeric form. Do note you would also need to create a numeric mapping for the padding term\n",
    "#which we shall call PAD_INDEX and assign it the numeric index of 0. For unknown terms, in case they are\n",
    "#encountered later on in the test dataset or newer, previously unseen reviews, we would need to assign it to\n",
    "#some index too. This would be because we will vectorize, engineer features, and build models only on the\n",
    "#training data. Hence, if some new term should come up in the future (which was originally not a part of the\n",
    "#model training), we will consider it as an out of vocabulary (OOV) term and assign it to a constant index (we\n",
    "#will name this term NOT_FOUND_INDEX and assign it the index of vocab_size+1). The following snippet helps\n",
    "#us create this vocabulary from our tokenized_train corpus of training text reviews.\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# build word to index vocabulary\n",
    "token_counter = Counter([token for review in tokenized_train for token in review])\n",
    "vocab_map = {item[0]: index+1 for index, item in enumerate(dict(token_counter).items())}\n",
    "max_index = np.max(list(vocab_map.values()))\n",
    "vocab_map['PAD_INDEX'] = 0\n",
    "vocab_map['NOT_FOUND_INDEX'] = max_index+1\n",
    "vocab_size = len(vocab_map)\n",
    "# view vocabulary size and part of the vocabulary map\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Sample slice of vocabulary map:', dict(list(vocab_map.items())[10:20]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
