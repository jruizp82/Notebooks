{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Sentiment with Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary depencencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s load the necessary dependencies and settings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import model_evaluation_utils as meu\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "#We can now load our IMDb movie reviews dataset, use the first 35,000 reviews for training models and\n",
    "#the remaining 15,000 reviews as the test dataset to evaluate model performance. Besides this, we will also\n",
    "#use our normalization module to normalize our review datasets\n",
    "dataset = pd.read_csv(r'data\\movie_reviews.csv')\n",
    "\n",
    "# take a peek at the data\n",
    "print(dataset.head())\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "\n",
    "# build train and test datasets\n",
    "train_reviews = reviews[:35000]\n",
    "train_sentiments = sentiments[:35000]\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]\n",
    "\n",
    "# normalize datasets\n",
    "norm_train_reviews = tn.normalize_corpus(train_reviews)\n",
    "norm_test_reviews = tn.normalize_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Supervised Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will be using traditional classification models in this section to classify the sentiment of our movie\n",
    "#reviews. Our feature engineering techniques will be based on the Bag of Words model and the TF-IDF model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# build BOW features on train reviews\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(norm_train_reviews)\n",
    "# build TFIDF features on train reviews\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2),\n",
    "                     sublinear_tf=True)\n",
    "tv_train_features = tv.fit_transform(norm_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test reviews into features\n",
    "cv_test_features = cv.transform(norm_test_reviews)\n",
    "tv_test_features = tv.transform(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train features shape: (35000, 2104735)  Test features shape: (15000, 2104735)\n",
      "TFIDF model:> Train features shape: (35000, 2104735)  Test features shape: (15000, 2104735)\n"
     ]
    }
   ],
   "source": [
    "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
    "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training, Prediction and Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We take into account word as well as bi-grams for our feature-sets. We can now use some traditional\n",
    "#supervised Machine Learning algorithms which work very well on text classification. We recommend using\n",
    "#logistic regression, support vector machines, and multinomial Naïve Bayes models when you work on your\n",
    "#own datasets in the future.\n",
    "#we built models using Logistic Regression as well as SVM\n",
    "#The following snippet helps initialize these classification model estimators\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', max_iter=100, C=1)\n",
    "svm = SGDClassifier(loss='hinge', n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.9057\n",
      "Precision: 0.9057\n",
      "Recall: 0.9057\n",
      "F1 Score: 0.9057\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.90      0.91      0.91      7510\n",
      "   negative       0.91      0.90      0.91      7490\n",
      "\n",
      "avg / total       0.91      0.91      0.91     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6823      687\n",
      "        negative        727     6763\n"
     ]
    }
   ],
   "source": [
    "#We will now use our utility function train_predict_model(...) from our model_evaluation_utils\n",
    "#module to build a logistic regression model on our training features and evaluate the model performance on\n",
    "#our test features\n",
    "\n",
    "# Logistic Regression model on BOW features\n",
    "lr_bow_predictions = meu.train_predict_model(classifier=lr, \n",
    "                                             train_features=cv_train_features, train_labels=train_sentiments,\n",
    "                                             test_features=cv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_bow_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.895\n",
      "Precision: 0.8951\n",
      "Recall: 0.895\n",
      "F1 Score: 0.895\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.89      0.90      0.90      7510\n",
      "   negative       0.90      0.89      0.89      7490\n",
      "\n",
      "avg / total       0.90      0.90      0.89     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6782      728\n",
      "        negative        847     6643\n"
     ]
    }
   ],
   "source": [
    "#We get an overall F1-Score and model accuracy of 91%, which is really\n",
    "#excellent! We can now build a logistic regression model similarly on our TF-IDF features using the following snippet.\n",
    "\n",
    "# Logistic Regression model on TF-IDF features\n",
    "lr_tfidf_predictions = meu.train_predict_model(classifier=lr, \n",
    "                                               train_features=tv_train_features, train_labels=train_sentiments,\n",
    "                                               test_features=tv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_tfidf_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RUIZ\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.9045\n",
      "Precision: 0.9045\n",
      "Recall: 0.9045\n",
      "F1 Score: 0.9045\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.91      0.90      0.90      7510\n",
      "   negative       0.90      0.91      0.90      7490\n",
      "\n",
      "avg / total       0.90      0.90      0.90     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6760      750\n",
      "        negative        683     6807\n"
     ]
    }
   ],
   "source": [
    "#We get an overall F1-Score and model accuracy of 90%, which is great\n",
    "#but our previous model is still slightly better. You can similarly use the Support Vector Machine model\n",
    "#estimator object svm, which we created earlier, and use the same snippet to train and predict using an SVM model\n",
    "\n",
    "svm_bow_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                             train_features=cv_train_features, train_labels=train_sentiments,\n",
    "                                             test_features=cv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_bow_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RUIZ\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8971\n",
      "Precision: 0.8974\n",
      "Recall: 0.8971\n",
      "F1 Score: 0.897\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.89      0.91      0.90      7510\n",
      "   negative       0.91      0.88      0.90      7490\n",
      "\n",
      "avg / total       0.90      0.90      0.90     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6841      669\n",
      "        negative        875     6615\n"
     ]
    }
   ],
   "source": [
    "svm_tfidf_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                                train_features=tv_train_features, train_labels=train_sentiments,\n",
    "                                                test_features=tv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_tfidf_predictions,\n",
    "                                      classes=['positive', 'negative'])\n",
    "\n",
    "#We obtained a maximum accuracy and F1-score of 90% with the SVM model\n",
    "#Thus you can see how effective and accurate these supervised\n",
    "#Machine Learning classification algorithms are in building a text sentiment classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newer Supervised Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RUIZ\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\RUIZ\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#we will be building some deep neural networks and train them on some advanced text features based on word embeddings\n",
    "#to build a text sentiment classification system similar to what we did in the previous section. Let’s load the\n",
    "#following necessary dependencies before we start our analysis.\n",
    "import gensim\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Activation, Dense\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction class label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So far, our models in scikit-learn directly accepted the sentiment class labels as\n",
    "#positive and negative and internally performed these operations. However for our Deep Learning models,\n",
    "#we need to do this explicitly. The following snippet helps us tokenize our movie reviews and also converts\n",
    "#the text-based sentiment class labels into one-hot encoded vectors\n",
    "le = LabelEncoder()\n",
    "num_classes=2 \n",
    "# tokenize train reviews & encode train labels\n",
    "tokenized_train = [tn.tokenizer.tokenize(text)\n",
    "                   for text in norm_train_reviews]\n",
    "y_tr = le.fit_transform(train_sentiments)\n",
    "y_train = keras.utils.to_categorical(y_tr, num_classes)\n",
    "# tokenize test reviews & encode test labels\n",
    "tokenized_test = [tn.tokenizer.tokenize(text)\n",
    "                   for text in norm_test_reviews]\n",
    "y_ts = le.fit_transform(test_sentiments)\n",
    "y_test = keras.utils.to_categorical(y_ts, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment class label map: {'negative': 0, 'positive': 1}\n",
      "Sample test label transformation:\n",
      "----------------------------------- \n",
      "Actual Labels: ['negative' 'positive' 'negative'] \n",
      "Encoded Labels: [0 1 0] \n",
      "One hot encoded Labels:\n",
      " [[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# print class label encoding map and encoded labels\n",
    "print('Sentiment class label map:', dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "print('Sample test label transformation:\\n'+'-'*35,\n",
    "      '\\nActual Labels:', test_sentiments[:3], '\\nEncoded Labels:', y_ts[:3], \n",
    "      '\\nOne hot encoded Labels:\\n', y_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thus, we can see from the preceding sample outputs how our sentiment class labels have been encoded\n",
    "#into numeric representations, which in turn have been converted into one-hot encoded vectors. The\n",
    "#feature engineering techniques we will be using in this section are slightly more advanced word\n",
    "#vectorization techniques that are based on the concept of word embeddings. We will be using the word2vec\n",
    "#and GloVe models to generate embeddings. We will be choosing the size parameter to\n",
    "#be 500 in this scenario representing feature vector size to be 500 for each word\n",
    "# build word2vec model\n",
    "w2v_num_features = 500\n",
    "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=150,\n",
    "                                   min_count=10, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will be using the document word vector averaging scheme on this model to\n",
    "#represent each movie review as an averaged vector of all the word vector representations for the different\n",
    "#words in the review. The following function helps us compute averaged word vector representations for any\n",
    "#corpus of text documents.\n",
    "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model[word])\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "        return feature_vector\n",
    "\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RUIZ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "#We can now use the previous function to generate averaged word vector representations on our two\n",
    "#movie review datasets.\n",
    "# generate averaged word vector features from word2vec model\n",
    "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
    "                                                     num_features=500)\n",
    "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
    "                                                    num_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-2ddfc841d658>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#embeddings for our two datasets.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# feature engineering with GloVe model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_nlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnorm_train_reviews\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtrain_glove_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_nlp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-2ddfc841d658>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#embeddings for our two datasets.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# feature engineering with GloVe model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_nlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnorm_train_reviews\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtrain_glove_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_nlp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable)\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__call__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.get_batch_model\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(seqs_in, drop)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         X, bp_layer = layer.begin_update(layer.ops.flatten(seqs_in, pad=pad),\n\u001b[1;32m--> 280\u001b[1;33m                                          drop=drop)\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\resnet.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbp_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mresidual_bwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\layernorm.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackprop_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_moments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mXhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\layernorm.py\u001b[0m in \u001b[0;36m_get_moments\u001b[1;34m(ops, X)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_get_moments_reproduce_bug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mmu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-08\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'f'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_var\u001b[1;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;31m# Note that x may not be inexact and that we need it to be an array,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;31m# not a scalar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0marrmean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomplexfloating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#The GloVe model, which stands for Global Vectors, is an unsupervised model for obtaining word vector\n",
    "#representations.The spacy library provided 300-dimensional word vectors trained on the Common Crawl corpus using the GloVe\n",
    "#model. They provide a simple standard interface to get feature vectors of size 300 for each word as well as the\n",
    "#averaged feature vector of a complete text document. The following snippet leverages spacy to get the GloVe\n",
    "#embeddings for our two datasets.\n",
    "# feature engineering with GloVe model\n",
    "train_nlp = [tn.nlp(item) for item in norm_train_reviews]\n",
    "train_glove_features = np.array([item.vector for item in train_nlp])\n",
    "\n",
    "test_nlp = [tn.nlp(item) for item in norm_test_reviews]\n",
    "test_glove_features = np.array([item.vector for item in test_nlp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can check the feature vector dimensions for our datasets based on each of the previous models\n",
    "#using the following code.\n",
    "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)\n",
    "print('GloVe model:> Train features shape:', train_glove_features.shape, ' Test features shape:', test_glove_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Deep neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see from the preceding output that as expected the word2vec model features are of size 500\n",
    "# and the GloVe features are of size 300. We can now proceed to Step 4 of our classification system workflow\n",
    "#where we will build and train a deep neural network on these features.\n",
    "#We will be using a fully-connected four layer deep neural network (multi-layer perceptron or deep\n",
    "#ANN) for our model. We do not count the input layer usually in any deep architecture, hence our model will\n",
    "#consist of three hidden layers of 512 neurons or units and one output layer with two units that will be used to\n",
    "#either predict a positive or negative sentiment based on the input layer features.\n",
    "\n",
    "#We call this a fully connected deep neural network (DNN) because neurons or units in each pair\n",
    "#of adjacent layers are fully pairwise connected. These networks are also known as deep artificial neural\n",
    "#networks (ANNs) or Multi-Layer Perceptrons (MLPs) since they have more than one hidden layer. The\n",
    "#following function leverages keras on top of tensorflow to build the desired DNN model.\n",
    "\n",
    "def construct_deepnn_architecture(num_input_features):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(512, activation='relu', input_shape=(num_input_features,)))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(512, activation='relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(512, activation='relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(2))\n",
    "    dnn_model.add(Activation('softmax'))\n",
    "\n",
    "    dnn_model.compile(loss='categorical_crossentropy', optimizer='adam',                 \n",
    "                      metrics=['accuracy'])\n",
    "    return dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the preceding function, you can see that we accept a parameter num_input_features, which\n",
    "#decides the number of units needed in the input layer (500 for word2vec and 300 for glove features). We\n",
    "#build a Sequential model, which helps us linearly stack our hidden and output layers.\n",
    "\n",
    "#Let’s now build a DNN model based on our word2vec input feature representations for our training reviews\n",
    "w2v_dnn = construct_deepnn_architecture(num_input_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"709pt\" viewBox=\"0.00 0.00 211.00 709.00\" width=\"211pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 705)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-705 207,-705 207,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1412901154208 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1412901154208</title>\n",
       "<polygon fill=\"none\" points=\"6.5,-581.5 6.5,-627.5 196.5,-627.5 196.5,-581.5 6.5,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"32\" y=\"-600.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"57.5,-581.5 57.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"57.5,-604.5 113.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-581.5 113.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155\" y=\"-612.3\">(None, 500)</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-604.5 196.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155\" y=\"-589.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 1412901124248 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1412901124248</title>\n",
       "<polygon fill=\"none\" points=\"0,-498.5 0,-544.5 203,-544.5 203,-498.5 0,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"32\" y=\"-517.8\">Dropout</text>\n",
       "<polyline fill=\"none\" points=\"64,-498.5 64,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"64,-521.5 120,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"120,-498.5 120,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"161.5\" y=\"-529.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"120,-521.5 203,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"161.5\" y=\"-506.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 1412901154208&#45;&gt;1412901124248 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1412901154208-&gt;1412901124248</title>\n",
       "<path d=\"M101.5,-581.366C101.5,-573.152 101.5,-563.658 101.5,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105,-554.607 101.5,-544.607 98.0001,-554.607 105,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1412901151128 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1412901151128</title>\n",
       "<polygon fill=\"none\" points=\"6.5,-415.5 6.5,-461.5 196.5,-461.5 196.5,-415.5 6.5,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"32\" y=\"-434.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"57.5,-415.5 57.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"57.5,-438.5 113.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-415.5 113.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155\" y=\"-446.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-438.5 196.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155\" y=\"-423.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 1412901124248&#45;&gt;1412901151128 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>1412901124248-&gt;1412901151128</title>\n",
       "<path d=\"M101.5,-498.366C101.5,-490.152 101.5,-480.658 101.5,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105,-471.607 101.5,-461.607 98.0001,-471.607 105,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1412901126040 -->\n",
       "<g class=\"node\" id=\"node4\"><title>1412901126040</title>\n",
       "<polygon fill=\"none\" points=\"0,-332.5 0,-378.5 203,-378.5 203,-332.5 0,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"32\" y=\"-351.8\">Dropout</text>\n",
       "<polyline fill=\"none\" points=\"64,-332.5 64,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"64,-355.5 120,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"120,-332.5 120,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"161.5\" y=\"-363.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"120,-355.5 203,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"161.5\" y=\"-340.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 1412901151128&#45;&gt;1412901126040 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>1412901151128-&gt;1412901126040</title>\n",
       "<path d=\"M101.5,-415.366C101.5,-407.152 101.5,-397.658 101.5,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105,-388.607 101.5,-378.607 98.0001,-388.607 105,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1424176011024 -->\n",
       "<g class=\"node\" id=\"node5\"><title>1424176011024</title>\n",
       "<polygon fill=\"none\" points=\"6.5,-249.5 6.5,-295.5 196.5,-295.5 196.5,-249.5 6.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"32\" y=\"-268.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"57.5,-249.5 57.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"57.5,-272.5 113.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-249.5 113.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155\" y=\"-280.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-272.5 196.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155\" y=\"-257.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 1412901126040&#45;&gt;1424176011024 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>1412901126040-&gt;1424176011024</title>\n",
       "<path d=\"M101.5,-332.366C101.5,-324.152 101.5,-314.658 101.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105,-305.607 101.5,-295.607 98.0001,-305.607 105,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1424176422080 -->\n",
       "<g class=\"node\" id=\"node6\"><title>1424176422080</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 203,-212.5 203,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"32\" y=\"-185.8\">Dropout</text>\n",
       "<polyline fill=\"none\" points=\"64,-166.5 64,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"64,-189.5 120,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"120,-166.5 120,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"161.5\" y=\"-197.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"120,-189.5 203,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"161.5\" y=\"-174.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 1424176011024&#45;&gt;1424176422080 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>1424176011024-&gt;1424176422080</title>\n",
       "<path d=\"M101.5,-249.366C101.5,-241.152 101.5,-231.658 101.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105,-222.607 101.5,-212.607 98.0001,-222.607 105,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1424176419896 -->\n",
       "<g class=\"node\" id=\"node7\"><title>1424176419896</title>\n",
       "<polygon fill=\"none\" points=\"6.5,-83.5 6.5,-129.5 196.5,-129.5 196.5,-83.5 6.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"32\" y=\"-102.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"57.5,-83.5 57.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"57.5,-106.5 113.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-83.5 113.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155\" y=\"-114.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-106.5 196.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155\" y=\"-91.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 1424176422080&#45;&gt;1424176419896 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>1424176422080-&gt;1424176419896</title>\n",
       "<path d=\"M101.5,-166.366C101.5,-158.152 101.5,-148.658 101.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105,-139.607 101.5,-129.607 98.0001,-139.607 105,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1424176885600 -->\n",
       "<g class=\"node\" id=\"node8\"><title>1424176885600</title>\n",
       "<polygon fill=\"none\" points=\"1.5,-0.5 1.5,-46.5 201.5,-46.5 201.5,-0.5 1.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"38.5\" y=\"-19.8\">Activation</text>\n",
       "<polyline fill=\"none\" points=\"75.5,-0.5 75.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"75.5,-23.5 131.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"131.5,-0.5 131.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166.5\" y=\"-31.3\">(None, 2)</text>\n",
       "<polyline fill=\"none\" points=\"131.5,-23.5 201.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166.5\" y=\"-8.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 1424176419896&#45;&gt;1424176885600 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>1424176419896-&gt;1424176885600</title>\n",
       "<path d=\"M101.5,-83.3664C101.5,-75.1516 101.5,-65.6579 101.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105,-56.6068 101.5,-46.6068 98.0001,-56.6069 105,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1412901150904 -->\n",
       "<g class=\"node\" id=\"node9\"><title>1412901150904</title>\n",
       "<polygon fill=\"none\" points=\"49.5,-664.5 49.5,-700.5 153.5,-700.5 153.5,-664.5 49.5,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.5\" y=\"-678.8\">1412901150904</text>\n",
       "</g>\n",
       "<!-- 1412901150904&#45;&gt;1412901154208 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1412901150904-&gt;1412901154208</title>\n",
       "<path d=\"M101.5,-664.254C101.5,-656.363 101.5,-646.749 101.5,-637.602\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105,-637.591 101.5,-627.591 98.0001,-637.591 105,-637.591\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can also visualize the DNN model architecture with the help of keras\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(w2v_dnn, show_shapes=True, show_layer_names=False, \n",
    "                 rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training, Prediction and Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31500 samples, validate on 3500 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21300/31500 [===================>..........] - ETA: 3:19 - loss: 0.2356 - acc: 0.890 - ETA: 1:52 - loss: 0.2048 - acc: 0.905 - ETA: 1:24 - loss: 0.1970 - acc: 0.903 - ETA: 1:09 - loss: 0.2127 - acc: 0.897 - ETA: 1:00 - loss: 0.2083 - acc: 0.904 - ETA: 54s - loss: 0.2174 - acc: 0.905 - ETA: 49s - loss: 0.2189 - acc: 0.90 - ETA: 46s - loss: 0.2253 - acc: 0.90 - ETA: 43s - loss: 0.2237 - acc: 0.90 - ETA: 41s - loss: 0.2266 - acc: 0.90 - ETA: 39s - loss: 0.2272 - acc: 0.90 - ETA: 37s - loss: 0.2391 - acc: 0.89 - ETA: 36s - loss: 0.2418 - acc: 0.89 - ETA: 35s - loss: 0.2394 - acc: 0.89 - ETA: 34s - loss: 0.2409 - acc: 0.89 - ETA: 33s - loss: 0.2423 - acc: 0.89 - ETA: 32s - loss: 0.2444 - acc: 0.89 - ETA: 32s - loss: 0.2428 - acc: 0.89 - ETA: 31s - loss: 0.2397 - acc: 0.89 - ETA: 31s - loss: 0.2418 - acc: 0.89 - ETA: 30s - loss: 0.2410 - acc: 0.89 - ETA: 30s - loss: 0.2396 - acc: 0.89 - ETA: 29s - loss: 0.2370 - acc: 0.89 - ETA: 29s - loss: 0.2349 - acc: 0.89 - ETA: 28s - loss: 0.2344 - acc: 0.89 - ETA: 28s - loss: 0.2335 - acc: 0.89 - ETA: 27s - loss: 0.2305 - acc: 0.89 - ETA: 27s - loss: 0.2316 - acc: 0.89 - ETA: 27s - loss: 0.2326 - acc: 0.89 - ETA: 27s - loss: 0.2328 - acc: 0.89 - ETA: 26s - loss: 0.2313 - acc: 0.89 - ETA: 26s - loss: 0.2326 - acc: 0.89 - ETA: 26s - loss: 0.2377 - acc: 0.89 - ETA: 25s - loss: 0.2379 - acc: 0.89 - ETA: 25s - loss: 0.2365 - acc: 0.89 - ETA: 25s - loss: 0.2366 - acc: 0.89 - ETA: 25s - loss: 0.2363 - acc: 0.89 - ETA: 25s - loss: 0.2379 - acc: 0.89 - ETA: 24s - loss: 0.2391 - acc: 0.89 - ETA: 24s - loss: 0.2390 - acc: 0.89 - ETA: 24s - loss: 0.2380 - acc: 0.89 - ETA: 24s - loss: 0.2371 - acc: 0.89 - ETA: 24s - loss: 0.2378 - acc: 0.89 - ETA: 23s - loss: 0.2376 - acc: 0.89 - ETA: 23s - loss: 0.2357 - acc: 0.89 - ETA: 23s - loss: 0.2352 - acc: 0.89 - ETA: 23s - loss: 0.2341 - acc: 0.90 - ETA: 23s - loss: 0.2332 - acc: 0.90 - ETA: 23s - loss: 0.2332 - acc: 0.90 - ETA: 22s - loss: 0.2336 - acc: 0.90 - ETA: 22s - loss: 0.2326 - acc: 0.90 - ETA: 22s - loss: 0.2334 - acc: 0.90 - ETA: 22s - loss: 0.2357 - acc: 0.90 - ETA: 22s - loss: 0.2344 - acc: 0.90 - ETA: 22s - loss: 0.2335 - acc: 0.90 - ETA: 22s - loss: 0.2332 - acc: 0.90 - ETA: 21s - loss: 0.2329 - acc: 0.90 - ETA: 21s - loss: 0.2340 - acc: 0.90 - ETA: 21s - loss: 0.2342 - acc: 0.90 - ETA: 21s - loss: 0.2343 - acc: 0.90 - ETA: 21s - loss: 0.2343 - acc: 0.90 - ETA: 21s - loss: 0.2338 - acc: 0.90 - ETA: 21s - loss: 0.2351 - acc: 0.90 - ETA: 21s - loss: 0.2365 - acc: 0.90 - ETA: 21s - loss: 0.2359 - acc: 0.90 - ETA: 20s - loss: 0.2370 - acc: 0.90 - ETA: 20s - loss: 0.2369 - acc: 0.90 - ETA: 20s - loss: 0.2374 - acc: 0.90 - ETA: 20s - loss: 0.2391 - acc: 0.90 - ETA: 20s - loss: 0.2390 - acc: 0.90 - ETA: 20s - loss: 0.2387 - acc: 0.90 - ETA: 20s - loss: 0.2376 - acc: 0.90 - ETA: 20s - loss: 0.2376 - acc: 0.90 - ETA: 19s - loss: 0.2380 - acc: 0.90 - ETA: 19s - loss: 0.2376 - acc: 0.90 - ETA: 19s - loss: 0.2370 - acc: 0.90 - ETA: 19s - loss: 0.2370 - acc: 0.90 - ETA: 19s - loss: 0.2378 - acc: 0.90 - ETA: 19s - loss: 0.2389 - acc: 0.90 - ETA: 19s - loss: 0.2387 - acc: 0.90 - ETA: 19s - loss: 0.2388 - acc: 0.89 - ETA: 19s - loss: 0.2386 - acc: 0.89 - ETA: 19s - loss: 0.2386 - acc: 0.89 - ETA: 18s - loss: 0.2385 - acc: 0.89 - ETA: 18s - loss: 0.2388 - acc: 0.89 - ETA: 18s - loss: 0.2384 - acc: 0.89 - ETA: 18s - loss: 0.2386 - acc: 0.89 - ETA: 18s - loss: 0.2393 - acc: 0.89 - ETA: 18s - loss: 0.2402 - acc: 0.89 - ETA: 18s - loss: 0.2392 - acc: 0.89 - ETA: 18s - loss: 0.2391 - acc: 0.89 - ETA: 18s - loss: 0.2399 - acc: 0.89 - ETA: 18s - loss: 0.2399 - acc: 0.89 - ETA: 17s - loss: 0.2399 - acc: 0.89 - ETA: 17s - loss: 0.2405 - acc: 0.89 - ETA: 17s - loss: 0.2412 - acc: 0.89 - ETA: 17s - loss: 0.2417 - acc: 0.89 - ETA: 17s - loss: 0.2421 - acc: 0.89 - ETA: 17s - loss: 0.2420 - acc: 0.89 - ETA: 17s - loss: 0.2417 - acc: 0.89 - ETA: 17s - loss: 0.2411 - acc: 0.89 - ETA: 17s - loss: 0.2417 - acc: 0.89 - ETA: 17s - loss: 0.2432 - acc: 0.89 - ETA: 17s - loss: 0.2428 - acc: 0.89 - ETA: 16s - loss: 0.2436 - acc: 0.89 - ETA: 16s - loss: 0.2439 - acc: 0.89 - ETA: 16s - loss: 0.2433 - acc: 0.89 - ETA: 16s - loss: 0.2434 - acc: 0.89 - ETA: 16s - loss: 0.2435 - acc: 0.89 - ETA: 16s - loss: 0.2432 - acc: 0.89 - ETA: 16s - loss: 0.2436 - acc: 0.89 - ETA: 16s - loss: 0.2426 - acc: 0.89 - ETA: 16s - loss: 0.2430 - acc: 0.89 - ETA: 16s - loss: 0.2436 - acc: 0.89 - ETA: 16s - loss: 0.2438 - acc: 0.89 - ETA: 15s - loss: 0.2436 - acc: 0.89 - ETA: 15s - loss: 0.2449 - acc: 0.89 - ETA: 15s - loss: 0.2447 - acc: 0.89 - ETA: 15s - loss: 0.2446 - acc: 0.89 - ETA: 15s - loss: 0.2441 - acc: 0.89 - ETA: 15s - loss: 0.2445 - acc: 0.89 - ETA: 15s - loss: 0.2440 - acc: 0.89 - ETA: 15s - loss: 0.2441 - acc: 0.89 - ETA: 15s - loss: 0.2436 - acc: 0.89 - ETA: 15s - loss: 0.2431 - acc: 0.89 - ETA: 15s - loss: 0.2428 - acc: 0.89 - ETA: 14s - loss: 0.2428 - acc: 0.89 - ETA: 14s - loss: 0.2429 - acc: 0.89 - ETA: 14s - loss: 0.2426 - acc: 0.89 - ETA: 14s - loss: 0.2429 - acc: 0.89 - ETA: 14s - loss: 0.2432 - acc: 0.89 - ETA: 14s - loss: 0.2433 - acc: 0.89 - ETA: 14s - loss: 0.2435 - acc: 0.89 - ETA: 14s - loss: 0.2436 - acc: 0.89 - ETA: 14s - loss: 0.2434 - acc: 0.89 - ETA: 14s - loss: 0.2438 - acc: 0.89 - ETA: 14s - loss: 0.2438 - acc: 0.89 - ETA: 14s - loss: 0.2432 - acc: 0.89 - ETA: 13s - loss: 0.2430 - acc: 0.89 - ETA: 13s - loss: 0.2428 - acc: 0.89 - ETA: 13s - loss: 0.2429 - acc: 0.89 - ETA: 13s - loss: 0.2430 - acc: 0.89 - ETA: 13s - loss: 0.2435 - acc: 0.89 - ETA: 13s - loss: 0.2431 - acc: 0.89 - ETA: 13s - loss: 0.2431 - acc: 0.89 - ETA: 13s - loss: 0.2432 - acc: 0.89 - ETA: 13s - loss: 0.2429 - acc: 0.89 - ETA: 13s - loss: 0.2422 - acc: 0.89 - ETA: 13s - loss: 0.2427 - acc: 0.89 - ETA: 13s - loss: 0.2432 - acc: 0.89 - ETA: 12s - loss: 0.2433 - acc: 0.89 - ETA: 12s - loss: 0.2440 - acc: 0.89 - ETA: 12s - loss: 0.2444 - acc: 0.89 - ETA: 12s - loss: 0.2441 - acc: 0.89 - ETA: 12s - loss: 0.2441 - acc: 0.89 - ETA: 12s - loss: 0.2440 - acc: 0.89 - ETA: 12s - loss: 0.2455 - acc: 0.89 - ETA: 12s - loss: 0.2458 - acc: 0.89 - ETA: 12s - loss: 0.2462 - acc: 0.89 - ETA: 12s - loss: 0.2461 - acc: 0.89 - ETA: 12s - loss: 0.2458 - acc: 0.89 - ETA: 12s - loss: 0.2461 - acc: 0.89 - ETA: 12s - loss: 0.2463 - acc: 0.89 - ETA: 11s - loss: 0.2465 - acc: 0.89 - ETA: 11s - loss: 0.2464 - acc: 0.89 - ETA: 11s - loss: 0.2466 - acc: 0.89 - ETA: 11s - loss: 0.2468 - acc: 0.89 - ETA: 11s - loss: 0.2469 - acc: 0.89 - ETA: 11s - loss: 0.2466 - acc: 0.89 - ETA: 11s - loss: 0.2459 - acc: 0.89 - ETA: 11s - loss: 0.2462 - acc: 0.89 - ETA: 11s - loss: 0.2464 - acc: 0.89 - ETA: 11s - loss: 0.2465 - acc: 0.89 - ETA: 11s - loss: 0.2470 - acc: 0.89 - ETA: 11s - loss: 0.2466 - acc: 0.89 - ETA: 11s - loss: 0.2465 - acc: 0.89 - ETA: 11s - loss: 0.2465 - acc: 0.89 - ETA: 10s - loss: 0.2464 - acc: 0.89 - ETA: 10s - loss: 0.2466 - acc: 0.89 - ETA: 10s - loss: 0.2463 - acc: 0.89 - ETA: 10s - loss: 0.2458 - acc: 0.89 - ETA: 10s - loss: 0.2462 - acc: 0.89 - ETA: 10s - loss: 0.2460 - acc: 0.89 - ETA: 10s - loss: 0.2463 - acc: 0.89 - ETA: 10s - loss: 0.2462 - acc: 0.89 - ETA: 10s - loss: 0.2461 - acc: 0.89 - ETA: 10s - loss: 0.2464 - acc: 0.89 - ETA: 10s - loss: 0.2462 - acc: 0.89 - ETA: 10s - loss: 0.2463 - acc: 0.89 - ETA: 9s - loss: 0.2468 - acc: 0.8954 - ETA: 9s - loss: 0.2469 - acc: 0.895 - ETA: 9s - loss: 0.2464 - acc: 0.895 - ETA: 9s - loss: 0.2461 - acc: 0.895 - ETA: 9s - loss: 0.2467 - acc: 0.895 - ETA: 9s - loss: 0.2465 - acc: 0.896 - ETA: 9s - loss: 0.2461 - acc: 0.896 - ETA: 9s - loss: 0.2465 - acc: 0.896 - ETA: 9s - loss: 0.2465 - acc: 0.896 - ETA: 9s - loss: 0.2469 - acc: 0.896 - ETA: 9s - loss: 0.2466 - acc: 0.896 - ETA: 9s - loss: 0.2469 - acc: 0.896 - ETA: 9s - loss: 0.2468 - acc: 0.896 - ETA: 8s - loss: 0.2472 - acc: 0.896 - ETA: 8s - loss: 0.2471 - acc: 0.896 - ETA: 8s - loss: 0.2471 - acc: 0.896 - ETA: 8s - loss: 0.2476 - acc: 0.896 - ETA: 8s - loss: 0.2475 - acc: 0.896 - ETA: 8s - loss: 0.2472 - acc: 0.896 - ETA: 8s - loss: 0.2470 - acc: 0.896 - ETA: 8s - loss: 0.2467 - acc: 0.896 - ETA: 8s - loss: 0.2467 - acc: 0.896 - ETA: 8s - loss: 0.2464 - acc: 0.896 - ETA: 8s - loss: 0.2462 - acc: 0.8967\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31500/31500 [==============================] - ETA: 8s - loss: 0.2464 - acc: 0.896 - ETA: 8s - loss: 0.2465 - acc: 0.896 - ETA: 7s - loss: 0.2461 - acc: 0.896 - ETA: 7s - loss: 0.2461 - acc: 0.896 - ETA: 7s - loss: 0.2461 - acc: 0.896 - ETA: 7s - loss: 0.2461 - acc: 0.896 - ETA: 7s - loss: 0.2461 - acc: 0.896 - ETA: 7s - loss: 0.2463 - acc: 0.896 - ETA: 7s - loss: 0.2460 - acc: 0.896 - ETA: 7s - loss: 0.2460 - acc: 0.896 - ETA: 7s - loss: 0.2465 - acc: 0.896 - ETA: 7s - loss: 0.2467 - acc: 0.896 - ETA: 7s - loss: 0.2463 - acc: 0.896 - ETA: 7s - loss: 0.2464 - acc: 0.896 - ETA: 6s - loss: 0.2462 - acc: 0.896 - ETA: 6s - loss: 0.2461 - acc: 0.896 - ETA: 6s - loss: 0.2467 - acc: 0.896 - ETA: 6s - loss: 0.2466 - acc: 0.896 - ETA: 6s - loss: 0.2468 - acc: 0.896 - ETA: 6s - loss: 0.2471 - acc: 0.896 - ETA: 6s - loss: 0.2472 - acc: 0.895 - ETA: 6s - loss: 0.2469 - acc: 0.896 - ETA: 6s - loss: 0.2466 - acc: 0.896 - ETA: 6s - loss: 0.2465 - acc: 0.896 - ETA: 6s - loss: 0.2465 - acc: 0.896 - ETA: 6s - loss: 0.2465 - acc: 0.896 - ETA: 6s - loss: 0.2468 - acc: 0.896 - ETA: 5s - loss: 0.2469 - acc: 0.896 - ETA: 5s - loss: 0.2473 - acc: 0.896 - ETA: 5s - loss: 0.2479 - acc: 0.896 - ETA: 5s - loss: 0.2476 - acc: 0.896 - ETA: 5s - loss: 0.2477 - acc: 0.896 - ETA: 5s - loss: 0.2479 - acc: 0.896 - ETA: 5s - loss: 0.2484 - acc: 0.896 - ETA: 5s - loss: 0.2487 - acc: 0.895 - ETA: 5s - loss: 0.2483 - acc: 0.896 - ETA: 5s - loss: 0.2483 - acc: 0.896 - ETA: 5s - loss: 0.2490 - acc: 0.896 - ETA: 5s - loss: 0.2491 - acc: 0.896 - ETA: 4s - loss: 0.2489 - acc: 0.896 - ETA: 4s - loss: 0.2488 - acc: 0.896 - ETA: 4s - loss: 0.2485 - acc: 0.896 - ETA: 4s - loss: 0.2484 - acc: 0.896 - ETA: 4s - loss: 0.2482 - acc: 0.896 - ETA: 4s - loss: 0.2480 - acc: 0.896 - ETA: 4s - loss: 0.2479 - acc: 0.896 - ETA: 4s - loss: 0.2476 - acc: 0.896 - ETA: 4s - loss: 0.2476 - acc: 0.896 - ETA: 4s - loss: 0.2476 - acc: 0.896 - ETA: 4s - loss: 0.2477 - acc: 0.896 - ETA: 4s - loss: 0.2479 - acc: 0.896 - ETA: 3s - loss: 0.2485 - acc: 0.896 - ETA: 3s - loss: 0.2489 - acc: 0.896 - ETA: 3s - loss: 0.2494 - acc: 0.896 - ETA: 3s - loss: 0.2493 - acc: 0.896 - ETA: 3s - loss: 0.2495 - acc: 0.895 - ETA: 3s - loss: 0.2498 - acc: 0.895 - ETA: 3s - loss: 0.2497 - acc: 0.895 - ETA: 3s - loss: 0.2495 - acc: 0.896 - ETA: 3s - loss: 0.2498 - acc: 0.895 - ETA: 3s - loss: 0.2499 - acc: 0.895 - ETA: 3s - loss: 0.2501 - acc: 0.895 - ETA: 3s - loss: 0.2499 - acc: 0.895 - ETA: 3s - loss: 0.2497 - acc: 0.896 - ETA: 2s - loss: 0.2499 - acc: 0.895 - ETA: 2s - loss: 0.2497 - acc: 0.895 - ETA: 2s - loss: 0.2499 - acc: 0.895 - ETA: 2s - loss: 0.2501 - acc: 0.895 - ETA: 2s - loss: 0.2499 - acc: 0.895 - ETA: 2s - loss: 0.2502 - acc: 0.895 - ETA: 2s - loss: 0.2505 - acc: 0.895 - ETA: 2s - loss: 0.2507 - acc: 0.895 - ETA: 2s - loss: 0.2508 - acc: 0.895 - ETA: 2s - loss: 0.2505 - acc: 0.895 - ETA: 2s - loss: 0.2507 - acc: 0.895 - ETA: 2s - loss: 0.2506 - acc: 0.895 - ETA: 2s - loss: 0.2506 - acc: 0.895 - ETA: 1s - loss: 0.2506 - acc: 0.895 - ETA: 1s - loss: 0.2505 - acc: 0.895 - ETA: 1s - loss: 0.2505 - acc: 0.895 - ETA: 1s - loss: 0.2508 - acc: 0.895 - ETA: 1s - loss: 0.2512 - acc: 0.895 - ETA: 1s - loss: 0.2511 - acc: 0.895 - ETA: 1s - loss: 0.2512 - acc: 0.895 - ETA: 1s - loss: 0.2516 - acc: 0.894 - ETA: 1s - loss: 0.2518 - acc: 0.894 - ETA: 1s - loss: 0.2520 - acc: 0.894 - ETA: 1s - loss: 0.2520 - acc: 0.894 - ETA: 1s - loss: 0.2519 - acc: 0.894 - ETA: 0s - loss: 0.2521 - acc: 0.894 - ETA: 0s - loss: 0.2523 - acc: 0.894 - ETA: 0s - loss: 0.2524 - acc: 0.894 - ETA: 0s - loss: 0.2525 - acc: 0.894 - ETA: 0s - loss: 0.2530 - acc: 0.894 - ETA: 0s - loss: 0.2529 - acc: 0.894 - ETA: 0s - loss: 0.2528 - acc: 0.894 - ETA: 0s - loss: 0.2529 - acc: 0.894 - ETA: 0s - loss: 0.2529 - acc: 0.894 - ETA: 0s - loss: 0.2531 - acc: 0.894 - ETA: 0s - loss: 0.2532 - acc: 0.894 - ETA: 0s - loss: 0.2532 - acc: 0.893 - 26s 818us/step - loss: 0.2532 - acc: 0.8939 - val_loss: 0.3037 - val_acc: 0.8740\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21500/31500 [===================>..........] - ETA: 20s - loss: 0.2035 - acc: 0.92 - ETA: 24s - loss: 0.1776 - acc: 0.93 - ETA: 25s - loss: 0.1823 - acc: 0.93 - ETA: 26s - loss: 0.2028 - acc: 0.92 - ETA: 25s - loss: 0.2139 - acc: 0.91 - ETA: 25s - loss: 0.2201 - acc: 0.90 - ETA: 24s - loss: 0.2326 - acc: 0.90 - ETA: 24s - loss: 0.2259 - acc: 0.90 - ETA: 24s - loss: 0.2360 - acc: 0.89 - ETA: 24s - loss: 0.2390 - acc: 0.89 - ETA: 24s - loss: 0.2380 - acc: 0.89 - ETA: 24s - loss: 0.2331 - acc: 0.89 - ETA: 24s - loss: 0.2322 - acc: 0.89 - ETA: 24s - loss: 0.2312 - acc: 0.89 - ETA: 24s - loss: 0.2298 - acc: 0.89 - ETA: 24s - loss: 0.2348 - acc: 0.89 - ETA: 23s - loss: 0.2316 - acc: 0.89 - ETA: 23s - loss: 0.2296 - acc: 0.89 - ETA: 23s - loss: 0.2341 - acc: 0.89 - ETA: 23s - loss: 0.2290 - acc: 0.89 - ETA: 23s - loss: 0.2270 - acc: 0.89 - ETA: 23s - loss: 0.2260 - acc: 0.90 - ETA: 23s - loss: 0.2266 - acc: 0.89 - ETA: 23s - loss: 0.2283 - acc: 0.89 - ETA: 23s - loss: 0.2291 - acc: 0.89 - ETA: 23s - loss: 0.2340 - acc: 0.89 - ETA: 23s - loss: 0.2356 - acc: 0.89 - ETA: 23s - loss: 0.2329 - acc: 0.89 - ETA: 22s - loss: 0.2302 - acc: 0.89 - ETA: 22s - loss: 0.2309 - acc: 0.89 - ETA: 22s - loss: 0.2335 - acc: 0.89 - ETA: 22s - loss: 0.2333 - acc: 0.89 - ETA: 22s - loss: 0.2315 - acc: 0.89 - ETA: 22s - loss: 0.2302 - acc: 0.89 - ETA: 22s - loss: 0.2329 - acc: 0.89 - ETA: 22s - loss: 0.2316 - acc: 0.89 - ETA: 22s - loss: 0.2327 - acc: 0.89 - ETA: 22s - loss: 0.2340 - acc: 0.89 - ETA: 22s - loss: 0.2348 - acc: 0.89 - ETA: 22s - loss: 0.2363 - acc: 0.89 - ETA: 22s - loss: 0.2367 - acc: 0.89 - ETA: 22s - loss: 0.2350 - acc: 0.89 - ETA: 22s - loss: 0.2335 - acc: 0.89 - ETA: 21s - loss: 0.2324 - acc: 0.89 - ETA: 21s - loss: 0.2316 - acc: 0.89 - ETA: 21s - loss: 0.2297 - acc: 0.90 - ETA: 21s - loss: 0.2300 - acc: 0.90 - ETA: 21s - loss: 0.2338 - acc: 0.90 - ETA: 21s - loss: 0.2336 - acc: 0.90 - ETA: 21s - loss: 0.2325 - acc: 0.90 - ETA: 21s - loss: 0.2320 - acc: 0.90 - ETA: 21s - loss: 0.2316 - acc: 0.90 - ETA: 21s - loss: 0.2336 - acc: 0.90 - ETA: 21s - loss: 0.2345 - acc: 0.90 - ETA: 21s - loss: 0.2359 - acc: 0.90 - ETA: 20s - loss: 0.2348 - acc: 0.90 - ETA: 20s - loss: 0.2346 - acc: 0.90 - ETA: 20s - loss: 0.2344 - acc: 0.90 - ETA: 20s - loss: 0.2340 - acc: 0.90 - ETA: 20s - loss: 0.2341 - acc: 0.90 - ETA: 20s - loss: 0.2341 - acc: 0.90 - ETA: 20s - loss: 0.2329 - acc: 0.90 - ETA: 20s - loss: 0.2327 - acc: 0.90 - ETA: 20s - loss: 0.2330 - acc: 0.90 - ETA: 20s - loss: 0.2319 - acc: 0.90 - ETA: 20s - loss: 0.2317 - acc: 0.90 - ETA: 19s - loss: 0.2326 - acc: 0.90 - ETA: 19s - loss: 0.2348 - acc: 0.90 - ETA: 19s - loss: 0.2349 - acc: 0.90 - ETA: 19s - loss: 0.2349 - acc: 0.90 - ETA: 19s - loss: 0.2341 - acc: 0.90 - ETA: 19s - loss: 0.2349 - acc: 0.90 - ETA: 19s - loss: 0.2359 - acc: 0.90 - ETA: 19s - loss: 0.2348 - acc: 0.90 - ETA: 19s - loss: 0.2343 - acc: 0.90 - ETA: 19s - loss: 0.2340 - acc: 0.90 - ETA: 19s - loss: 0.2346 - acc: 0.90 - ETA: 19s - loss: 0.2335 - acc: 0.90 - ETA: 18s - loss: 0.2344 - acc: 0.90 - ETA: 18s - loss: 0.2338 - acc: 0.90 - ETA: 18s - loss: 0.2345 - acc: 0.90 - ETA: 18s - loss: 0.2353 - acc: 0.90 - ETA: 18s - loss: 0.2352 - acc: 0.90 - ETA: 18s - loss: 0.2350 - acc: 0.90 - ETA: 18s - loss: 0.2355 - acc: 0.90 - ETA: 18s - loss: 0.2350 - acc: 0.90 - ETA: 18s - loss: 0.2354 - acc: 0.90 - ETA: 18s - loss: 0.2354 - acc: 0.90 - ETA: 18s - loss: 0.2354 - acc: 0.90 - ETA: 18s - loss: 0.2353 - acc: 0.90 - ETA: 18s - loss: 0.2360 - acc: 0.90 - ETA: 17s - loss: 0.2355 - acc: 0.90 - ETA: 17s - loss: 0.2352 - acc: 0.90 - ETA: 17s - loss: 0.2357 - acc: 0.90 - ETA: 17s - loss: 0.2357 - acc: 0.90 - ETA: 17s - loss: 0.2369 - acc: 0.90 - ETA: 17s - loss: 0.2368 - acc: 0.90 - ETA: 17s - loss: 0.2367 - acc: 0.89 - ETA: 17s - loss: 0.2367 - acc: 0.89 - ETA: 17s - loss: 0.2369 - acc: 0.89 - ETA: 17s - loss: 0.2368 - acc: 0.89 - ETA: 17s - loss: 0.2368 - acc: 0.90 - ETA: 17s - loss: 0.2367 - acc: 0.89 - ETA: 17s - loss: 0.2363 - acc: 0.90 - ETA: 16s - loss: 0.2365 - acc: 0.90 - ETA: 16s - loss: 0.2365 - acc: 0.90 - ETA: 16s - loss: 0.2369 - acc: 0.89 - ETA: 16s - loss: 0.2383 - acc: 0.89 - ETA: 16s - loss: 0.2385 - acc: 0.89 - ETA: 16s - loss: 0.2391 - acc: 0.89 - ETA: 16s - loss: 0.2387 - acc: 0.89 - ETA: 16s - loss: 0.2395 - acc: 0.89 - ETA: 16s - loss: 0.2399 - acc: 0.89 - ETA: 16s - loss: 0.2395 - acc: 0.89 - ETA: 16s - loss: 0.2396 - acc: 0.89 - ETA: 16s - loss: 0.2400 - acc: 0.89 - ETA: 16s - loss: 0.2393 - acc: 0.89 - ETA: 16s - loss: 0.2394 - acc: 0.89 - ETA: 15s - loss: 0.2398 - acc: 0.89 - ETA: 15s - loss: 0.2394 - acc: 0.89 - ETA: 15s - loss: 0.2406 - acc: 0.89 - ETA: 15s - loss: 0.2417 - acc: 0.89 - ETA: 15s - loss: 0.2420 - acc: 0.89 - ETA: 15s - loss: 0.2420 - acc: 0.89 - ETA: 15s - loss: 0.2422 - acc: 0.89 - ETA: 15s - loss: 0.2427 - acc: 0.89 - ETA: 15s - loss: 0.2429 - acc: 0.89 - ETA: 15s - loss: 0.2425 - acc: 0.89 - ETA: 15s - loss: 0.2420 - acc: 0.89 - ETA: 15s - loss: 0.2421 - acc: 0.89 - ETA: 15s - loss: 0.2427 - acc: 0.89 - ETA: 15s - loss: 0.2433 - acc: 0.89 - ETA: 14s - loss: 0.2433 - acc: 0.89 - ETA: 14s - loss: 0.2437 - acc: 0.89 - ETA: 14s - loss: 0.2436 - acc: 0.89 - ETA: 14s - loss: 0.2442 - acc: 0.89 - ETA: 14s - loss: 0.2441 - acc: 0.89 - ETA: 14s - loss: 0.2439 - acc: 0.89 - ETA: 14s - loss: 0.2436 - acc: 0.89 - ETA: 14s - loss: 0.2433 - acc: 0.89 - ETA: 14s - loss: 0.2429 - acc: 0.89 - ETA: 14s - loss: 0.2427 - acc: 0.89 - ETA: 14s - loss: 0.2426 - acc: 0.89 - ETA: 13s - loss: 0.2429 - acc: 0.89 - ETA: 13s - loss: 0.2425 - acc: 0.89 - ETA: 13s - loss: 0.2425 - acc: 0.89 - ETA: 13s - loss: 0.2427 - acc: 0.89 - ETA: 13s - loss: 0.2427 - acc: 0.89 - ETA: 13s - loss: 0.2420 - acc: 0.89 - ETA: 13s - loss: 0.2414 - acc: 0.89 - ETA: 13s - loss: 0.2412 - acc: 0.89 - ETA: 13s - loss: 0.2409 - acc: 0.89 - ETA: 13s - loss: 0.2411 - acc: 0.89 - ETA: 13s - loss: 0.2408 - acc: 0.89 - ETA: 13s - loss: 0.2408 - acc: 0.89 - ETA: 12s - loss: 0.2406 - acc: 0.89 - ETA: 12s - loss: 0.2405 - acc: 0.89 - ETA: 12s - loss: 0.2407 - acc: 0.89 - ETA: 12s - loss: 0.2406 - acc: 0.89 - ETA: 12s - loss: 0.2406 - acc: 0.89 - ETA: 12s - loss: 0.2405 - acc: 0.89 - ETA: 12s - loss: 0.2404 - acc: 0.89 - ETA: 12s - loss: 0.2399 - acc: 0.89 - ETA: 12s - loss: 0.2403 - acc: 0.89 - ETA: 12s - loss: 0.2406 - acc: 0.89 - ETA: 12s - loss: 0.2410 - acc: 0.89 - ETA: 11s - loss: 0.2409 - acc: 0.89 - ETA: 11s - loss: 0.2412 - acc: 0.89 - ETA: 11s - loss: 0.2414 - acc: 0.89 - ETA: 11s - loss: 0.2415 - acc: 0.89 - ETA: 11s - loss: 0.2421 - acc: 0.89 - ETA: 11s - loss: 0.2422 - acc: 0.89 - ETA: 11s - loss: 0.2420 - acc: 0.89 - ETA: 11s - loss: 0.2423 - acc: 0.89 - ETA: 11s - loss: 0.2422 - acc: 0.89 - ETA: 11s - loss: 0.2419 - acc: 0.89 - ETA: 11s - loss: 0.2421 - acc: 0.89 - ETA: 11s - loss: 0.2421 - acc: 0.89 - ETA: 10s - loss: 0.2417 - acc: 0.89 - ETA: 10s - loss: 0.2422 - acc: 0.89 - ETA: 10s - loss: 0.2423 - acc: 0.89 - ETA: 10s - loss: 0.2427 - acc: 0.89 - ETA: 10s - loss: 0.2428 - acc: 0.89 - ETA: 10s - loss: 0.2423 - acc: 0.89 - ETA: 10s - loss: 0.2424 - acc: 0.89 - ETA: 10s - loss: 0.2422 - acc: 0.89 - ETA: 10s - loss: 0.2424 - acc: 0.89 - ETA: 10s - loss: 0.2424 - acc: 0.89 - ETA: 10s - loss: 0.2424 - acc: 0.89 - ETA: 10s - loss: 0.2427 - acc: 0.89 - ETA: 9s - loss: 0.2432 - acc: 0.8981 - ETA: 9s - loss: 0.2434 - acc: 0.898 - ETA: 9s - loss: 0.2436 - acc: 0.897 - ETA: 9s - loss: 0.2435 - acc: 0.897 - ETA: 9s - loss: 0.2432 - acc: 0.898 - ETA: 9s - loss: 0.2433 - acc: 0.898 - ETA: 9s - loss: 0.2431 - acc: 0.898 - ETA: 9s - loss: 0.2431 - acc: 0.898 - ETA: 9s - loss: 0.2438 - acc: 0.897 - ETA: 9s - loss: 0.2435 - acc: 0.898 - ETA: 9s - loss: 0.2436 - acc: 0.898 - ETA: 9s - loss: 0.2435 - acc: 0.898 - ETA: 8s - loss: 0.2431 - acc: 0.898 - ETA: 8s - loss: 0.2429 - acc: 0.898 - ETA: 8s - loss: 0.2431 - acc: 0.898 - ETA: 8s - loss: 0.2427 - acc: 0.898 - ETA: 8s - loss: 0.2428 - acc: 0.898 - ETA: 8s - loss: 0.2430 - acc: 0.898 - ETA: 8s - loss: 0.2429 - acc: 0.898 - ETA: 8s - loss: 0.2431 - acc: 0.898 - ETA: 8s - loss: 0.2434 - acc: 0.898 - ETA: 8s - loss: 0.2434 - acc: 0.898 - ETA: 8s - loss: 0.2434 - acc: 0.898 - ETA: 8s - loss: 0.2433 - acc: 0.898 - ETA: 8s - loss: 0.2433 - acc: 0.8982"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31500/31500 [==============================] - ETA: 7s - loss: 0.2434 - acc: 0.898 - ETA: 7s - loss: 0.2436 - acc: 0.898 - ETA: 7s - loss: 0.2434 - acc: 0.898 - ETA: 7s - loss: 0.2437 - acc: 0.898 - ETA: 7s - loss: 0.2435 - acc: 0.898 - ETA: 7s - loss: 0.2434 - acc: 0.898 - ETA: 7s - loss: 0.2433 - acc: 0.898 - ETA: 7s - loss: 0.2432 - acc: 0.898 - ETA: 7s - loss: 0.2438 - acc: 0.898 - ETA: 7s - loss: 0.2434 - acc: 0.898 - ETA: 7s - loss: 0.2434 - acc: 0.898 - ETA: 7s - loss: 0.2432 - acc: 0.898 - ETA: 6s - loss: 0.2436 - acc: 0.898 - ETA: 6s - loss: 0.2436 - acc: 0.898 - ETA: 6s - loss: 0.2438 - acc: 0.898 - ETA: 6s - loss: 0.2441 - acc: 0.898 - ETA: 6s - loss: 0.2442 - acc: 0.898 - ETA: 6s - loss: 0.2441 - acc: 0.898 - ETA: 6s - loss: 0.2441 - acc: 0.898 - ETA: 6s - loss: 0.2446 - acc: 0.898 - ETA: 6s - loss: 0.2442 - acc: 0.898 - ETA: 6s - loss: 0.2446 - acc: 0.898 - ETA: 6s - loss: 0.2445 - acc: 0.898 - ETA: 6s - loss: 0.2444 - acc: 0.898 - ETA: 5s - loss: 0.2444 - acc: 0.898 - ETA: 5s - loss: 0.2441 - acc: 0.898 - ETA: 5s - loss: 0.2442 - acc: 0.898 - ETA: 5s - loss: 0.2441 - acc: 0.898 - ETA: 5s - loss: 0.2441 - acc: 0.898 - ETA: 5s - loss: 0.2439 - acc: 0.898 - ETA: 5s - loss: 0.2438 - acc: 0.898 - ETA: 5s - loss: 0.2438 - acc: 0.898 - ETA: 5s - loss: 0.2436 - acc: 0.899 - ETA: 5s - loss: 0.2435 - acc: 0.899 - ETA: 5s - loss: 0.2435 - acc: 0.899 - ETA: 5s - loss: 0.2437 - acc: 0.899 - ETA: 4s - loss: 0.2438 - acc: 0.899 - ETA: 4s - loss: 0.2439 - acc: 0.898 - ETA: 4s - loss: 0.2438 - acc: 0.898 - ETA: 4s - loss: 0.2435 - acc: 0.898 - ETA: 4s - loss: 0.2436 - acc: 0.898 - ETA: 4s - loss: 0.2438 - acc: 0.898 - ETA: 4s - loss: 0.2437 - acc: 0.898 - ETA: 4s - loss: 0.2434 - acc: 0.899 - ETA: 4s - loss: 0.2436 - acc: 0.899 - ETA: 4s - loss: 0.2438 - acc: 0.898 - ETA: 4s - loss: 0.2441 - acc: 0.898 - ETA: 4s - loss: 0.2443 - acc: 0.898 - ETA: 4s - loss: 0.2443 - acc: 0.898 - ETA: 3s - loss: 0.2445 - acc: 0.898 - ETA: 3s - loss: 0.2445 - acc: 0.898 - ETA: 3s - loss: 0.2444 - acc: 0.898 - ETA: 3s - loss: 0.2446 - acc: 0.898 - ETA: 3s - loss: 0.2445 - acc: 0.898 - ETA: 3s - loss: 0.2444 - acc: 0.898 - ETA: 3s - loss: 0.2443 - acc: 0.898 - ETA: 3s - loss: 0.2443 - acc: 0.898 - ETA: 3s - loss: 0.2445 - acc: 0.898 - ETA: 3s - loss: 0.2444 - acc: 0.898 - ETA: 3s - loss: 0.2443 - acc: 0.898 - ETA: 3s - loss: 0.2439 - acc: 0.899 - ETA: 2s - loss: 0.2436 - acc: 0.899 - ETA: 2s - loss: 0.2434 - acc: 0.899 - ETA: 2s - loss: 0.2432 - acc: 0.899 - ETA: 2s - loss: 0.2433 - acc: 0.899 - ETA: 2s - loss: 0.2437 - acc: 0.898 - ETA: 2s - loss: 0.2438 - acc: 0.898 - ETA: 2s - loss: 0.2435 - acc: 0.898 - ETA: 2s - loss: 0.2434 - acc: 0.899 - ETA: 2s - loss: 0.2434 - acc: 0.899 - ETA: 2s - loss: 0.2433 - acc: 0.899 - ETA: 2s - loss: 0.2430 - acc: 0.899 - ETA: 2s - loss: 0.2432 - acc: 0.899 - ETA: 2s - loss: 0.2434 - acc: 0.898 - ETA: 1s - loss: 0.2434 - acc: 0.898 - ETA: 1s - loss: 0.2435 - acc: 0.898 - ETA: 1s - loss: 0.2438 - acc: 0.898 - ETA: 1s - loss: 0.2437 - acc: 0.898 - ETA: 1s - loss: 0.2437 - acc: 0.898 - ETA: 1s - loss: 0.2440 - acc: 0.898 - ETA: 1s - loss: 0.2438 - acc: 0.898 - ETA: 1s - loss: 0.2438 - acc: 0.898 - ETA: 1s - loss: 0.2440 - acc: 0.898 - ETA: 1s - loss: 0.2441 - acc: 0.898 - ETA: 1s - loss: 0.2445 - acc: 0.898 - ETA: 1s - loss: 0.2447 - acc: 0.898 - ETA: 1s - loss: 0.2448 - acc: 0.898 - ETA: 0s - loss: 0.2450 - acc: 0.898 - ETA: 0s - loss: 0.2449 - acc: 0.898 - ETA: 0s - loss: 0.2449 - acc: 0.898 - ETA: 0s - loss: 0.2447 - acc: 0.898 - ETA: 0s - loss: 0.2446 - acc: 0.898 - ETA: 0s - loss: 0.2445 - acc: 0.898 - ETA: 0s - loss: 0.2445 - acc: 0.898 - ETA: 0s - loss: 0.2444 - acc: 0.898 - ETA: 0s - loss: 0.2447 - acc: 0.898 - ETA: 0s - loss: 0.2443 - acc: 0.898 - ETA: 0s - loss: 0.2443 - acc: 0.898 - ETA: 0s - loss: 0.2445 - acc: 0.898 - 25s 796us/step - loss: 0.2445 - acc: 0.8983 - val_loss: 0.3045 - val_acc: 0.8749\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21500/31500 [===================>..........] - ETA: 23s - loss: 0.2360 - acc: 0.91 - ETA: 22s - loss: 0.2185 - acc: 0.91 - ETA: 22s - loss: 0.2395 - acc: 0.90 - ETA: 22s - loss: 0.2534 - acc: 0.89 - ETA: 22s - loss: 0.2495 - acc: 0.89 - ETA: 22s - loss: 0.2473 - acc: 0.89 - ETA: 22s - loss: 0.2459 - acc: 0.89 - ETA: 22s - loss: 0.2416 - acc: 0.89 - ETA: 22s - loss: 0.2335 - acc: 0.89 - ETA: 22s - loss: 0.2315 - acc: 0.90 - ETA: 22s - loss: 0.2310 - acc: 0.90 - ETA: 22s - loss: 0.2363 - acc: 0.89 - ETA: 22s - loss: 0.2406 - acc: 0.89 - ETA: 22s - loss: 0.2422 - acc: 0.89 - ETA: 22s - loss: 0.2401 - acc: 0.89 - ETA: 22s - loss: 0.2424 - acc: 0.89 - ETA: 22s - loss: 0.2488 - acc: 0.89 - ETA: 22s - loss: 0.2434 - acc: 0.89 - ETA: 22s - loss: 0.2382 - acc: 0.90 - ETA: 22s - loss: 0.2423 - acc: 0.89 - ETA: 22s - loss: 0.2376 - acc: 0.90 - ETA: 22s - loss: 0.2332 - acc: 0.90 - ETA: 22s - loss: 0.2329 - acc: 0.90 - ETA: 21s - loss: 0.2332 - acc: 0.90 - ETA: 21s - loss: 0.2316 - acc: 0.90 - ETA: 21s - loss: 0.2308 - acc: 0.90 - ETA: 21s - loss: 0.2304 - acc: 0.90 - ETA: 21s - loss: 0.2322 - acc: 0.90 - ETA: 21s - loss: 0.2310 - acc: 0.90 - ETA: 21s - loss: 0.2284 - acc: 0.90 - ETA: 21s - loss: 0.2297 - acc: 0.90 - ETA: 21s - loss: 0.2267 - acc: 0.90 - ETA: 21s - loss: 0.2266 - acc: 0.90 - ETA: 20s - loss: 0.2267 - acc: 0.90 - ETA: 21s - loss: 0.2252 - acc: 0.90 - ETA: 20s - loss: 0.2279 - acc: 0.90 - ETA: 20s - loss: 0.2273 - acc: 0.90 - ETA: 20s - loss: 0.2286 - acc: 0.90 - ETA: 20s - loss: 0.2263 - acc: 0.90 - ETA: 20s - loss: 0.2272 - acc: 0.90 - ETA: 20s - loss: 0.2279 - acc: 0.90 - ETA: 20s - loss: 0.2279 - acc: 0.90 - ETA: 20s - loss: 0.2274 - acc: 0.90 - ETA: 20s - loss: 0.2275 - acc: 0.90 - ETA: 20s - loss: 0.2261 - acc: 0.90 - ETA: 20s - loss: 0.2284 - acc: 0.90 - ETA: 20s - loss: 0.2289 - acc: 0.90 - ETA: 20s - loss: 0.2271 - acc: 0.90 - ETA: 19s - loss: 0.2271 - acc: 0.90 - ETA: 19s - loss: 0.2260 - acc: 0.90 - ETA: 19s - loss: 0.2258 - acc: 0.90 - ETA: 19s - loss: 0.2259 - acc: 0.90 - ETA: 19s - loss: 0.2258 - acc: 0.90 - ETA: 19s - loss: 0.2275 - acc: 0.90 - ETA: 19s - loss: 0.2262 - acc: 0.90 - ETA: 19s - loss: 0.2275 - acc: 0.90 - ETA: 19s - loss: 0.2285 - acc: 0.90 - ETA: 19s - loss: 0.2287 - acc: 0.90 - ETA: 19s - loss: 0.2283 - acc: 0.90 - ETA: 19s - loss: 0.2276 - acc: 0.90 - ETA: 18s - loss: 0.2261 - acc: 0.90 - ETA: 18s - loss: 0.2260 - acc: 0.90 - ETA: 18s - loss: 0.2273 - acc: 0.90 - ETA: 18s - loss: 0.2263 - acc: 0.90 - ETA: 18s - loss: 0.2266 - acc: 0.90 - ETA: 18s - loss: 0.2248 - acc: 0.90 - ETA: 18s - loss: 0.2241 - acc: 0.90 - ETA: 18s - loss: 0.2234 - acc: 0.90 - ETA: 18s - loss: 0.2236 - acc: 0.90 - ETA: 18s - loss: 0.2241 - acc: 0.90 - ETA: 18s - loss: 0.2235 - acc: 0.90 - ETA: 18s - loss: 0.2238 - acc: 0.90 - ETA: 18s - loss: 0.2238 - acc: 0.90 - ETA: 18s - loss: 0.2241 - acc: 0.90 - ETA: 17s - loss: 0.2237 - acc: 0.90 - ETA: 17s - loss: 0.2229 - acc: 0.90 - ETA: 17s - loss: 0.2224 - acc: 0.90 - ETA: 17s - loss: 0.2228 - acc: 0.90 - ETA: 17s - loss: 0.2230 - acc: 0.90 - ETA: 17s - loss: 0.2228 - acc: 0.90 - ETA: 17s - loss: 0.2241 - acc: 0.90 - ETA: 17s - loss: 0.2236 - acc: 0.90 - ETA: 17s - loss: 0.2230 - acc: 0.90 - ETA: 17s - loss: 0.2238 - acc: 0.90 - ETA: 17s - loss: 0.2239 - acc: 0.90 - ETA: 17s - loss: 0.2234 - acc: 0.90 - ETA: 17s - loss: 0.2238 - acc: 0.90 - ETA: 16s - loss: 0.2234 - acc: 0.90 - ETA: 16s - loss: 0.2233 - acc: 0.90 - ETA: 16s - loss: 0.2228 - acc: 0.90 - ETA: 16s - loss: 0.2223 - acc: 0.90 - ETA: 16s - loss: 0.2233 - acc: 0.90 - ETA: 16s - loss: 0.2242 - acc: 0.90 - ETA: 16s - loss: 0.2250 - acc: 0.90 - ETA: 16s - loss: 0.2250 - acc: 0.90 - ETA: 16s - loss: 0.2254 - acc: 0.90 - ETA: 16s - loss: 0.2247 - acc: 0.90 - ETA: 16s - loss: 0.2259 - acc: 0.90 - ETA: 16s - loss: 0.2262 - acc: 0.90 - ETA: 16s - loss: 0.2266 - acc: 0.90 - ETA: 15s - loss: 0.2267 - acc: 0.90 - ETA: 15s - loss: 0.2272 - acc: 0.90 - ETA: 15s - loss: 0.2286 - acc: 0.90 - ETA: 15s - loss: 0.2289 - acc: 0.90 - ETA: 15s - loss: 0.2288 - acc: 0.90 - ETA: 15s - loss: 0.2292 - acc: 0.90 - ETA: 15s - loss: 0.2288 - acc: 0.90 - ETA: 15s - loss: 0.2281 - acc: 0.90 - ETA: 15s - loss: 0.2291 - acc: 0.90 - ETA: 15s - loss: 0.2302 - acc: 0.90 - ETA: 15s - loss: 0.2299 - acc: 0.90 - ETA: 15s - loss: 0.2299 - acc: 0.90 - ETA: 15s - loss: 0.2299 - acc: 0.90 - ETA: 15s - loss: 0.2306 - acc: 0.90 - ETA: 14s - loss: 0.2300 - acc: 0.90 - ETA: 14s - loss: 0.2305 - acc: 0.90 - ETA: 14s - loss: 0.2307 - acc: 0.90 - ETA: 14s - loss: 0.2305 - acc: 0.90 - ETA: 14s - loss: 0.2301 - acc: 0.90 - ETA: 14s - loss: 0.2302 - acc: 0.90 - ETA: 14s - loss: 0.2300 - acc: 0.90 - ETA: 14s - loss: 0.2301 - acc: 0.90 - ETA: 14s - loss: 0.2311 - acc: 0.90 - ETA: 14s - loss: 0.2307 - acc: 0.90 - ETA: 14s - loss: 0.2303 - acc: 0.90 - ETA: 14s - loss: 0.2309 - acc: 0.90 - ETA: 14s - loss: 0.2308 - acc: 0.90 - ETA: 14s - loss: 0.2303 - acc: 0.90 - ETA: 13s - loss: 0.2299 - acc: 0.90 - ETA: 13s - loss: 0.2298 - acc: 0.90 - ETA: 13s - loss: 0.2296 - acc: 0.90 - ETA: 13s - loss: 0.2300 - acc: 0.90 - ETA: 13s - loss: 0.2301 - acc: 0.90 - ETA: 13s - loss: 0.2300 - acc: 0.90 - ETA: 13s - loss: 0.2303 - acc: 0.90 - ETA: 13s - loss: 0.2297 - acc: 0.90 - ETA: 13s - loss: 0.2295 - acc: 0.90 - ETA: 13s - loss: 0.2297 - acc: 0.90 - ETA: 13s - loss: 0.2294 - acc: 0.90 - ETA: 13s - loss: 0.2296 - acc: 0.90 - ETA: 13s - loss: 0.2305 - acc: 0.90 - ETA: 12s - loss: 0.2301 - acc: 0.90 - ETA: 12s - loss: 0.2300 - acc: 0.90 - ETA: 12s - loss: 0.2298 - acc: 0.90 - ETA: 12s - loss: 0.2307 - acc: 0.90 - ETA: 12s - loss: 0.2303 - acc: 0.90 - ETA: 12s - loss: 0.2303 - acc: 0.90 - ETA: 12s - loss: 0.2302 - acc: 0.90 - ETA: 12s - loss: 0.2299 - acc: 0.90 - ETA: 12s - loss: 0.2294 - acc: 0.90 - ETA: 12s - loss: 0.2294 - acc: 0.90 - ETA: 12s - loss: 0.2291 - acc: 0.90 - ETA: 12s - loss: 0.2295 - acc: 0.90 - ETA: 12s - loss: 0.2291 - acc: 0.90 - ETA: 12s - loss: 0.2290 - acc: 0.90 - ETA: 11s - loss: 0.2293 - acc: 0.90 - ETA: 11s - loss: 0.2290 - acc: 0.90 - ETA: 11s - loss: 0.2290 - acc: 0.90 - ETA: 11s - loss: 0.2290 - acc: 0.90 - ETA: 11s - loss: 0.2292 - acc: 0.90 - ETA: 11s - loss: 0.2288 - acc: 0.90 - ETA: 11s - loss: 0.2290 - acc: 0.90 - ETA: 11s - loss: 0.2289 - acc: 0.90 - ETA: 11s - loss: 0.2290 - acc: 0.90 - ETA: 11s - loss: 0.2293 - acc: 0.90 - ETA: 11s - loss: 0.2296 - acc: 0.90 - ETA: 11s - loss: 0.2293 - acc: 0.90 - ETA: 11s - loss: 0.2291 - acc: 0.90 - ETA: 10s - loss: 0.2300 - acc: 0.90 - ETA: 10s - loss: 0.2299 - acc: 0.90 - ETA: 10s - loss: 0.2303 - acc: 0.90 - ETA: 10s - loss: 0.2300 - acc: 0.90 - ETA: 10s - loss: 0.2302 - acc: 0.90 - ETA: 10s - loss: 0.2302 - acc: 0.90 - ETA: 10s - loss: 0.2305 - acc: 0.90 - ETA: 10s - loss: 0.2306 - acc: 0.90 - ETA: 10s - loss: 0.2308 - acc: 0.90 - ETA: 10s - loss: 0.2309 - acc: 0.90 - ETA: 10s - loss: 0.2309 - acc: 0.90 - ETA: 10s - loss: 0.2313 - acc: 0.90 - ETA: 10s - loss: 0.2318 - acc: 0.90 - ETA: 10s - loss: 0.2318 - acc: 0.90 - ETA: 9s - loss: 0.2316 - acc: 0.9046 - ETA: 9s - loss: 0.2323 - acc: 0.904 - ETA: 9s - loss: 0.2323 - acc: 0.903 - ETA: 9s - loss: 0.2329 - acc: 0.903 - ETA: 9s - loss: 0.2332 - acc: 0.903 - ETA: 9s - loss: 0.2330 - acc: 0.903 - ETA: 9s - loss: 0.2331 - acc: 0.903 - ETA: 9s - loss: 0.2329 - acc: 0.904 - ETA: 9s - loss: 0.2329 - acc: 0.904 - ETA: 9s - loss: 0.2327 - acc: 0.904 - ETA: 9s - loss: 0.2332 - acc: 0.903 - ETA: 9s - loss: 0.2328 - acc: 0.903 - ETA: 9s - loss: 0.2331 - acc: 0.903 - ETA: 8s - loss: 0.2331 - acc: 0.903 - ETA: 8s - loss: 0.2333 - acc: 0.903 - ETA: 8s - loss: 0.2330 - acc: 0.903 - ETA: 8s - loss: 0.2332 - acc: 0.903 - ETA: 8s - loss: 0.2331 - acc: 0.903 - ETA: 8s - loss: 0.2329 - acc: 0.903 - ETA: 8s - loss: 0.2329 - acc: 0.903 - ETA: 8s - loss: 0.2331 - acc: 0.903 - ETA: 8s - loss: 0.2331 - acc: 0.903 - ETA: 8s - loss: 0.2334 - acc: 0.903 - ETA: 8s - loss: 0.2337 - acc: 0.903 - ETA: 8s - loss: 0.2336 - acc: 0.903 - ETA: 8s - loss: 0.2335 - acc: 0.903 - ETA: 8s - loss: 0.2337 - acc: 0.903 - ETA: 7s - loss: 0.2338 - acc: 0.903 - ETA: 7s - loss: 0.2339 - acc: 0.903 - ETA: 7s - loss: 0.2339 - acc: 0.903 - ETA: 7s - loss: 0.2343 - acc: 0.902 - ETA: 7s - loss: 0.2344 - acc: 0.902 - ETA: 7s - loss: 0.2345 - acc: 0.9028"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31500/31500 [==============================] - ETA: 7s - loss: 0.2348 - acc: 0.902 - ETA: 7s - loss: 0.2350 - acc: 0.902 - ETA: 7s - loss: 0.2352 - acc: 0.902 - ETA: 7s - loss: 0.2351 - acc: 0.902 - ETA: 7s - loss: 0.2352 - acc: 0.902 - ETA: 7s - loss: 0.2352 - acc: 0.902 - ETA: 7s - loss: 0.2351 - acc: 0.903 - ETA: 6s - loss: 0.2357 - acc: 0.902 - ETA: 6s - loss: 0.2353 - acc: 0.903 - ETA: 6s - loss: 0.2359 - acc: 0.902 - ETA: 6s - loss: 0.2364 - acc: 0.902 - ETA: 6s - loss: 0.2368 - acc: 0.902 - ETA: 6s - loss: 0.2369 - acc: 0.902 - ETA: 6s - loss: 0.2370 - acc: 0.902 - ETA: 6s - loss: 0.2373 - acc: 0.902 - ETA: 6s - loss: 0.2373 - acc: 0.902 - ETA: 6s - loss: 0.2372 - acc: 0.901 - ETA: 6s - loss: 0.2372 - acc: 0.901 - ETA: 6s - loss: 0.2372 - acc: 0.901 - ETA: 6s - loss: 0.2372 - acc: 0.902 - ETA: 5s - loss: 0.2373 - acc: 0.901 - ETA: 5s - loss: 0.2369 - acc: 0.902 - ETA: 5s - loss: 0.2369 - acc: 0.902 - ETA: 5s - loss: 0.2367 - acc: 0.902 - ETA: 5s - loss: 0.2366 - acc: 0.902 - ETA: 5s - loss: 0.2366 - acc: 0.902 - ETA: 5s - loss: 0.2362 - acc: 0.902 - ETA: 5s - loss: 0.2360 - acc: 0.902 - ETA: 5s - loss: 0.2362 - acc: 0.902 - ETA: 5s - loss: 0.2368 - acc: 0.902 - ETA: 5s - loss: 0.2368 - acc: 0.902 - ETA: 5s - loss: 0.2370 - acc: 0.902 - ETA: 5s - loss: 0.2372 - acc: 0.902 - ETA: 4s - loss: 0.2373 - acc: 0.902 - ETA: 4s - loss: 0.2371 - acc: 0.902 - ETA: 4s - loss: 0.2369 - acc: 0.902 - ETA: 4s - loss: 0.2368 - acc: 0.902 - ETA: 4s - loss: 0.2367 - acc: 0.902 - ETA: 4s - loss: 0.2372 - acc: 0.902 - ETA: 4s - loss: 0.2373 - acc: 0.902 - ETA: 4s - loss: 0.2371 - acc: 0.902 - ETA: 4s - loss: 0.2371 - acc: 0.902 - ETA: 4s - loss: 0.2370 - acc: 0.902 - ETA: 4s - loss: 0.2368 - acc: 0.902 - ETA: 4s - loss: 0.2367 - acc: 0.902 - ETA: 4s - loss: 0.2367 - acc: 0.902 - ETA: 3s - loss: 0.2362 - acc: 0.903 - ETA: 3s - loss: 0.2363 - acc: 0.902 - ETA: 3s - loss: 0.2367 - acc: 0.902 - ETA: 3s - loss: 0.2367 - acc: 0.902 - ETA: 3s - loss: 0.2368 - acc: 0.902 - ETA: 3s - loss: 0.2364 - acc: 0.903 - ETA: 3s - loss: 0.2365 - acc: 0.903 - ETA: 3s - loss: 0.2364 - acc: 0.903 - ETA: 3s - loss: 0.2367 - acc: 0.902 - ETA: 3s - loss: 0.2365 - acc: 0.903 - ETA: 3s - loss: 0.2367 - acc: 0.902 - ETA: 3s - loss: 0.2367 - acc: 0.903 - ETA: 3s - loss: 0.2365 - acc: 0.903 - ETA: 3s - loss: 0.2366 - acc: 0.902 - ETA: 2s - loss: 0.2365 - acc: 0.903 - ETA: 2s - loss: 0.2367 - acc: 0.902 - ETA: 2s - loss: 0.2371 - acc: 0.902 - ETA: 2s - loss: 0.2371 - acc: 0.902 - ETA: 2s - loss: 0.2371 - acc: 0.902 - ETA: 2s - loss: 0.2371 - acc: 0.902 - ETA: 2s - loss: 0.2368 - acc: 0.902 - ETA: 2s - loss: 0.2366 - acc: 0.902 - ETA: 2s - loss: 0.2367 - acc: 0.902 - ETA: 2s - loss: 0.2367 - acc: 0.902 - ETA: 2s - loss: 0.2366 - acc: 0.902 - ETA: 2s - loss: 0.2365 - acc: 0.902 - ETA: 2s - loss: 0.2363 - acc: 0.902 - ETA: 1s - loss: 0.2364 - acc: 0.902 - ETA: 1s - loss: 0.2362 - acc: 0.902 - ETA: 1s - loss: 0.2364 - acc: 0.902 - ETA: 1s - loss: 0.2361 - acc: 0.902 - ETA: 1s - loss: 0.2359 - acc: 0.902 - ETA: 1s - loss: 0.2359 - acc: 0.902 - ETA: 1s - loss: 0.2362 - acc: 0.902 - ETA: 1s - loss: 0.2362 - acc: 0.902 - ETA: 1s - loss: 0.2364 - acc: 0.902 - ETA: 1s - loss: 0.2363 - acc: 0.902 - ETA: 1s - loss: 0.2362 - acc: 0.902 - ETA: 1s - loss: 0.2365 - acc: 0.902 - ETA: 1s - loss: 0.2366 - acc: 0.902 - ETA: 0s - loss: 0.2366 - acc: 0.902 - ETA: 0s - loss: 0.2365 - acc: 0.902 - ETA: 0s - loss: 0.2365 - acc: 0.902 - ETA: 0s - loss: 0.2369 - acc: 0.902 - ETA: 0s - loss: 0.2370 - acc: 0.902 - ETA: 0s - loss: 0.2368 - acc: 0.902 - ETA: 0s - loss: 0.2369 - acc: 0.902 - ETA: 0s - loss: 0.2367 - acc: 0.902 - ETA: 0s - loss: 0.2367 - acc: 0.902 - ETA: 0s - loss: 0.2368 - acc: 0.902 - ETA: 0s - loss: 0.2372 - acc: 0.902 - ETA: 0s - loss: 0.2372 - acc: 0.901 - ETA: 0s - loss: 0.2370 - acc: 0.901 - 24s 765us/step - loss: 0.2368 - acc: 0.9020 - val_loss: 0.3002 - val_acc: 0.8817\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21500/31500 [===================>..........] - ETA: 18s - loss: 0.2712 - acc: 0.90 - ETA: 20s - loss: 0.2292 - acc: 0.92 - ETA: 21s - loss: 0.2104 - acc: 0.92 - ETA: 21s - loss: 0.2168 - acc: 0.92 - ETA: 22s - loss: 0.1939 - acc: 0.93 - ETA: 22s - loss: 0.1963 - acc: 0.92 - ETA: 22s - loss: 0.2001 - acc: 0.92 - ETA: 22s - loss: 0.1977 - acc: 0.92 - ETA: 21s - loss: 0.2027 - acc: 0.91 - ETA: 22s - loss: 0.1982 - acc: 0.92 - ETA: 22s - loss: 0.1977 - acc: 0.92 - ETA: 21s - loss: 0.2078 - acc: 0.91 - ETA: 21s - loss: 0.2146 - acc: 0.91 - ETA: 21s - loss: 0.2180 - acc: 0.91 - ETA: 21s - loss: 0.2152 - acc: 0.91 - ETA: 21s - loss: 0.2154 - acc: 0.91 - ETA: 21s - loss: 0.2119 - acc: 0.91 - ETA: 21s - loss: 0.2100 - acc: 0.91 - ETA: 21s - loss: 0.2095 - acc: 0.91 - ETA: 21s - loss: 0.2097 - acc: 0.91 - ETA: 21s - loss: 0.2183 - acc: 0.91 - ETA: 21s - loss: 0.2161 - acc: 0.91 - ETA: 21s - loss: 0.2132 - acc: 0.91 - ETA: 21s - loss: 0.2155 - acc: 0.91 - ETA: 21s - loss: 0.2144 - acc: 0.91 - ETA: 21s - loss: 0.2144 - acc: 0.91 - ETA: 21s - loss: 0.2164 - acc: 0.91 - ETA: 21s - loss: 0.2131 - acc: 0.91 - ETA: 20s - loss: 0.2120 - acc: 0.91 - ETA: 20s - loss: 0.2123 - acc: 0.91 - ETA: 20s - loss: 0.2109 - acc: 0.91 - ETA: 20s - loss: 0.2111 - acc: 0.91 - ETA: 20s - loss: 0.2115 - acc: 0.91 - ETA: 20s - loss: 0.2140 - acc: 0.91 - ETA: 20s - loss: 0.2131 - acc: 0.91 - ETA: 20s - loss: 0.2134 - acc: 0.91 - ETA: 20s - loss: 0.2135 - acc: 0.91 - ETA: 20s - loss: 0.2127 - acc: 0.91 - ETA: 20s - loss: 0.2130 - acc: 0.91 - ETA: 20s - loss: 0.2129 - acc: 0.91 - ETA: 20s - loss: 0.2112 - acc: 0.91 - ETA: 20s - loss: 0.2117 - acc: 0.91 - ETA: 20s - loss: 0.2127 - acc: 0.91 - ETA: 20s - loss: 0.2125 - acc: 0.91 - ETA: 20s - loss: 0.2130 - acc: 0.91 - ETA: 20s - loss: 0.2124 - acc: 0.91 - ETA: 19s - loss: 0.2151 - acc: 0.91 - ETA: 19s - loss: 0.2148 - acc: 0.91 - ETA: 19s - loss: 0.2138 - acc: 0.91 - ETA: 19s - loss: 0.2146 - acc: 0.91 - ETA: 19s - loss: 0.2139 - acc: 0.91 - ETA: 19s - loss: 0.2135 - acc: 0.91 - ETA: 19s - loss: 0.2118 - acc: 0.91 - ETA: 19s - loss: 0.2129 - acc: 0.91 - ETA: 19s - loss: 0.2130 - acc: 0.91 - ETA: 19s - loss: 0.2124 - acc: 0.91 - ETA: 19s - loss: 0.2123 - acc: 0.91 - ETA: 19s - loss: 0.2125 - acc: 0.91 - ETA: 19s - loss: 0.2110 - acc: 0.91 - ETA: 19s - loss: 0.2116 - acc: 0.91 - ETA: 18s - loss: 0.2132 - acc: 0.91 - ETA: 18s - loss: 0.2141 - acc: 0.91 - ETA: 18s - loss: 0.2153 - acc: 0.91 - ETA: 18s - loss: 0.2146 - acc: 0.91 - ETA: 18s - loss: 0.2158 - acc: 0.91 - ETA: 18s - loss: 0.2165 - acc: 0.91 - ETA: 18s - loss: 0.2167 - acc: 0.91 - ETA: 18s - loss: 0.2169 - acc: 0.91 - ETA: 18s - loss: 0.2167 - acc: 0.91 - ETA: 18s - loss: 0.2168 - acc: 0.91 - ETA: 18s - loss: 0.2165 - acc: 0.91 - ETA: 18s - loss: 0.2164 - acc: 0.91 - ETA: 18s - loss: 0.2157 - acc: 0.91 - ETA: 18s - loss: 0.2158 - acc: 0.91 - ETA: 18s - loss: 0.2167 - acc: 0.91 - ETA: 17s - loss: 0.2167 - acc: 0.91 - ETA: 17s - loss: 0.2171 - acc: 0.91 - ETA: 17s - loss: 0.2170 - acc: 0.91 - ETA: 17s - loss: 0.2169 - acc: 0.91 - ETA: 17s - loss: 0.2176 - acc: 0.91 - ETA: 17s - loss: 0.2179 - acc: 0.91 - ETA: 17s - loss: 0.2172 - acc: 0.91 - ETA: 17s - loss: 0.2180 - acc: 0.91 - ETA: 17s - loss: 0.2184 - acc: 0.91 - ETA: 17s - loss: 0.2182 - acc: 0.91 - ETA: 17s - loss: 0.2181 - acc: 0.91 - ETA: 17s - loss: 0.2183 - acc: 0.91 - ETA: 17s - loss: 0.2183 - acc: 0.91 - ETA: 17s - loss: 0.2176 - acc: 0.91 - ETA: 16s - loss: 0.2174 - acc: 0.91 - ETA: 16s - loss: 0.2176 - acc: 0.91 - ETA: 16s - loss: 0.2177 - acc: 0.91 - ETA: 16s - loss: 0.2172 - acc: 0.91 - ETA: 16s - loss: 0.2172 - acc: 0.91 - ETA: 16s - loss: 0.2184 - acc: 0.91 - ETA: 16s - loss: 0.2188 - acc: 0.91 - ETA: 16s - loss: 0.2185 - acc: 0.91 - ETA: 16s - loss: 0.2190 - acc: 0.91 - ETA: 16s - loss: 0.2195 - acc: 0.90 - ETA: 16s - loss: 0.2193 - acc: 0.91 - ETA: 16s - loss: 0.2193 - acc: 0.91 - ETA: 16s - loss: 0.2194 - acc: 0.91 - ETA: 16s - loss: 0.2195 - acc: 0.91 - ETA: 15s - loss: 0.2196 - acc: 0.90 - ETA: 15s - loss: 0.2194 - acc: 0.90 - ETA: 15s - loss: 0.2191 - acc: 0.90 - ETA: 15s - loss: 0.2196 - acc: 0.90 - ETA: 15s - loss: 0.2196 - acc: 0.90 - ETA: 15s - loss: 0.2203 - acc: 0.90 - ETA: 15s - loss: 0.2212 - acc: 0.90 - ETA: 15s - loss: 0.2208 - acc: 0.90 - ETA: 15s - loss: 0.2209 - acc: 0.90 - ETA: 15s - loss: 0.2205 - acc: 0.90 - ETA: 15s - loss: 0.2206 - acc: 0.90 - ETA: 15s - loss: 0.2203 - acc: 0.90 - ETA: 15s - loss: 0.2203 - acc: 0.90 - ETA: 15s - loss: 0.2211 - acc: 0.90 - ETA: 14s - loss: 0.2211 - acc: 0.90 - ETA: 14s - loss: 0.2216 - acc: 0.90 - ETA: 14s - loss: 0.2225 - acc: 0.90 - ETA: 14s - loss: 0.2220 - acc: 0.90 - ETA: 14s - loss: 0.2234 - acc: 0.90 - ETA: 14s - loss: 0.2236 - acc: 0.90 - ETA: 14s - loss: 0.2233 - acc: 0.90 - ETA: 14s - loss: 0.2234 - acc: 0.90 - ETA: 14s - loss: 0.2230 - acc: 0.90 - ETA: 14s - loss: 0.2231 - acc: 0.90 - ETA: 14s - loss: 0.2227 - acc: 0.90 - ETA: 14s - loss: 0.2229 - acc: 0.90 - ETA: 14s - loss: 0.2229 - acc: 0.90 - ETA: 13s - loss: 0.2225 - acc: 0.90 - ETA: 13s - loss: 0.2220 - acc: 0.90 - ETA: 13s - loss: 0.2221 - acc: 0.90 - ETA: 13s - loss: 0.2218 - acc: 0.90 - ETA: 13s - loss: 0.2213 - acc: 0.90 - ETA: 13s - loss: 0.2211 - acc: 0.90 - ETA: 13s - loss: 0.2207 - acc: 0.90 - ETA: 13s - loss: 0.2211 - acc: 0.90 - ETA: 13s - loss: 0.2213 - acc: 0.90 - ETA: 13s - loss: 0.2206 - acc: 0.90 - ETA: 13s - loss: 0.2201 - acc: 0.90 - ETA: 13s - loss: 0.2203 - acc: 0.90 - ETA: 13s - loss: 0.2202 - acc: 0.91 - ETA: 12s - loss: 0.2198 - acc: 0.91 - ETA: 12s - loss: 0.2206 - acc: 0.90 - ETA: 12s - loss: 0.2209 - acc: 0.90 - ETA: 12s - loss: 0.2205 - acc: 0.91 - ETA: 12s - loss: 0.2202 - acc: 0.91 - ETA: 12s - loss: 0.2201 - acc: 0.91 - ETA: 12s - loss: 0.2200 - acc: 0.91 - ETA: 12s - loss: 0.2201 - acc: 0.91 - ETA: 12s - loss: 0.2204 - acc: 0.91 - ETA: 12s - loss: 0.2205 - acc: 0.91 - ETA: 12s - loss: 0.2204 - acc: 0.90 - ETA: 12s - loss: 0.2203 - acc: 0.91 - ETA: 12s - loss: 0.2205 - acc: 0.90 - ETA: 11s - loss: 0.2203 - acc: 0.91 - ETA: 11s - loss: 0.2205 - acc: 0.91 - ETA: 11s - loss: 0.2203 - acc: 0.91 - ETA: 11s - loss: 0.2203 - acc: 0.91 - ETA: 11s - loss: 0.2203 - acc: 0.91 - ETA: 11s - loss: 0.2203 - acc: 0.91 - ETA: 11s - loss: 0.2204 - acc: 0.91 - ETA: 11s - loss: 0.2203 - acc: 0.91 - ETA: 11s - loss: 0.2201 - acc: 0.91 - ETA: 11s - loss: 0.2197 - acc: 0.91 - ETA: 11s - loss: 0.2203 - acc: 0.91 - ETA: 11s - loss: 0.2201 - acc: 0.91 - ETA: 11s - loss: 0.2197 - acc: 0.91 - ETA: 10s - loss: 0.2205 - acc: 0.91 - ETA: 10s - loss: 0.2204 - acc: 0.91 - ETA: 10s - loss: 0.2204 - acc: 0.91 - ETA: 10s - loss: 0.2206 - acc: 0.91 - ETA: 10s - loss: 0.2203 - acc: 0.91 - ETA: 10s - loss: 0.2203 - acc: 0.91 - ETA: 10s - loss: 0.2200 - acc: 0.91 - ETA: 10s - loss: 0.2199 - acc: 0.91 - ETA: 10s - loss: 0.2198 - acc: 0.91 - ETA: 10s - loss: 0.2199 - acc: 0.91 - ETA: 10s - loss: 0.2199 - acc: 0.91 - ETA: 10s - loss: 0.2199 - acc: 0.91 - ETA: 10s - loss: 0.2201 - acc: 0.91 - ETA: 10s - loss: 0.2203 - acc: 0.91 - ETA: 9s - loss: 0.2201 - acc: 0.9102 - ETA: 9s - loss: 0.2201 - acc: 0.910 - ETA: 9s - loss: 0.2197 - acc: 0.910 - ETA: 9s - loss: 0.2198 - acc: 0.910 - ETA: 9s - loss: 0.2198 - acc: 0.910 - ETA: 9s - loss: 0.2197 - acc: 0.910 - ETA: 9s - loss: 0.2193 - acc: 0.910 - ETA: 9s - loss: 0.2196 - acc: 0.910 - ETA: 9s - loss: 0.2199 - acc: 0.910 - ETA: 9s - loss: 0.2203 - acc: 0.910 - ETA: 9s - loss: 0.2205 - acc: 0.909 - ETA: 9s - loss: 0.2205 - acc: 0.910 - ETA: 9s - loss: 0.2206 - acc: 0.909 - ETA: 8s - loss: 0.2205 - acc: 0.910 - ETA: 8s - loss: 0.2206 - acc: 0.910 - ETA: 8s - loss: 0.2205 - acc: 0.910 - ETA: 8s - loss: 0.2206 - acc: 0.910 - ETA: 8s - loss: 0.2206 - acc: 0.909 - ETA: 8s - loss: 0.2204 - acc: 0.909 - ETA: 8s - loss: 0.2207 - acc: 0.909 - ETA: 8s - loss: 0.2209 - acc: 0.909 - ETA: 8s - loss: 0.2210 - acc: 0.909 - ETA: 8s - loss: 0.2212 - acc: 0.909 - ETA: 8s - loss: 0.2212 - acc: 0.909 - ETA: 8s - loss: 0.2216 - acc: 0.908 - ETA: 8s - loss: 0.2220 - acc: 0.908 - ETA: 7s - loss: 0.2218 - acc: 0.908 - ETA: 7s - loss: 0.2222 - acc: 0.908 - ETA: 7s - loss: 0.2221 - acc: 0.908 - ETA: 7s - loss: 0.2223 - acc: 0.908 - ETA: 7s - loss: 0.2222 - acc: 0.908 - ETA: 7s - loss: 0.2222 - acc: 0.9090"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31500/31500 [==============================] - ETA: 7s - loss: 0.2222 - acc: 0.909 - ETA: 7s - loss: 0.2220 - acc: 0.909 - ETA: 7s - loss: 0.2218 - acc: 0.909 - ETA: 7s - loss: 0.2216 - acc: 0.909 - ETA: 7s - loss: 0.2215 - acc: 0.909 - ETA: 7s - loss: 0.2214 - acc: 0.909 - ETA: 7s - loss: 0.2219 - acc: 0.909 - ETA: 6s - loss: 0.2217 - acc: 0.909 - ETA: 6s - loss: 0.2215 - acc: 0.909 - ETA: 6s - loss: 0.2215 - acc: 0.909 - ETA: 6s - loss: 0.2223 - acc: 0.908 - ETA: 6s - loss: 0.2223 - acc: 0.908 - ETA: 6s - loss: 0.2226 - acc: 0.908 - ETA: 6s - loss: 0.2222 - acc: 0.909 - ETA: 6s - loss: 0.2223 - acc: 0.908 - ETA: 6s - loss: 0.2221 - acc: 0.909 - ETA: 6s - loss: 0.2225 - acc: 0.908 - ETA: 6s - loss: 0.2224 - acc: 0.908 - ETA: 6s - loss: 0.2226 - acc: 0.908 - ETA: 6s - loss: 0.2226 - acc: 0.908 - ETA: 5s - loss: 0.2229 - acc: 0.908 - ETA: 5s - loss: 0.2231 - acc: 0.908 - ETA: 5s - loss: 0.2232 - acc: 0.908 - ETA: 5s - loss: 0.2233 - acc: 0.908 - ETA: 5s - loss: 0.2236 - acc: 0.908 - ETA: 5s - loss: 0.2239 - acc: 0.907 - ETA: 5s - loss: 0.2243 - acc: 0.907 - ETA: 5s - loss: 0.2251 - acc: 0.907 - ETA: 5s - loss: 0.2252 - acc: 0.907 - ETA: 5s - loss: 0.2252 - acc: 0.907 - ETA: 5s - loss: 0.2254 - acc: 0.907 - ETA: 5s - loss: 0.2254 - acc: 0.907 - ETA: 5s - loss: 0.2257 - acc: 0.907 - ETA: 5s - loss: 0.2258 - acc: 0.907 - ETA: 4s - loss: 0.2261 - acc: 0.907 - ETA: 4s - loss: 0.2262 - acc: 0.907 - ETA: 4s - loss: 0.2263 - acc: 0.906 - ETA: 4s - loss: 0.2264 - acc: 0.906 - ETA: 4s - loss: 0.2263 - acc: 0.906 - ETA: 4s - loss: 0.2264 - acc: 0.906 - ETA: 4s - loss: 0.2265 - acc: 0.906 - ETA: 4s - loss: 0.2264 - acc: 0.906 - ETA: 4s - loss: 0.2264 - acc: 0.906 - ETA: 4s - loss: 0.2265 - acc: 0.906 - ETA: 4s - loss: 0.2265 - acc: 0.906 - ETA: 4s - loss: 0.2267 - acc: 0.906 - ETA: 4s - loss: 0.2267 - acc: 0.906 - ETA: 3s - loss: 0.2268 - acc: 0.906 - ETA: 3s - loss: 0.2267 - acc: 0.906 - ETA: 3s - loss: 0.2265 - acc: 0.906 - ETA: 3s - loss: 0.2260 - acc: 0.906 - ETA: 3s - loss: 0.2257 - acc: 0.907 - ETA: 3s - loss: 0.2254 - acc: 0.907 - ETA: 3s - loss: 0.2251 - acc: 0.907 - ETA: 3s - loss: 0.2249 - acc: 0.907 - ETA: 3s - loss: 0.2248 - acc: 0.907 - ETA: 3s - loss: 0.2248 - acc: 0.907 - ETA: 3s - loss: 0.2247 - acc: 0.907 - ETA: 3s - loss: 0.2244 - acc: 0.907 - ETA: 3s - loss: 0.2247 - acc: 0.907 - ETA: 2s - loss: 0.2247 - acc: 0.907 - ETA: 2s - loss: 0.2250 - acc: 0.907 - ETA: 2s - loss: 0.2251 - acc: 0.907 - ETA: 2s - loss: 0.2252 - acc: 0.907 - ETA: 2s - loss: 0.2250 - acc: 0.907 - ETA: 2s - loss: 0.2250 - acc: 0.907 - ETA: 2s - loss: 0.2251 - acc: 0.907 - ETA: 2s - loss: 0.2252 - acc: 0.907 - ETA: 2s - loss: 0.2251 - acc: 0.907 - ETA: 2s - loss: 0.2249 - acc: 0.907 - ETA: 2s - loss: 0.2247 - acc: 0.907 - ETA: 2s - loss: 0.2250 - acc: 0.907 - ETA: 2s - loss: 0.2250 - acc: 0.906 - ETA: 1s - loss: 0.2248 - acc: 0.907 - ETA: 1s - loss: 0.2248 - acc: 0.907 - ETA: 1s - loss: 0.2248 - acc: 0.907 - ETA: 1s - loss: 0.2248 - acc: 0.907 - ETA: 1s - loss: 0.2248 - acc: 0.907 - ETA: 1s - loss: 0.2248 - acc: 0.907 - ETA: 1s - loss: 0.2249 - acc: 0.906 - ETA: 1s - loss: 0.2253 - acc: 0.906 - ETA: 1s - loss: 0.2249 - acc: 0.906 - ETA: 1s - loss: 0.2248 - acc: 0.906 - ETA: 1s - loss: 0.2249 - acc: 0.906 - ETA: 1s - loss: 0.2250 - acc: 0.906 - ETA: 1s - loss: 0.2248 - acc: 0.906 - ETA: 0s - loss: 0.2247 - acc: 0.906 - ETA: 0s - loss: 0.2248 - acc: 0.906 - ETA: 0s - loss: 0.2246 - acc: 0.906 - ETA: 0s - loss: 0.2249 - acc: 0.906 - ETA: 0s - loss: 0.2249 - acc: 0.906 - ETA: 0s - loss: 0.2249 - acc: 0.906 - ETA: 0s - loss: 0.2251 - acc: 0.906 - ETA: 0s - loss: 0.2253 - acc: 0.906 - ETA: 0s - loss: 0.2252 - acc: 0.906 - ETA: 0s - loss: 0.2251 - acc: 0.906 - ETA: 0s - loss: 0.2249 - acc: 0.906 - ETA: 0s - loss: 0.2251 - acc: 0.906 - ETA: 0s - loss: 0.2250 - acc: 0.906 - 24s 771us/step - loss: 0.2249 - acc: 0.9065 - val_loss: 0.3114 - val_acc: 0.8697\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21500/31500 [===================>..........] - ETA: 17s - loss: 0.2320 - acc: 0.91 - ETA: 20s - loss: 0.2444 - acc: 0.91 - ETA: 21s - loss: 0.2047 - acc: 0.92 - ETA: 21s - loss: 0.1903 - acc: 0.92 - ETA: 21s - loss: 0.1849 - acc: 0.92 - ETA: 21s - loss: 0.1818 - acc: 0.92 - ETA: 22s - loss: 0.1836 - acc: 0.92 - ETA: 22s - loss: 0.1924 - acc: 0.92 - ETA: 22s - loss: 0.1976 - acc: 0.91 - ETA: 22s - loss: 0.1929 - acc: 0.92 - ETA: 22s - loss: 0.1950 - acc: 0.91 - ETA: 22s - loss: 0.1932 - acc: 0.91 - ETA: 22s - loss: 0.1963 - acc: 0.91 - ETA: 22s - loss: 0.1953 - acc: 0.91 - ETA: 21s - loss: 0.1904 - acc: 0.91 - ETA: 21s - loss: 0.1896 - acc: 0.92 - ETA: 21s - loss: 0.1906 - acc: 0.92 - ETA: 21s - loss: 0.1923 - acc: 0.92 - ETA: 21s - loss: 0.1951 - acc: 0.91 - ETA: 21s - loss: 0.1968 - acc: 0.91 - ETA: 21s - loss: 0.1997 - acc: 0.91 - ETA: 21s - loss: 0.1994 - acc: 0.91 - ETA: 21s - loss: 0.2017 - acc: 0.91 - ETA: 21s - loss: 0.2029 - acc: 0.91 - ETA: 21s - loss: 0.2003 - acc: 0.91 - ETA: 21s - loss: 0.2006 - acc: 0.91 - ETA: 21s - loss: 0.2024 - acc: 0.91 - ETA: 21s - loss: 0.2020 - acc: 0.91 - ETA: 21s - loss: 0.2035 - acc: 0.91 - ETA: 21s - loss: 0.2030 - acc: 0.91 - ETA: 21s - loss: 0.2009 - acc: 0.91 - ETA: 20s - loss: 0.2020 - acc: 0.91 - ETA: 20s - loss: 0.2024 - acc: 0.91 - ETA: 20s - loss: 0.2029 - acc: 0.91 - ETA: 20s - loss: 0.2034 - acc: 0.91 - ETA: 20s - loss: 0.2061 - acc: 0.91 - ETA: 20s - loss: 0.2051 - acc: 0.91 - ETA: 20s - loss: 0.2038 - acc: 0.91 - ETA: 20s - loss: 0.2041 - acc: 0.91 - ETA: 20s - loss: 0.2024 - acc: 0.91 - ETA: 20s - loss: 0.2032 - acc: 0.91 - ETA: 20s - loss: 0.2047 - acc: 0.91 - ETA: 20s - loss: 0.2064 - acc: 0.91 - ETA: 20s - loss: 0.2064 - acc: 0.91 - ETA: 19s - loss: 0.2080 - acc: 0.91 - ETA: 19s - loss: 0.2075 - acc: 0.91 - ETA: 19s - loss: 0.2068 - acc: 0.91 - ETA: 19s - loss: 0.2081 - acc: 0.91 - ETA: 19s - loss: 0.2085 - acc: 0.91 - ETA: 19s - loss: 0.2091 - acc: 0.91 - ETA: 19s - loss: 0.2083 - acc: 0.91 - ETA: 19s - loss: 0.2090 - acc: 0.91 - ETA: 19s - loss: 0.2086 - acc: 0.91 - ETA: 19s - loss: 0.2080 - acc: 0.91 - ETA: 19s - loss: 0.2086 - acc: 0.91 - ETA: 19s - loss: 0.2076 - acc: 0.91 - ETA: 19s - loss: 0.2070 - acc: 0.91 - ETA: 19s - loss: 0.2067 - acc: 0.91 - ETA: 18s - loss: 0.2073 - acc: 0.91 - ETA: 18s - loss: 0.2062 - acc: 0.91 - ETA: 18s - loss: 0.2062 - acc: 0.91 - ETA: 18s - loss: 0.2057 - acc: 0.91 - ETA: 18s - loss: 0.2049 - acc: 0.91 - ETA: 18s - loss: 0.2033 - acc: 0.91 - ETA: 18s - loss: 0.2043 - acc: 0.91 - ETA: 18s - loss: 0.2041 - acc: 0.91 - ETA: 18s - loss: 0.2039 - acc: 0.91 - ETA: 18s - loss: 0.2045 - acc: 0.91 - ETA: 18s - loss: 0.2041 - acc: 0.91 - ETA: 18s - loss: 0.2041 - acc: 0.91 - ETA: 18s - loss: 0.2048 - acc: 0.91 - ETA: 18s - loss: 0.2048 - acc: 0.91 - ETA: 17s - loss: 0.2057 - acc: 0.91 - ETA: 17s - loss: 0.2045 - acc: 0.91 - ETA: 17s - loss: 0.2041 - acc: 0.91 - ETA: 17s - loss: 0.2039 - acc: 0.91 - ETA: 17s - loss: 0.2045 - acc: 0.91 - ETA: 17s - loss: 0.2047 - acc: 0.91 - ETA: 17s - loss: 0.2066 - acc: 0.91 - ETA: 17s - loss: 0.2075 - acc: 0.91 - ETA: 17s - loss: 0.2080 - acc: 0.91 - ETA: 17s - loss: 0.2075 - acc: 0.91 - ETA: 17s - loss: 0.2072 - acc: 0.91 - ETA: 17s - loss: 0.2081 - acc: 0.91 - ETA: 17s - loss: 0.2071 - acc: 0.91 - ETA: 17s - loss: 0.2065 - acc: 0.91 - ETA: 16s - loss: 0.2067 - acc: 0.91 - ETA: 16s - loss: 0.2064 - acc: 0.91 - ETA: 16s - loss: 0.2066 - acc: 0.91 - ETA: 16s - loss: 0.2071 - acc: 0.91 - ETA: 16s - loss: 0.2081 - acc: 0.91 - ETA: 16s - loss: 0.2082 - acc: 0.91 - ETA: 16s - loss: 0.2077 - acc: 0.91 - ETA: 16s - loss: 0.2071 - acc: 0.91 - ETA: 16s - loss: 0.2075 - acc: 0.91 - ETA: 16s - loss: 0.2069 - acc: 0.91 - ETA: 16s - loss: 0.2074 - acc: 0.91 - ETA: 16s - loss: 0.2080 - acc: 0.91 - ETA: 16s - loss: 0.2074 - acc: 0.91 - ETA: 16s - loss: 0.2067 - acc: 0.91 - ETA: 16s - loss: 0.2067 - acc: 0.91 - ETA: 15s - loss: 0.2061 - acc: 0.91 - ETA: 15s - loss: 0.2062 - acc: 0.91 - ETA: 15s - loss: 0.2056 - acc: 0.91 - ETA: 15s - loss: 0.2051 - acc: 0.91 - ETA: 15s - loss: 0.2056 - acc: 0.91 - ETA: 15s - loss: 0.2053 - acc: 0.91 - ETA: 15s - loss: 0.2052 - acc: 0.91 - ETA: 15s - loss: 0.2054 - acc: 0.91 - ETA: 15s - loss: 0.2063 - acc: 0.91 - ETA: 15s - loss: 0.2062 - acc: 0.91 - ETA: 15s - loss: 0.2055 - acc: 0.91 - ETA: 15s - loss: 0.2057 - acc: 0.91 - ETA: 15s - loss: 0.2058 - acc: 0.91 - ETA: 15s - loss: 0.2061 - acc: 0.91 - ETA: 14s - loss: 0.2055 - acc: 0.91 - ETA: 14s - loss: 0.2051 - acc: 0.91 - ETA: 14s - loss: 0.2049 - acc: 0.91 - ETA: 14s - loss: 0.2053 - acc: 0.91 - ETA: 14s - loss: 0.2052 - acc: 0.91 - ETA: 14s - loss: 0.2045 - acc: 0.91 - ETA: 14s - loss: 0.2051 - acc: 0.91 - ETA: 14s - loss: 0.2051 - acc: 0.91 - ETA: 14s - loss: 0.2054 - acc: 0.91 - ETA: 14s - loss: 0.2053 - acc: 0.91 - ETA: 14s - loss: 0.2055 - acc: 0.91 - ETA: 14s - loss: 0.2056 - acc: 0.91 - ETA: 14s - loss: 0.2051 - acc: 0.91 - ETA: 13s - loss: 0.2049 - acc: 0.91 - ETA: 13s - loss: 0.2051 - acc: 0.91 - ETA: 13s - loss: 0.2050 - acc: 0.91 - ETA: 13s - loss: 0.2052 - acc: 0.91 - ETA: 13s - loss: 0.2053 - acc: 0.91 - ETA: 13s - loss: 0.2059 - acc: 0.91 - ETA: 13s - loss: 0.2061 - acc: 0.91 - ETA: 13s - loss: 0.2065 - acc: 0.91 - ETA: 13s - loss: 0.2066 - acc: 0.91 - ETA: 13s - loss: 0.2066 - acc: 0.91 - ETA: 13s - loss: 0.2062 - acc: 0.91 - ETA: 13s - loss: 0.2063 - acc: 0.91 - ETA: 13s - loss: 0.2062 - acc: 0.91 - ETA: 12s - loss: 0.2064 - acc: 0.91 - ETA: 12s - loss: 0.2064 - acc: 0.91 - ETA: 12s - loss: 0.2062 - acc: 0.91 - ETA: 12s - loss: 0.2061 - acc: 0.91 - ETA: 12s - loss: 0.2065 - acc: 0.91 - ETA: 12s - loss: 0.2064 - acc: 0.91 - ETA: 12s - loss: 0.2063 - acc: 0.91 - ETA: 12s - loss: 0.2060 - acc: 0.91 - ETA: 12s - loss: 0.2065 - acc: 0.91 - ETA: 12s - loss: 0.2067 - acc: 0.91 - ETA: 12s - loss: 0.2069 - acc: 0.91 - ETA: 12s - loss: 0.2075 - acc: 0.91 - ETA: 12s - loss: 0.2075 - acc: 0.91 - ETA: 12s - loss: 0.2076 - acc: 0.91 - ETA: 11s - loss: 0.2084 - acc: 0.91 - ETA: 11s - loss: 0.2079 - acc: 0.91 - ETA: 11s - loss: 0.2084 - acc: 0.91 - ETA: 11s - loss: 0.2086 - acc: 0.91 - ETA: 11s - loss: 0.2084 - acc: 0.91 - ETA: 11s - loss: 0.2084 - acc: 0.91 - ETA: 11s - loss: 0.2086 - acc: 0.91 - ETA: 11s - loss: 0.2088 - acc: 0.91 - ETA: 11s - loss: 0.2088 - acc: 0.91 - ETA: 11s - loss: 0.2087 - acc: 0.91 - ETA: 11s - loss: 0.2088 - acc: 0.91 - ETA: 11s - loss: 0.2083 - acc: 0.91 - ETA: 11s - loss: 0.2080 - acc: 0.91 - ETA: 10s - loss: 0.2081 - acc: 0.91 - ETA: 10s - loss: 0.2087 - acc: 0.91 - ETA: 10s - loss: 0.2087 - acc: 0.91 - ETA: 10s - loss: 0.2085 - acc: 0.91 - ETA: 10s - loss: 0.2084 - acc: 0.91 - ETA: 10s - loss: 0.2086 - acc: 0.91 - ETA: 10s - loss: 0.2092 - acc: 0.91 - ETA: 10s - loss: 0.2092 - acc: 0.91 - ETA: 10s - loss: 0.2092 - acc: 0.91 - ETA: 10s - loss: 0.2098 - acc: 0.91 - ETA: 10s - loss: 0.2097 - acc: 0.91 - ETA: 10s - loss: 0.2092 - acc: 0.91 - ETA: 10s - loss: 0.2094 - acc: 0.91 - ETA: 10s - loss: 0.2095 - acc: 0.91 - ETA: 9s - loss: 0.2094 - acc: 0.9119 - ETA: 9s - loss: 0.2091 - acc: 0.912 - ETA: 9s - loss: 0.2090 - acc: 0.912 - ETA: 9s - loss: 0.2092 - acc: 0.912 - ETA: 9s - loss: 0.2088 - acc: 0.912 - ETA: 9s - loss: 0.2085 - acc: 0.912 - ETA: 9s - loss: 0.2085 - acc: 0.912 - ETA: 9s - loss: 0.2085 - acc: 0.912 - ETA: 9s - loss: 0.2084 - acc: 0.912 - ETA: 9s - loss: 0.2080 - acc: 0.912 - ETA: 9s - loss: 0.2085 - acc: 0.912 - ETA: 9s - loss: 0.2083 - acc: 0.912 - ETA: 9s - loss: 0.2084 - acc: 0.912 - ETA: 8s - loss: 0.2085 - acc: 0.912 - ETA: 8s - loss: 0.2086 - acc: 0.911 - ETA: 8s - loss: 0.2087 - acc: 0.911 - ETA: 8s - loss: 0.2086 - acc: 0.911 - ETA: 8s - loss: 0.2087 - acc: 0.911 - ETA: 8s - loss: 0.2088 - acc: 0.911 - ETA: 8s - loss: 0.2088 - acc: 0.911 - ETA: 8s - loss: 0.2088 - acc: 0.911 - ETA: 8s - loss: 0.2089 - acc: 0.911 - ETA: 8s - loss: 0.2088 - acc: 0.911 - ETA: 8s - loss: 0.2090 - acc: 0.911 - ETA: 8s - loss: 0.2086 - acc: 0.911 - ETA: 8s - loss: 0.2083 - acc: 0.911 - ETA: 8s - loss: 0.2087 - acc: 0.911 - ETA: 7s - loss: 0.2091 - acc: 0.911 - ETA: 7s - loss: 0.2090 - acc: 0.911 - ETA: 7s - loss: 0.2089 - acc: 0.911 - ETA: 7s - loss: 0.2090 - acc: 0.911 - ETA: 7s - loss: 0.2091 - acc: 0.911 - ETA: 7s - loss: 0.2090 - acc: 0.9119"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31500/31500 [==============================] - ETA: 7s - loss: 0.2089 - acc: 0.912 - ETA: 7s - loss: 0.2093 - acc: 0.911 - ETA: 7s - loss: 0.2095 - acc: 0.911 - ETA: 7s - loss: 0.2094 - acc: 0.912 - ETA: 7s - loss: 0.2097 - acc: 0.911 - ETA: 7s - loss: 0.2100 - acc: 0.911 - ETA: 7s - loss: 0.2099 - acc: 0.911 - ETA: 6s - loss: 0.2099 - acc: 0.911 - ETA: 6s - loss: 0.2101 - acc: 0.911 - ETA: 6s - loss: 0.2100 - acc: 0.911 - ETA: 6s - loss: 0.2100 - acc: 0.911 - ETA: 6s - loss: 0.2102 - acc: 0.911 - ETA: 6s - loss: 0.2100 - acc: 0.911 - ETA: 6s - loss: 0.2105 - acc: 0.911 - ETA: 6s - loss: 0.2107 - acc: 0.911 - ETA: 6s - loss: 0.2106 - acc: 0.911 - ETA: 6s - loss: 0.2105 - acc: 0.911 - ETA: 6s - loss: 0.2105 - acc: 0.911 - ETA: 6s - loss: 0.2102 - acc: 0.911 - ETA: 6s - loss: 0.2102 - acc: 0.911 - ETA: 5s - loss: 0.2102 - acc: 0.911 - ETA: 5s - loss: 0.2107 - acc: 0.911 - ETA: 5s - loss: 0.2108 - acc: 0.911 - ETA: 5s - loss: 0.2106 - acc: 0.911 - ETA: 5s - loss: 0.2104 - acc: 0.911 - ETA: 5s - loss: 0.2102 - acc: 0.911 - ETA: 5s - loss: 0.2101 - acc: 0.911 - ETA: 5s - loss: 0.2104 - acc: 0.911 - ETA: 5s - loss: 0.2111 - acc: 0.911 - ETA: 5s - loss: 0.2117 - acc: 0.911 - ETA: 5s - loss: 0.2120 - acc: 0.911 - ETA: 5s - loss: 0.2121 - acc: 0.911 - ETA: 5s - loss: 0.2126 - acc: 0.910 - ETA: 4s - loss: 0.2126 - acc: 0.910 - ETA: 4s - loss: 0.2124 - acc: 0.911 - ETA: 4s - loss: 0.2128 - acc: 0.910 - ETA: 4s - loss: 0.2132 - acc: 0.910 - ETA: 4s - loss: 0.2135 - acc: 0.910 - ETA: 4s - loss: 0.2135 - acc: 0.910 - ETA: 4s - loss: 0.2135 - acc: 0.910 - ETA: 4s - loss: 0.2133 - acc: 0.910 - ETA: 4s - loss: 0.2132 - acc: 0.910 - ETA: 4s - loss: 0.2132 - acc: 0.910 - ETA: 4s - loss: 0.2133 - acc: 0.910 - ETA: 4s - loss: 0.2133 - acc: 0.910 - ETA: 4s - loss: 0.2132 - acc: 0.910 - ETA: 3s - loss: 0.2135 - acc: 0.910 - ETA: 3s - loss: 0.2135 - acc: 0.910 - ETA: 3s - loss: 0.2136 - acc: 0.910 - ETA: 3s - loss: 0.2133 - acc: 0.910 - ETA: 3s - loss: 0.2133 - acc: 0.910 - ETA: 3s - loss: 0.2135 - acc: 0.910 - ETA: 3s - loss: 0.2136 - acc: 0.910 - ETA: 3s - loss: 0.2133 - acc: 0.910 - ETA: 3s - loss: 0.2132 - acc: 0.910 - ETA: 3s - loss: 0.2130 - acc: 0.910 - ETA: 3s - loss: 0.2129 - acc: 0.910 - ETA: 3s - loss: 0.2131 - acc: 0.910 - ETA: 3s - loss: 0.2133 - acc: 0.910 - ETA: 3s - loss: 0.2135 - acc: 0.910 - ETA: 2s - loss: 0.2135 - acc: 0.910 - ETA: 2s - loss: 0.2136 - acc: 0.910 - ETA: 2s - loss: 0.2134 - acc: 0.910 - ETA: 2s - loss: 0.2134 - acc: 0.910 - ETA: 2s - loss: 0.2136 - acc: 0.910 - ETA: 2s - loss: 0.2138 - acc: 0.910 - ETA: 2s - loss: 0.2139 - acc: 0.910 - ETA: 2s - loss: 0.2138 - acc: 0.910 - ETA: 2s - loss: 0.2140 - acc: 0.910 - ETA: 2s - loss: 0.2139 - acc: 0.910 - ETA: 2s - loss: 0.2138 - acc: 0.910 - ETA: 2s - loss: 0.2137 - acc: 0.910 - ETA: 2s - loss: 0.2137 - acc: 0.910 - ETA: 1s - loss: 0.2137 - acc: 0.910 - ETA: 1s - loss: 0.2138 - acc: 0.910 - ETA: 1s - loss: 0.2140 - acc: 0.910 - ETA: 1s - loss: 0.2142 - acc: 0.910 - ETA: 1s - loss: 0.2144 - acc: 0.910 - ETA: 1s - loss: 0.2148 - acc: 0.910 - ETA: 1s - loss: 0.2147 - acc: 0.910 - ETA: 1s - loss: 0.2147 - acc: 0.910 - ETA: 1s - loss: 0.2149 - acc: 0.909 - ETA: 1s - loss: 0.2150 - acc: 0.910 - ETA: 1s - loss: 0.2149 - acc: 0.910 - ETA: 1s - loss: 0.2150 - acc: 0.909 - ETA: 1s - loss: 0.2153 - acc: 0.909 - ETA: 0s - loss: 0.2153 - acc: 0.909 - ETA: 0s - loss: 0.2157 - acc: 0.909 - ETA: 0s - loss: 0.2157 - acc: 0.909 - ETA: 0s - loss: 0.2157 - acc: 0.909 - ETA: 0s - loss: 0.2159 - acc: 0.909 - ETA: 0s - loss: 0.2158 - acc: 0.909 - ETA: 0s - loss: 0.2158 - acc: 0.909 - ETA: 0s - loss: 0.2157 - acc: 0.909 - ETA: 0s - loss: 0.2156 - acc: 0.909 - ETA: 0s - loss: 0.2156 - acc: 0.909 - ETA: 0s - loss: 0.2158 - acc: 0.909 - ETA: 0s - loss: 0.2157 - acc: 0.909 - ETA: 0s - loss: 0.2160 - acc: 0.909 - 24s 764us/step - loss: 0.2159 - acc: 0.9097 - val_loss: 0.3222 - val_acc: 0.8709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14b98c5cd30>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will now be training our model on our training reviews dataset of word2vec features represented\n",
    "#by avg_wv_train_features. We will be using the fit(...) function from keras for the training\n",
    "#process and there are some parameters which you should be aware of. The epoch parameter indicates one\n",
    "#complete forward and backward pass of all the training examples through the network. The batch_size\n",
    "#parameter indicates the total number of samples which are propagated through the DNN model at a time\n",
    "#for one backward and forward pass for training the model and updating the gradient. Thus if you have 1,000\n",
    "#observations and your batch size is 100, each epoch will consist of 10 iterations where 100 observations will\n",
    "#be passed through the network at a time and the weights on the hidden layer units will be updated.\n",
    "#We also specify a validation_split of 0.1 to extract 10% of the training data and use it as a validation dataset for\n",
    "#evaluating the performance at each epoch. The shuffle parameter helps shuffle the samples in each epoch\n",
    "#when training the model.\n",
    "batch_size = 100\n",
    "w2v_dnn.fit(avg_wv_train_features, y_train, epochs=5, batch_size=batch_size, \n",
    "            shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RUIZ\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#The preceding snippet tells us that we have trained our DNN model on the training data for five epochs\n",
    "#with 100 as the batch size. We get a validation accuracy of close to 88%, which is quite good. Time now to put\n",
    "#our model to the real test! Let’s evaluate our model performance on the test review word2vec features\n",
    "y_pred = w2v_dnn.predict_classes(avg_wv_test_features)\n",
    "predictions = le.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8799\n",
      "Precision: 0.8803\n",
      "Recall: 0.8799\n",
      "F1 Score: 0.8799\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.87      0.89      0.88      7510\n",
      "   negative       0.89      0.87      0.88      7490\n",
      "\n",
      "avg / total       0.88      0.88      0.88     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6720      790\n",
      "        negative       1011     6479\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The results depicted show us that we have obtained a model accuracy and F1-score of\n",
    "#88%, which is great! You can use a similar workflow to build and train a DNN model for our GloVe based\n",
    "#features and evaluate the model performance.\n",
    "\n",
    "# build DNN model\n",
    "glove_dnn = construct_deepnn_architecture(num_input_features=384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31500 samples, validate on 3500 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21200/31500 [===================>..........] - ETA: 13:10 - loss: 0.9542 - acc: 0.44 - ETA: 6:44 - loss: 1.4249 - acc: 0.4650 - ETA: 4:36 - loss: 1.2965 - acc: 0.483 - ETA: 3:32 - loss: 1.1597 - acc: 0.467 - ETA: 2:54 - loss: 1.0878 - acc: 0.470 - ETA: 2:27 - loss: 1.0389 - acc: 0.481 - ETA: 2:09 - loss: 1.0065 - acc: 0.477 - ETA: 1:55 - loss: 0.9666 - acc: 0.486 - ETA: 1:44 - loss: 0.9333 - acc: 0.502 - ETA: 1:36 - loss: 0.9088 - acc: 0.511 - ETA: 1:29 - loss: 0.8890 - acc: 0.512 - ETA: 1:23 - loss: 0.8728 - acc: 0.511 - ETA: 1:18 - loss: 0.8585 - acc: 0.513 - ETA: 1:13 - loss: 0.8474 - acc: 0.512 - ETA: 1:09 - loss: 0.8364 - acc: 0.516 - ETA: 1:06 - loss: 0.8258 - acc: 0.524 - ETA: 1:03 - loss: 0.8174 - acc: 0.525 - ETA: 1:01 - loss: 0.8086 - acc: 0.529 - ETA: 58s - loss: 0.8033 - acc: 0.528 - ETA: 56s - loss: 0.7982 - acc: 0.52 - ETA: 54s - loss: 0.7929 - acc: 0.52 - ETA: 53s - loss: 0.7884 - acc: 0.53 - ETA: 51s - loss: 0.7842 - acc: 0.52 - ETA: 50s - loss: 0.7801 - acc: 0.53 - ETA: 48s - loss: 0.7772 - acc: 0.52 - ETA: 47s - loss: 0.7733 - acc: 0.53 - ETA: 46s - loss: 0.7690 - acc: 0.53 - ETA: 45s - loss: 0.7639 - acc: 0.53 - ETA: 44s - loss: 0.7622 - acc: 0.53 - ETA: 43s - loss: 0.7607 - acc: 0.53 - ETA: 42s - loss: 0.7578 - acc: 0.54 - ETA: 41s - loss: 0.7548 - acc: 0.54 - ETA: 40s - loss: 0.7534 - acc: 0.54 - ETA: 40s - loss: 0.7521 - acc: 0.54 - ETA: 39s - loss: 0.7504 - acc: 0.54 - ETA: 38s - loss: 0.7483 - acc: 0.54 - ETA: 37s - loss: 0.7475 - acc: 0.54 - ETA: 37s - loss: 0.7461 - acc: 0.54 - ETA: 36s - loss: 0.7439 - acc: 0.54 - ETA: 36s - loss: 0.7423 - acc: 0.54 - ETA: 35s - loss: 0.7401 - acc: 0.54 - ETA: 35s - loss: 0.7381 - acc: 0.54 - ETA: 34s - loss: 0.7365 - acc: 0.54 - ETA: 34s - loss: 0.7351 - acc: 0.54 - ETA: 33s - loss: 0.7336 - acc: 0.54 - ETA: 33s - loss: 0.7323 - acc: 0.54 - ETA: 32s - loss: 0.7317 - acc: 0.54 - ETA: 32s - loss: 0.7317 - acc: 0.54 - ETA: 32s - loss: 0.7303 - acc: 0.54 - ETA: 31s - loss: 0.7295 - acc: 0.54 - ETA: 31s - loss: 0.7281 - acc: 0.54 - ETA: 31s - loss: 0.7271 - acc: 0.55 - ETA: 30s - loss: 0.7254 - acc: 0.55 - ETA: 30s - loss: 0.7244 - acc: 0.55 - ETA: 30s - loss: 0.7235 - acc: 0.55 - ETA: 29s - loss: 0.7228 - acc: 0.55 - ETA: 29s - loss: 0.7220 - acc: 0.55 - ETA: 29s - loss: 0.7210 - acc: 0.55 - ETA: 29s - loss: 0.7203 - acc: 0.55 - ETA: 28s - loss: 0.7193 - acc: 0.55 - ETA: 28s - loss: 0.7186 - acc: 0.55 - ETA: 28s - loss: 0.7174 - acc: 0.55 - ETA: 27s - loss: 0.7167 - acc: 0.55 - ETA: 27s - loss: 0.7158 - acc: 0.55 - ETA: 27s - loss: 0.7153 - acc: 0.55 - ETA: 27s - loss: 0.7145 - acc: 0.55 - ETA: 26s - loss: 0.7133 - acc: 0.55 - ETA: 26s - loss: 0.7132 - acc: 0.55 - ETA: 26s - loss: 0.7120 - acc: 0.56 - ETA: 26s - loss: 0.7116 - acc: 0.56 - ETA: 25s - loss: 0.7111 - acc: 0.56 - ETA: 25s - loss: 0.7105 - acc: 0.56 - ETA: 25s - loss: 0.7101 - acc: 0.56 - ETA: 25s - loss: 0.7094 - acc: 0.56 - ETA: 25s - loss: 0.7089 - acc: 0.56 - ETA: 24s - loss: 0.7079 - acc: 0.56 - ETA: 24s - loss: 0.7077 - acc: 0.56 - ETA: 24s - loss: 0.7071 - acc: 0.56 - ETA: 24s - loss: 0.7065 - acc: 0.56 - ETA: 24s - loss: 0.7056 - acc: 0.56 - ETA: 23s - loss: 0.7049 - acc: 0.56 - ETA: 23s - loss: 0.7052 - acc: 0.56 - ETA: 23s - loss: 0.7047 - acc: 0.56 - ETA: 23s - loss: 0.7040 - acc: 0.56 - ETA: 23s - loss: 0.7033 - acc: 0.56 - ETA: 23s - loss: 0.7030 - acc: 0.56 - ETA: 23s - loss: 0.7027 - acc: 0.56 - ETA: 22s - loss: 0.7022 - acc: 0.56 - ETA: 22s - loss: 0.7020 - acc: 0.56 - ETA: 22s - loss: 0.7012 - acc: 0.56 - ETA: 22s - loss: 0.7007 - acc: 0.56 - ETA: 22s - loss: 0.7003 - acc: 0.56 - ETA: 22s - loss: 0.6999 - acc: 0.56 - ETA: 21s - loss: 0.6999 - acc: 0.56 - ETA: 21s - loss: 0.6994 - acc: 0.57 - ETA: 21s - loss: 0.6990 - acc: 0.57 - ETA: 21s - loss: 0.6986 - acc: 0.57 - ETA: 21s - loss: 0.6982 - acc: 0.57 - ETA: 21s - loss: 0.6979 - acc: 0.57 - ETA: 20s - loss: 0.6977 - acc: 0.57 - ETA: 20s - loss: 0.6972 - acc: 0.57 - ETA: 20s - loss: 0.6970 - acc: 0.57 - ETA: 20s - loss: 0.6966 - acc: 0.57 - ETA: 20s - loss: 0.6964 - acc: 0.57 - ETA: 20s - loss: 0.6964 - acc: 0.57 - ETA: 20s - loss: 0.6964 - acc: 0.57 - ETA: 20s - loss: 0.6960 - acc: 0.57 - ETA: 19s - loss: 0.6957 - acc: 0.57 - ETA: 19s - loss: 0.6955 - acc: 0.57 - ETA: 19s - loss: 0.6955 - acc: 0.57 - ETA: 19s - loss: 0.6951 - acc: 0.57 - ETA: 19s - loss: 0.6946 - acc: 0.57 - ETA: 19s - loss: 0.6940 - acc: 0.57 - ETA: 19s - loss: 0.6936 - acc: 0.57 - ETA: 18s - loss: 0.6933 - acc: 0.57 - ETA: 18s - loss: 0.6928 - acc: 0.57 - ETA: 18s - loss: 0.6926 - acc: 0.57 - ETA: 18s - loss: 0.6920 - acc: 0.57 - ETA: 18s - loss: 0.6916 - acc: 0.57 - ETA: 18s - loss: 0.6913 - acc: 0.57 - ETA: 18s - loss: 0.6908 - acc: 0.57 - ETA: 18s - loss: 0.6908 - acc: 0.57 - ETA: 17s - loss: 0.6904 - acc: 0.57 - ETA: 17s - loss: 0.6905 - acc: 0.57 - ETA: 17s - loss: 0.6897 - acc: 0.58 - ETA: 17s - loss: 0.6896 - acc: 0.58 - ETA: 17s - loss: 0.6891 - acc: 0.58 - ETA: 17s - loss: 0.6884 - acc: 0.58 - ETA: 17s - loss: 0.6877 - acc: 0.58 - ETA: 17s - loss: 0.6873 - acc: 0.58 - ETA: 17s - loss: 0.6867 - acc: 0.58 - ETA: 16s - loss: 0.6865 - acc: 0.58 - ETA: 16s - loss: 0.6862 - acc: 0.58 - ETA: 16s - loss: 0.6856 - acc: 0.58 - ETA: 16s - loss: 0.6853 - acc: 0.58 - ETA: 16s - loss: 0.6844 - acc: 0.58 - ETA: 16s - loss: 0.6839 - acc: 0.58 - ETA: 16s - loss: 0.6836 - acc: 0.58 - ETA: 16s - loss: 0.6832 - acc: 0.58 - ETA: 16s - loss: 0.6825 - acc: 0.58 - ETA: 15s - loss: 0.6821 - acc: 0.58 - ETA: 15s - loss: 0.6817 - acc: 0.58 - ETA: 15s - loss: 0.6817 - acc: 0.58 - ETA: 15s - loss: 0.6811 - acc: 0.59 - ETA: 15s - loss: 0.6809 - acc: 0.59 - ETA: 15s - loss: 0.6806 - acc: 0.59 - ETA: 15s - loss: 0.6809 - acc: 0.59 - ETA: 15s - loss: 0.6804 - acc: 0.59 - ETA: 15s - loss: 0.6801 - acc: 0.59 - ETA: 14s - loss: 0.6795 - acc: 0.59 - ETA: 14s - loss: 0.6795 - acc: 0.59 - ETA: 14s - loss: 0.6791 - acc: 0.59 - ETA: 14s - loss: 0.6786 - acc: 0.59 - ETA: 14s - loss: 0.6778 - acc: 0.59 - ETA: 14s - loss: 0.6776 - acc: 0.59 - ETA: 14s - loss: 0.6772 - acc: 0.59 - ETA: 14s - loss: 0.6772 - acc: 0.59 - ETA: 14s - loss: 0.6769 - acc: 0.59 - ETA: 14s - loss: 0.6768 - acc: 0.59 - ETA: 13s - loss: 0.6768 - acc: 0.59 - ETA: 13s - loss: 0.6764 - acc: 0.59 - ETA: 13s - loss: 0.6764 - acc: 0.59 - ETA: 13s - loss: 0.6753 - acc: 0.59 - ETA: 13s - loss: 0.6752 - acc: 0.59 - ETA: 13s - loss: 0.6753 - acc: 0.59 - ETA: 13s - loss: 0.6746 - acc: 0.59 - ETA: 13s - loss: 0.6744 - acc: 0.59 - ETA: 13s - loss: 0.6743 - acc: 0.59 - ETA: 13s - loss: 0.6738 - acc: 0.59 - ETA: 12s - loss: 0.6735 - acc: 0.59 - ETA: 12s - loss: 0.6734 - acc: 0.59 - ETA: 12s - loss: 0.6729 - acc: 0.59 - ETA: 12s - loss: 0.6728 - acc: 0.59 - ETA: 12s - loss: 0.6728 - acc: 0.59 - ETA: 12s - loss: 0.6725 - acc: 0.59 - ETA: 12s - loss: 0.6724 - acc: 0.59 - ETA: 12s - loss: 0.6718 - acc: 0.60 - ETA: 12s - loss: 0.6715 - acc: 0.60 - ETA: 12s - loss: 0.6711 - acc: 0.60 - ETA: 11s - loss: 0.6705 - acc: 0.60 - ETA: 11s - loss: 0.6700 - acc: 0.60 - ETA: 11s - loss: 0.6699 - acc: 0.60 - ETA: 11s - loss: 0.6698 - acc: 0.60 - ETA: 11s - loss: 0.6697 - acc: 0.60 - ETA: 11s - loss: 0.6693 - acc: 0.60 - ETA: 11s - loss: 0.6692 - acc: 0.60 - ETA: 11s - loss: 0.6690 - acc: 0.60 - ETA: 11s - loss: 0.6685 - acc: 0.60 - ETA: 11s - loss: 0.6680 - acc: 0.60 - ETA: 11s - loss: 0.6679 - acc: 0.60 - ETA: 10s - loss: 0.6675 - acc: 0.60 - ETA: 10s - loss: 0.6671 - acc: 0.60 - ETA: 10s - loss: 0.6667 - acc: 0.60 - ETA: 10s - loss: 0.6664 - acc: 0.60 - ETA: 10s - loss: 0.6663 - acc: 0.60 - ETA: 10s - loss: 0.6659 - acc: 0.60 - ETA: 10s - loss: 0.6658 - acc: 0.60 - ETA: 10s - loss: 0.6658 - acc: 0.60 - ETA: 10s - loss: 0.6656 - acc: 0.60 - ETA: 10s - loss: 0.6652 - acc: 0.60 - ETA: 10s - loss: 0.6647 - acc: 0.60 - ETA: 9s - loss: 0.6645 - acc: 0.6089 - ETA: 9s - loss: 0.6641 - acc: 0.609 - ETA: 9s - loss: 0.6639 - acc: 0.609 - ETA: 9s - loss: 0.6638 - acc: 0.609 - ETA: 9s - loss: 0.6634 - acc: 0.610 - ETA: 9s - loss: 0.6626 - acc: 0.610 - ETA: 9s - loss: 0.6628 - acc: 0.610 - ETA: 9s - loss: 0.6624 - acc: 0.611 - ETA: 9s - loss: 0.6624 - acc: 0.611 - ETA: 9s - loss: 0.6623 - acc: 0.611 - ETA: 9s - loss: 0.6624 - acc: 0.6113\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31500/31500 [==============================] - ETA: 8s - loss: 0.6621 - acc: 0.611 - ETA: 8s - loss: 0.6621 - acc: 0.611 - ETA: 8s - loss: 0.6620 - acc: 0.611 - ETA: 8s - loss: 0.6618 - acc: 0.611 - ETA: 8s - loss: 0.6616 - acc: 0.611 - ETA: 8s - loss: 0.6614 - acc: 0.612 - ETA: 8s - loss: 0.6613 - acc: 0.612 - ETA: 8s - loss: 0.6613 - acc: 0.612 - ETA: 8s - loss: 0.6611 - acc: 0.612 - ETA: 8s - loss: 0.6609 - acc: 0.612 - ETA: 7s - loss: 0.6611 - acc: 0.612 - ETA: 7s - loss: 0.6610 - acc: 0.612 - ETA: 7s - loss: 0.6609 - acc: 0.612 - ETA: 7s - loss: 0.6608 - acc: 0.612 - ETA: 7s - loss: 0.6604 - acc: 0.613 - ETA: 7s - loss: 0.6601 - acc: 0.613 - ETA: 7s - loss: 0.6599 - acc: 0.614 - ETA: 7s - loss: 0.6598 - acc: 0.614 - ETA: 7s - loss: 0.6597 - acc: 0.614 - ETA: 7s - loss: 0.6594 - acc: 0.614 - ETA: 7s - loss: 0.6594 - acc: 0.614 - ETA: 6s - loss: 0.6595 - acc: 0.614 - ETA: 6s - loss: 0.6591 - acc: 0.614 - ETA: 6s - loss: 0.6588 - acc: 0.615 - ETA: 6s - loss: 0.6585 - acc: 0.615 - ETA: 6s - loss: 0.6583 - acc: 0.615 - ETA: 6s - loss: 0.6582 - acc: 0.615 - ETA: 6s - loss: 0.6587 - acc: 0.615 - ETA: 6s - loss: 0.6585 - acc: 0.615 - ETA: 6s - loss: 0.6580 - acc: 0.615 - ETA: 6s - loss: 0.6580 - acc: 0.615 - ETA: 6s - loss: 0.6581 - acc: 0.615 - ETA: 6s - loss: 0.6576 - acc: 0.616 - ETA: 5s - loss: 0.6576 - acc: 0.616 - ETA: 5s - loss: 0.6575 - acc: 0.616 - ETA: 5s - loss: 0.6573 - acc: 0.617 - ETA: 5s - loss: 0.6572 - acc: 0.616 - ETA: 5s - loss: 0.6570 - acc: 0.617 - ETA: 5s - loss: 0.6571 - acc: 0.616 - ETA: 5s - loss: 0.6571 - acc: 0.617 - ETA: 5s - loss: 0.6569 - acc: 0.617 - ETA: 5s - loss: 0.6568 - acc: 0.617 - ETA: 5s - loss: 0.6566 - acc: 0.617 - ETA: 5s - loss: 0.6564 - acc: 0.618 - ETA: 4s - loss: 0.6562 - acc: 0.618 - ETA: 4s - loss: 0.6559 - acc: 0.618 - ETA: 4s - loss: 0.6555 - acc: 0.619 - ETA: 4s - loss: 0.6555 - acc: 0.619 - ETA: 4s - loss: 0.6554 - acc: 0.619 - ETA: 4s - loss: 0.6552 - acc: 0.619 - ETA: 4s - loss: 0.6551 - acc: 0.619 - ETA: 4s - loss: 0.6547 - acc: 0.620 - ETA: 4s - loss: 0.6545 - acc: 0.620 - ETA: 4s - loss: 0.6544 - acc: 0.620 - ETA: 4s - loss: 0.6543 - acc: 0.620 - ETA: 4s - loss: 0.6539 - acc: 0.620 - ETA: 3s - loss: 0.6538 - acc: 0.620 - ETA: 3s - loss: 0.6535 - acc: 0.621 - ETA: 3s - loss: 0.6535 - acc: 0.621 - ETA: 3s - loss: 0.6531 - acc: 0.621 - ETA: 3s - loss: 0.6528 - acc: 0.621 - ETA: 3s - loss: 0.6527 - acc: 0.621 - ETA: 3s - loss: 0.6526 - acc: 0.622 - ETA: 3s - loss: 0.6527 - acc: 0.621 - ETA: 3s - loss: 0.6526 - acc: 0.622 - ETA: 3s - loss: 0.6523 - acc: 0.622 - ETA: 3s - loss: 0.6522 - acc: 0.622 - ETA: 2s - loss: 0.6519 - acc: 0.622 - ETA: 2s - loss: 0.6523 - acc: 0.622 - ETA: 2s - loss: 0.6521 - acc: 0.622 - ETA: 2s - loss: 0.6520 - acc: 0.622 - ETA: 2s - loss: 0.6519 - acc: 0.622 - ETA: 2s - loss: 0.6518 - acc: 0.622 - ETA: 2s - loss: 0.6519 - acc: 0.622 - ETA: 2s - loss: 0.6518 - acc: 0.622 - ETA: 2s - loss: 0.6514 - acc: 0.623 - ETA: 2s - loss: 0.6515 - acc: 0.623 - ETA: 2s - loss: 0.6515 - acc: 0.623 - ETA: 2s - loss: 0.6514 - acc: 0.623 - ETA: 1s - loss: 0.6513 - acc: 0.623 - ETA: 1s - loss: 0.6512 - acc: 0.623 - ETA: 1s - loss: 0.6511 - acc: 0.623 - ETA: 1s - loss: 0.6508 - acc: 0.624 - ETA: 1s - loss: 0.6506 - acc: 0.624 - ETA: 1s - loss: 0.6505 - acc: 0.624 - ETA: 1s - loss: 0.6504 - acc: 0.624 - ETA: 1s - loss: 0.6504 - acc: 0.624 - ETA: 1s - loss: 0.6503 - acc: 0.624 - ETA: 1s - loss: 0.6501 - acc: 0.625 - ETA: 1s - loss: 0.6499 - acc: 0.625 - ETA: 1s - loss: 0.6497 - acc: 0.625 - ETA: 0s - loss: 0.6495 - acc: 0.625 - ETA: 0s - loss: 0.6493 - acc: 0.626 - ETA: 0s - loss: 0.6489 - acc: 0.626 - ETA: 0s - loss: 0.6486 - acc: 0.627 - ETA: 0s - loss: 0.6484 - acc: 0.627 - ETA: 0s - loss: 0.6487 - acc: 0.626 - ETA: 0s - loss: 0.6486 - acc: 0.626 - ETA: 0s - loss: 0.6484 - acc: 0.627 - ETA: 0s - loss: 0.6485 - acc: 0.627 - ETA: 0s - loss: 0.6484 - acc: 0.627 - ETA: 0s - loss: 0.6482 - acc: 0.627 - 27s 860us/step - loss: 0.6482 - acc: 0.6276 - val_loss: 0.6544 - val_acc: 0.6094\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21500/31500 [===================>..........] - ETA: 18s - loss: 0.6082 - acc: 0.65 - ETA: 23s - loss: 0.6339 - acc: 0.63 - ETA: 24s - loss: 0.6394 - acc: 0.62 - ETA: 24s - loss: 0.6242 - acc: 0.63 - ETA: 24s - loss: 0.6146 - acc: 0.65 - ETA: 24s - loss: 0.6302 - acc: 0.63 - ETA: 24s - loss: 0.6339 - acc: 0.63 - ETA: 25s - loss: 0.6309 - acc: 0.63 - ETA: 24s - loss: 0.6360 - acc: 0.62 - ETA: 24s - loss: 0.6351 - acc: 0.63 - ETA: 24s - loss: 0.6324 - acc: 0.63 - ETA: 24s - loss: 0.6328 - acc: 0.63 - ETA: 23s - loss: 0.6325 - acc: 0.63 - ETA: 23s - loss: 0.6317 - acc: 0.64 - ETA: 23s - loss: 0.6299 - acc: 0.64 - ETA: 23s - loss: 0.6289 - acc: 0.64 - ETA: 23s - loss: 0.6257 - acc: 0.65 - ETA: 23s - loss: 0.6242 - acc: 0.65 - ETA: 22s - loss: 0.6220 - acc: 0.65 - ETA: 22s - loss: 0.6227 - acc: 0.65 - ETA: 22s - loss: 0.6188 - acc: 0.66 - ETA: 22s - loss: 0.6162 - acc: 0.66 - ETA: 23s - loss: 0.6133 - acc: 0.67 - ETA: 23s - loss: 0.6131 - acc: 0.67 - ETA: 23s - loss: 0.6138 - acc: 0.67 - ETA: 23s - loss: 0.6136 - acc: 0.66 - ETA: 23s - loss: 0.6134 - acc: 0.67 - ETA: 23s - loss: 0.6110 - acc: 0.67 - ETA: 23s - loss: 0.6134 - acc: 0.67 - ETA: 22s - loss: 0.6135 - acc: 0.67 - ETA: 22s - loss: 0.6109 - acc: 0.67 - ETA: 22s - loss: 0.6116 - acc: 0.67 - ETA: 22s - loss: 0.6119 - acc: 0.67 - ETA: 22s - loss: 0.6138 - acc: 0.66 - ETA: 22s - loss: 0.6134 - acc: 0.66 - ETA: 22s - loss: 0.6134 - acc: 0.66 - ETA: 22s - loss: 0.6125 - acc: 0.67 - ETA: 22s - loss: 0.6121 - acc: 0.67 - ETA: 22s - loss: 0.6132 - acc: 0.67 - ETA: 22s - loss: 0.6128 - acc: 0.67 - ETA: 22s - loss: 0.6121 - acc: 0.67 - ETA: 22s - loss: 0.6115 - acc: 0.67 - ETA: 21s - loss: 0.6119 - acc: 0.67 - ETA: 21s - loss: 0.6116 - acc: 0.67 - ETA: 21s - loss: 0.6098 - acc: 0.67 - ETA: 21s - loss: 0.6125 - acc: 0.67 - ETA: 21s - loss: 0.6121 - acc: 0.67 - ETA: 21s - loss: 0.6120 - acc: 0.66 - ETA: 21s - loss: 0.6124 - acc: 0.66 - ETA: 21s - loss: 0.6110 - acc: 0.67 - ETA: 21s - loss: 0.6094 - acc: 0.67 - ETA: 21s - loss: 0.6099 - acc: 0.67 - ETA: 21s - loss: 0.6107 - acc: 0.67 - ETA: 21s - loss: 0.6102 - acc: 0.66 - ETA: 21s - loss: 0.6093 - acc: 0.67 - ETA: 20s - loss: 0.6090 - acc: 0.67 - ETA: 20s - loss: 0.6079 - acc: 0.67 - ETA: 20s - loss: 0.6072 - acc: 0.67 - ETA: 20s - loss: 0.6077 - acc: 0.67 - ETA: 20s - loss: 0.6082 - acc: 0.66 - ETA: 20s - loss: 0.6078 - acc: 0.66 - ETA: 20s - loss: 0.6065 - acc: 0.67 - ETA: 20s - loss: 0.6057 - acc: 0.67 - ETA: 20s - loss: 0.6060 - acc: 0.67 - ETA: 20s - loss: 0.6048 - acc: 0.67 - ETA: 20s - loss: 0.6044 - acc: 0.67 - ETA: 20s - loss: 0.6043 - acc: 0.67 - ETA: 19s - loss: 0.6042 - acc: 0.67 - ETA: 19s - loss: 0.6031 - acc: 0.67 - ETA: 19s - loss: 0.6031 - acc: 0.67 - ETA: 19s - loss: 0.6026 - acc: 0.67 - ETA: 19s - loss: 0.6016 - acc: 0.67 - ETA: 19s - loss: 0.6014 - acc: 0.67 - ETA: 19s - loss: 0.6028 - acc: 0.67 - ETA: 19s - loss: 0.6032 - acc: 0.67 - ETA: 19s - loss: 0.6015 - acc: 0.67 - ETA: 18s - loss: 0.6032 - acc: 0.67 - ETA: 18s - loss: 0.6032 - acc: 0.67 - ETA: 18s - loss: 0.6033 - acc: 0.67 - ETA: 18s - loss: 0.6026 - acc: 0.67 - ETA: 18s - loss: 0.6031 - acc: 0.67 - ETA: 18s - loss: 0.6033 - acc: 0.67 - ETA: 18s - loss: 0.6026 - acc: 0.67 - ETA: 18s - loss: 0.6024 - acc: 0.67 - ETA: 18s - loss: 0.6017 - acc: 0.67 - ETA: 18s - loss: 0.6010 - acc: 0.67 - ETA: 18s - loss: 0.6005 - acc: 0.67 - ETA: 17s - loss: 0.6005 - acc: 0.68 - ETA: 17s - loss: 0.5991 - acc: 0.68 - ETA: 17s - loss: 0.5997 - acc: 0.68 - ETA: 17s - loss: 0.5994 - acc: 0.68 - ETA: 17s - loss: 0.5996 - acc: 0.68 - ETA: 17s - loss: 0.5990 - acc: 0.68 - ETA: 17s - loss: 0.5987 - acc: 0.68 - ETA: 17s - loss: 0.5987 - acc: 0.68 - ETA: 17s - loss: 0.5990 - acc: 0.68 - ETA: 17s - loss: 0.5989 - acc: 0.68 - ETA: 17s - loss: 0.5987 - acc: 0.68 - ETA: 17s - loss: 0.5987 - acc: 0.68 - ETA: 16s - loss: 0.5981 - acc: 0.68 - ETA: 16s - loss: 0.5984 - acc: 0.68 - ETA: 16s - loss: 0.5984 - acc: 0.68 - ETA: 16s - loss: 0.5981 - acc: 0.68 - ETA: 16s - loss: 0.5984 - acc: 0.68 - ETA: 16s - loss: 0.5988 - acc: 0.68 - ETA: 16s - loss: 0.5984 - acc: 0.68 - ETA: 16s - loss: 0.5978 - acc: 0.68 - ETA: 16s - loss: 0.5978 - acc: 0.68 - ETA: 16s - loss: 0.5975 - acc: 0.68 - ETA: 16s - loss: 0.5973 - acc: 0.68 - ETA: 15s - loss: 0.5973 - acc: 0.68 - ETA: 15s - loss: 0.5970 - acc: 0.68 - ETA: 15s - loss: 0.5968 - acc: 0.68 - ETA: 15s - loss: 0.5963 - acc: 0.68 - ETA: 15s - loss: 0.5964 - acc: 0.68 - ETA: 15s - loss: 0.5972 - acc: 0.68 - ETA: 15s - loss: 0.5973 - acc: 0.68 - ETA: 15s - loss: 0.5983 - acc: 0.68 - ETA: 15s - loss: 0.5990 - acc: 0.68 - ETA: 15s - loss: 0.5992 - acc: 0.68 - ETA: 15s - loss: 0.5994 - acc: 0.68 - ETA: 15s - loss: 0.5995 - acc: 0.68 - ETA: 14s - loss: 0.5990 - acc: 0.68 - ETA: 14s - loss: 0.5983 - acc: 0.68 - ETA: 14s - loss: 0.5983 - acc: 0.68 - ETA: 14s - loss: 0.5981 - acc: 0.68 - ETA: 14s - loss: 0.5980 - acc: 0.68 - ETA: 14s - loss: 0.5979 - acc: 0.68 - ETA: 14s - loss: 0.5986 - acc: 0.68 - ETA: 14s - loss: 0.5982 - acc: 0.68 - ETA: 14s - loss: 0.5983 - acc: 0.68 - ETA: 14s - loss: 0.5983 - acc: 0.68 - ETA: 14s - loss: 0.5982 - acc: 0.68 - ETA: 14s - loss: 0.5985 - acc: 0.68 - ETA: 14s - loss: 0.5984 - acc: 0.68 - ETA: 13s - loss: 0.5983 - acc: 0.68 - ETA: 13s - loss: 0.5980 - acc: 0.68 - ETA: 13s - loss: 0.5981 - acc: 0.68 - ETA: 13s - loss: 0.5977 - acc: 0.68 - ETA: 13s - loss: 0.5983 - acc: 0.68 - ETA: 13s - loss: 0.5984 - acc: 0.68 - ETA: 13s - loss: 0.5985 - acc: 0.68 - ETA: 13s - loss: 0.5983 - acc: 0.68 - ETA: 13s - loss: 0.5985 - acc: 0.68 - ETA: 13s - loss: 0.5980 - acc: 0.68 - ETA: 13s - loss: 0.5977 - acc: 0.68 - ETA: 13s - loss: 0.5978 - acc: 0.68 - ETA: 12s - loss: 0.5979 - acc: 0.68 - ETA: 12s - loss: 0.5974 - acc: 0.68 - ETA: 12s - loss: 0.5969 - acc: 0.68 - ETA: 12s - loss: 0.5965 - acc: 0.68 - ETA: 12s - loss: 0.5964 - acc: 0.68 - ETA: 12s - loss: 0.5963 - acc: 0.68 - ETA: 12s - loss: 0.5961 - acc: 0.68 - ETA: 12s - loss: 0.5958 - acc: 0.68 - ETA: 12s - loss: 0.5958 - acc: 0.68 - ETA: 12s - loss: 0.5955 - acc: 0.68 - ETA: 12s - loss: 0.5954 - acc: 0.68 - ETA: 12s - loss: 0.5958 - acc: 0.68 - ETA: 12s - loss: 0.5961 - acc: 0.68 - ETA: 12s - loss: 0.5962 - acc: 0.68 - ETA: 11s - loss: 0.5966 - acc: 0.68 - ETA: 11s - loss: 0.5967 - acc: 0.68 - ETA: 11s - loss: 0.5967 - acc: 0.68 - ETA: 11s - loss: 0.5967 - acc: 0.68 - ETA: 11s - loss: 0.5966 - acc: 0.68 - ETA: 11s - loss: 0.5966 - acc: 0.68 - ETA: 11s - loss: 0.5966 - acc: 0.68 - ETA: 11s - loss: 0.5962 - acc: 0.68 - ETA: 11s - loss: 0.5963 - acc: 0.68 - ETA: 11s - loss: 0.5958 - acc: 0.68 - ETA: 11s - loss: 0.5959 - acc: 0.68 - ETA: 11s - loss: 0.5957 - acc: 0.68 - ETA: 11s - loss: 0.5957 - acc: 0.68 - ETA: 10s - loss: 0.5954 - acc: 0.68 - ETA: 10s - loss: 0.5957 - acc: 0.68 - ETA: 10s - loss: 0.5960 - acc: 0.68 - ETA: 10s - loss: 0.5955 - acc: 0.68 - ETA: 10s - loss: 0.5956 - acc: 0.68 - ETA: 10s - loss: 0.5961 - acc: 0.68 - ETA: 10s - loss: 0.5964 - acc: 0.68 - ETA: 10s - loss: 0.5965 - acc: 0.68 - ETA: 10s - loss: 0.5963 - acc: 0.68 - ETA: 10s - loss: 0.5967 - acc: 0.68 - ETA: 10s - loss: 0.5969 - acc: 0.68 - ETA: 10s - loss: 0.5972 - acc: 0.68 - ETA: 9s - loss: 0.5970 - acc: 0.6841 - ETA: 9s - loss: 0.5970 - acc: 0.683 - ETA: 9s - loss: 0.5972 - acc: 0.683 - ETA: 9s - loss: 0.5973 - acc: 0.683 - ETA: 9s - loss: 0.5973 - acc: 0.683 - ETA: 9s - loss: 0.5973 - acc: 0.683 - ETA: 9s - loss: 0.5971 - acc: 0.683 - ETA: 9s - loss: 0.5970 - acc: 0.683 - ETA: 9s - loss: 0.5965 - acc: 0.684 - ETA: 9s - loss: 0.5967 - acc: 0.684 - ETA: 9s - loss: 0.5968 - acc: 0.684 - ETA: 9s - loss: 0.5966 - acc: 0.684 - ETA: 9s - loss: 0.5968 - acc: 0.684 - ETA: 8s - loss: 0.5969 - acc: 0.684 - ETA: 8s - loss: 0.5971 - acc: 0.683 - ETA: 8s - loss: 0.5971 - acc: 0.683 - ETA: 8s - loss: 0.5972 - acc: 0.683 - ETA: 8s - loss: 0.5969 - acc: 0.683 - ETA: 8s - loss: 0.5970 - acc: 0.683 - ETA: 8s - loss: 0.5971 - acc: 0.683 - ETA: 8s - loss: 0.5972 - acc: 0.683 - ETA: 8s - loss: 0.5968 - acc: 0.683 - ETA: 8s - loss: 0.5968 - acc: 0.683 - ETA: 8s - loss: 0.5968 - acc: 0.683 - ETA: 8s - loss: 0.5966 - acc: 0.684 - ETA: 8s - loss: 0.5964 - acc: 0.684 - ETA: 7s - loss: 0.5966 - acc: 0.683 - ETA: 7s - loss: 0.5963 - acc: 0.684 - ETA: 7s - loss: 0.5965 - acc: 0.6837"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31500/31500 [==============================] - ETA: 7s - loss: 0.5964 - acc: 0.683 - ETA: 7s - loss: 0.5964 - acc: 0.683 - ETA: 7s - loss: 0.5965 - acc: 0.683 - ETA: 7s - loss: 0.5965 - acc: 0.683 - ETA: 7s - loss: 0.5963 - acc: 0.684 - ETA: 7s - loss: 0.5962 - acc: 0.684 - ETA: 7s - loss: 0.5963 - acc: 0.683 - ETA: 7s - loss: 0.5963 - acc: 0.683 - ETA: 7s - loss: 0.5960 - acc: 0.683 - ETA: 7s - loss: 0.5958 - acc: 0.684 - ETA: 6s - loss: 0.5959 - acc: 0.683 - ETA: 6s - loss: 0.5957 - acc: 0.684 - ETA: 6s - loss: 0.5954 - acc: 0.684 - ETA: 6s - loss: 0.5955 - acc: 0.684 - ETA: 6s - loss: 0.5951 - acc: 0.685 - ETA: 6s - loss: 0.5953 - acc: 0.684 - ETA: 6s - loss: 0.5953 - acc: 0.684 - ETA: 6s - loss: 0.5953 - acc: 0.684 - ETA: 6s - loss: 0.5954 - acc: 0.684 - ETA: 6s - loss: 0.5953 - acc: 0.685 - ETA: 6s - loss: 0.5951 - acc: 0.685 - ETA: 6s - loss: 0.5951 - acc: 0.685 - ETA: 6s - loss: 0.5947 - acc: 0.685 - ETA: 5s - loss: 0.5946 - acc: 0.685 - ETA: 5s - loss: 0.5948 - acc: 0.685 - ETA: 5s - loss: 0.5945 - acc: 0.685 - ETA: 5s - loss: 0.5940 - acc: 0.686 - ETA: 5s - loss: 0.5940 - acc: 0.686 - ETA: 5s - loss: 0.5939 - acc: 0.686 - ETA: 5s - loss: 0.5941 - acc: 0.686 - ETA: 5s - loss: 0.5939 - acc: 0.686 - ETA: 5s - loss: 0.5937 - acc: 0.686 - ETA: 5s - loss: 0.5934 - acc: 0.686 - ETA: 5s - loss: 0.5932 - acc: 0.686 - ETA: 5s - loss: 0.5933 - acc: 0.686 - ETA: 4s - loss: 0.5931 - acc: 0.686 - ETA: 4s - loss: 0.5930 - acc: 0.686 - ETA: 4s - loss: 0.5929 - acc: 0.686 - ETA: 4s - loss: 0.5929 - acc: 0.686 - ETA: 4s - loss: 0.5927 - acc: 0.687 - ETA: 4s - loss: 0.5927 - acc: 0.687 - ETA: 4s - loss: 0.5923 - acc: 0.687 - ETA: 4s - loss: 0.5921 - acc: 0.687 - ETA: 4s - loss: 0.5918 - acc: 0.687 - ETA: 4s - loss: 0.5915 - acc: 0.688 - ETA: 4s - loss: 0.5916 - acc: 0.687 - ETA: 4s - loss: 0.5918 - acc: 0.687 - ETA: 4s - loss: 0.5916 - acc: 0.688 - ETA: 3s - loss: 0.5914 - acc: 0.688 - ETA: 3s - loss: 0.5917 - acc: 0.687 - ETA: 3s - loss: 0.5915 - acc: 0.688 - ETA: 3s - loss: 0.5915 - acc: 0.688 - ETA: 3s - loss: 0.5913 - acc: 0.688 - ETA: 3s - loss: 0.5910 - acc: 0.688 - ETA: 3s - loss: 0.5911 - acc: 0.688 - ETA: 3s - loss: 0.5906 - acc: 0.688 - ETA: 3s - loss: 0.5908 - acc: 0.688 - ETA: 3s - loss: 0.5909 - acc: 0.688 - ETA: 3s - loss: 0.5909 - acc: 0.688 - ETA: 3s - loss: 0.5905 - acc: 0.688 - ETA: 3s - loss: 0.5901 - acc: 0.689 - ETA: 2s - loss: 0.5898 - acc: 0.689 - ETA: 2s - loss: 0.5901 - acc: 0.689 - ETA: 2s - loss: 0.5901 - acc: 0.688 - ETA: 2s - loss: 0.5897 - acc: 0.689 - ETA: 2s - loss: 0.5898 - acc: 0.689 - ETA: 2s - loss: 0.5897 - acc: 0.689 - ETA: 2s - loss: 0.5895 - acc: 0.689 - ETA: 2s - loss: 0.5895 - acc: 0.689 - ETA: 2s - loss: 0.5895 - acc: 0.689 - ETA: 2s - loss: 0.5895 - acc: 0.688 - ETA: 2s - loss: 0.5892 - acc: 0.689 - ETA: 2s - loss: 0.5891 - acc: 0.689 - ETA: 2s - loss: 0.5889 - acc: 0.689 - ETA: 1s - loss: 0.5888 - acc: 0.689 - ETA: 1s - loss: 0.5887 - acc: 0.689 - ETA: 1s - loss: 0.5888 - acc: 0.689 - ETA: 1s - loss: 0.5887 - acc: 0.689 - ETA: 1s - loss: 0.5885 - acc: 0.690 - ETA: 1s - loss: 0.5882 - acc: 0.690 - ETA: 1s - loss: 0.5879 - acc: 0.690 - ETA: 1s - loss: 0.5876 - acc: 0.690 - ETA: 1s - loss: 0.5876 - acc: 0.690 - ETA: 1s - loss: 0.5876 - acc: 0.690 - ETA: 1s - loss: 0.5877 - acc: 0.690 - ETA: 1s - loss: 0.5877 - acc: 0.690 - ETA: 1s - loss: 0.5877 - acc: 0.690 - ETA: 0s - loss: 0.5882 - acc: 0.690 - ETA: 0s - loss: 0.5882 - acc: 0.690 - ETA: 0s - loss: 0.5883 - acc: 0.690 - ETA: 0s - loss: 0.5882 - acc: 0.690 - ETA: 0s - loss: 0.5884 - acc: 0.689 - ETA: 0s - loss: 0.5884 - acc: 0.689 - ETA: 0s - loss: 0.5884 - acc: 0.690 - ETA: 0s - loss: 0.5884 - acc: 0.690 - ETA: 0s - loss: 0.5885 - acc: 0.690 - ETA: 0s - loss: 0.5886 - acc: 0.689 - ETA: 0s - loss: 0.5887 - acc: 0.689 - ETA: 0s - loss: 0.5888 - acc: 0.689 - 25s 797us/step - loss: 0.5887 - acc: 0.6896 - val_loss: 0.5901 - val_acc: 0.6883\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21500/31500 [===================>..........] - ETA: 21s - loss: 0.6215 - acc: 0.62 - ETA: 22s - loss: 0.6266 - acc: 0.63 - ETA: 22s - loss: 0.6087 - acc: 0.65 - ETA: 23s - loss: 0.6204 - acc: 0.64 - ETA: 23s - loss: 0.6253 - acc: 0.64 - ETA: 23s - loss: 0.6068 - acc: 0.66 - ETA: 23s - loss: 0.6074 - acc: 0.66 - ETA: 23s - loss: 0.6059 - acc: 0.66 - ETA: 23s - loss: 0.6082 - acc: 0.66 - ETA: 23s - loss: 0.6033 - acc: 0.66 - ETA: 23s - loss: 0.5984 - acc: 0.67 - ETA: 23s - loss: 0.6023 - acc: 0.67 - ETA: 23s - loss: 0.5984 - acc: 0.67 - ETA: 23s - loss: 0.5951 - acc: 0.68 - ETA: 23s - loss: 0.5944 - acc: 0.68 - ETA: 23s - loss: 0.5946 - acc: 0.68 - ETA: 23s - loss: 0.5965 - acc: 0.68 - ETA: 23s - loss: 0.5960 - acc: 0.68 - ETA: 23s - loss: 0.5935 - acc: 0.68 - ETA: 23s - loss: 0.5900 - acc: 0.68 - ETA: 23s - loss: 0.5894 - acc: 0.68 - ETA: 23s - loss: 0.5913 - acc: 0.68 - ETA: 22s - loss: 0.5887 - acc: 0.69 - ETA: 22s - loss: 0.5893 - acc: 0.68 - ETA: 22s - loss: 0.5882 - acc: 0.69 - ETA: 22s - loss: 0.5868 - acc: 0.69 - ETA: 22s - loss: 0.5865 - acc: 0.69 - ETA: 22s - loss: 0.5852 - acc: 0.69 - ETA: 22s - loss: 0.5831 - acc: 0.69 - ETA: 22s - loss: 0.5818 - acc: 0.69 - ETA: 22s - loss: 0.5806 - acc: 0.69 - ETA: 22s - loss: 0.5816 - acc: 0.69 - ETA: 22s - loss: 0.5808 - acc: 0.69 - ETA: 22s - loss: 0.5799 - acc: 0.69 - ETA: 22s - loss: 0.5813 - acc: 0.69 - ETA: 21s - loss: 0.5799 - acc: 0.69 - ETA: 21s - loss: 0.5783 - acc: 0.69 - ETA: 21s - loss: 0.5764 - acc: 0.70 - ETA: 21s - loss: 0.5767 - acc: 0.69 - ETA: 21s - loss: 0.5764 - acc: 0.69 - ETA: 21s - loss: 0.5761 - acc: 0.70 - ETA: 21s - loss: 0.5757 - acc: 0.70 - ETA: 21s - loss: 0.5753 - acc: 0.69 - ETA: 21s - loss: 0.5748 - acc: 0.69 - ETA: 21s - loss: 0.5742 - acc: 0.70 - ETA: 21s - loss: 0.5730 - acc: 0.70 - ETA: 21s - loss: 0.5724 - acc: 0.70 - ETA: 21s - loss: 0.5715 - acc: 0.70 - ETA: 21s - loss: 0.5715 - acc: 0.70 - ETA: 21s - loss: 0.5731 - acc: 0.70 - ETA: 20s - loss: 0.5723 - acc: 0.70 - ETA: 20s - loss: 0.5721 - acc: 0.70 - ETA: 20s - loss: 0.5713 - acc: 0.70 - ETA: 20s - loss: 0.5719 - acc: 0.70 - ETA: 20s - loss: 0.5709 - acc: 0.70 - ETA: 20s - loss: 0.5724 - acc: 0.70 - ETA: 20s - loss: 0.5727 - acc: 0.70 - ETA: 20s - loss: 0.5731 - acc: 0.70 - ETA: 20s - loss: 0.5732 - acc: 0.70 - ETA: 20s - loss: 0.5745 - acc: 0.70 - ETA: 20s - loss: 0.5735 - acc: 0.70 - ETA: 20s - loss: 0.5740 - acc: 0.70 - ETA: 20s - loss: 0.5741 - acc: 0.70 - ETA: 19s - loss: 0.5753 - acc: 0.70 - ETA: 19s - loss: 0.5753 - acc: 0.69 - ETA: 19s - loss: 0.5749 - acc: 0.70 - ETA: 19s - loss: 0.5750 - acc: 0.70 - ETA: 19s - loss: 0.5740 - acc: 0.70 - ETA: 19s - loss: 0.5743 - acc: 0.70 - ETA: 19s - loss: 0.5736 - acc: 0.70 - ETA: 19s - loss: 0.5739 - acc: 0.70 - ETA: 19s - loss: 0.5738 - acc: 0.70 - ETA: 19s - loss: 0.5726 - acc: 0.70 - ETA: 19s - loss: 0.5732 - acc: 0.70 - ETA: 19s - loss: 0.5729 - acc: 0.70 - ETA: 18s - loss: 0.5727 - acc: 0.70 - ETA: 18s - loss: 0.5718 - acc: 0.70 - ETA: 18s - loss: 0.5721 - acc: 0.70 - ETA: 18s - loss: 0.5719 - acc: 0.70 - ETA: 18s - loss: 0.5737 - acc: 0.70 - ETA: 18s - loss: 0.5734 - acc: 0.70 - ETA: 18s - loss: 0.5735 - acc: 0.70 - ETA: 18s - loss: 0.5726 - acc: 0.70 - ETA: 18s - loss: 0.5725 - acc: 0.70 - ETA: 18s - loss: 0.5728 - acc: 0.70 - ETA: 18s - loss: 0.5733 - acc: 0.70 - ETA: 18s - loss: 0.5732 - acc: 0.70 - ETA: 17s - loss: 0.5739 - acc: 0.70 - ETA: 17s - loss: 0.5751 - acc: 0.70 - ETA: 17s - loss: 0.5768 - acc: 0.69 - ETA: 17s - loss: 0.5770 - acc: 0.69 - ETA: 17s - loss: 0.5775 - acc: 0.69 - ETA: 17s - loss: 0.5772 - acc: 0.69 - ETA: 17s - loss: 0.5769 - acc: 0.69 - ETA: 17s - loss: 0.5775 - acc: 0.69 - ETA: 17s - loss: 0.5776 - acc: 0.69 - ETA: 17s - loss: 0.5782 - acc: 0.69 - ETA: 17s - loss: 0.5783 - acc: 0.69 - ETA: 16s - loss: 0.5783 - acc: 0.69 - ETA: 16s - loss: 0.5786 - acc: 0.69 - ETA: 16s - loss: 0.5794 - acc: 0.69 - ETA: 16s - loss: 0.5801 - acc: 0.69 - ETA: 16s - loss: 0.5801 - acc: 0.69 - ETA: 16s - loss: 0.5811 - acc: 0.69 - ETA: 16s - loss: 0.5816 - acc: 0.69 - ETA: 16s - loss: 0.5819 - acc: 0.69 - ETA: 16s - loss: 0.5826 - acc: 0.69 - ETA: 16s - loss: 0.5824 - acc: 0.69 - ETA: 16s - loss: 0.5827 - acc: 0.69 - ETA: 16s - loss: 0.5828 - acc: 0.69 - ETA: 15s - loss: 0.5827 - acc: 0.69 - ETA: 15s - loss: 0.5825 - acc: 0.69 - ETA: 15s - loss: 0.5827 - acc: 0.69 - ETA: 15s - loss: 0.5824 - acc: 0.69 - ETA: 15s - loss: 0.5826 - acc: 0.69 - ETA: 15s - loss: 0.5825 - acc: 0.69 - ETA: 15s - loss: 0.5828 - acc: 0.69 - ETA: 15s - loss: 0.5826 - acc: 0.69 - ETA: 15s - loss: 0.5833 - acc: 0.69 - ETA: 15s - loss: 0.5839 - acc: 0.69 - ETA: 15s - loss: 0.5834 - acc: 0.69 - ETA: 15s - loss: 0.5833 - acc: 0.69 - ETA: 14s - loss: 0.5838 - acc: 0.69 - ETA: 14s - loss: 0.5836 - acc: 0.69 - ETA: 14s - loss: 0.5836 - acc: 0.69 - ETA: 14s - loss: 0.5834 - acc: 0.69 - ETA: 14s - loss: 0.5834 - acc: 0.69 - ETA: 14s - loss: 0.5837 - acc: 0.69 - ETA: 14s - loss: 0.5833 - acc: 0.69 - ETA: 14s - loss: 0.5826 - acc: 0.69 - ETA: 14s - loss: 0.5828 - acc: 0.69 - ETA: 14s - loss: 0.5823 - acc: 0.69 - ETA: 14s - loss: 0.5825 - acc: 0.69 - ETA: 14s - loss: 0.5824 - acc: 0.69 - ETA: 14s - loss: 0.5822 - acc: 0.69 - ETA: 13s - loss: 0.5819 - acc: 0.69 - ETA: 13s - loss: 0.5816 - acc: 0.69 - ETA: 13s - loss: 0.5814 - acc: 0.69 - ETA: 13s - loss: 0.5818 - acc: 0.69 - ETA: 13s - loss: 0.5822 - acc: 0.69 - ETA: 13s - loss: 0.5828 - acc: 0.69 - ETA: 13s - loss: 0.5826 - acc: 0.69 - ETA: 13s - loss: 0.5829 - acc: 0.69 - ETA: 13s - loss: 0.5828 - acc: 0.69 - ETA: 13s - loss: 0.5829 - acc: 0.69 - ETA: 13s - loss: 0.5826 - acc: 0.69 - ETA: 13s - loss: 0.5828 - acc: 0.69 - ETA: 13s - loss: 0.5827 - acc: 0.69 - ETA: 12s - loss: 0.5828 - acc: 0.69 - ETA: 12s - loss: 0.5825 - acc: 0.69 - ETA: 12s - loss: 0.5825 - acc: 0.69 - ETA: 12s - loss: 0.5822 - acc: 0.69 - ETA: 12s - loss: 0.5820 - acc: 0.69 - ETA: 12s - loss: 0.5819 - acc: 0.69 - ETA: 12s - loss: 0.5819 - acc: 0.69 - ETA: 12s - loss: 0.5820 - acc: 0.69 - ETA: 12s - loss: 0.5823 - acc: 0.69 - ETA: 12s - loss: 0.5816 - acc: 0.69 - ETA: 12s - loss: 0.5815 - acc: 0.69 - ETA: 12s - loss: 0.5811 - acc: 0.69 - ETA: 11s - loss: 0.5816 - acc: 0.69 - ETA: 11s - loss: 0.5811 - acc: 0.69 - ETA: 11s - loss: 0.5809 - acc: 0.69 - ETA: 11s - loss: 0.5803 - acc: 0.69 - ETA: 11s - loss: 0.5798 - acc: 0.69 - ETA: 11s - loss: 0.5802 - acc: 0.69 - ETA: 11s - loss: 0.5796 - acc: 0.69 - ETA: 11s - loss: 0.5796 - acc: 0.69 - ETA: 11s - loss: 0.5797 - acc: 0.69 - ETA: 11s - loss: 0.5794 - acc: 0.69 - ETA: 11s - loss: 0.5791 - acc: 0.69 - ETA: 11s - loss: 0.5791 - acc: 0.69 - ETA: 11s - loss: 0.5791 - acc: 0.69 - ETA: 10s - loss: 0.5788 - acc: 0.69 - ETA: 10s - loss: 0.5785 - acc: 0.69 - ETA: 10s - loss: 0.5784 - acc: 0.69 - ETA: 10s - loss: 0.5779 - acc: 0.69 - ETA: 10s - loss: 0.5774 - acc: 0.69 - ETA: 10s - loss: 0.5776 - acc: 0.69 - ETA: 10s - loss: 0.5774 - acc: 0.69 - ETA: 10s - loss: 0.5773 - acc: 0.69 - ETA: 10s - loss: 0.5771 - acc: 0.69 - ETA: 10s - loss: 0.5767 - acc: 0.69 - ETA: 10s - loss: 0.5766 - acc: 0.69 - ETA: 10s - loss: 0.5763 - acc: 0.69 - ETA: 10s - loss: 0.5762 - acc: 0.69 - ETA: 10s - loss: 0.5765 - acc: 0.69 - ETA: 9s - loss: 0.5769 - acc: 0.6981 - ETA: 9s - loss: 0.5772 - acc: 0.698 - ETA: 9s - loss: 0.5769 - acc: 0.698 - ETA: 9s - loss: 0.5769 - acc: 0.698 - ETA: 9s - loss: 0.5769 - acc: 0.697 - ETA: 9s - loss: 0.5768 - acc: 0.697 - ETA: 9s - loss: 0.5769 - acc: 0.697 - ETA: 9s - loss: 0.5770 - acc: 0.697 - ETA: 9s - loss: 0.5768 - acc: 0.697 - ETA: 9s - loss: 0.5766 - acc: 0.697 - ETA: 9s - loss: 0.5767 - acc: 0.697 - ETA: 9s - loss: 0.5766 - acc: 0.697 - ETA: 8s - loss: 0.5767 - acc: 0.697 - ETA: 8s - loss: 0.5768 - acc: 0.697 - ETA: 8s - loss: 0.5767 - acc: 0.697 - ETA: 8s - loss: 0.5768 - acc: 0.697 - ETA: 8s - loss: 0.5769 - acc: 0.697 - ETA: 8s - loss: 0.5770 - acc: 0.697 - ETA: 8s - loss: 0.5770 - acc: 0.697 - ETA: 8s - loss: 0.5771 - acc: 0.697 - ETA: 8s - loss: 0.5771 - acc: 0.697 - ETA: 8s - loss: 0.5768 - acc: 0.697 - ETA: 8s - loss: 0.5766 - acc: 0.698 - ETA: 8s - loss: 0.5764 - acc: 0.698 - ETA: 8s - loss: 0.5763 - acc: 0.698 - ETA: 7s - loss: 0.5763 - acc: 0.698 - ETA: 7s - loss: 0.5762 - acc: 0.698 - ETA: 7s - loss: 0.5762 - acc: 0.6985"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31500/31500 [==============================] - ETA: 7s - loss: 0.5764 - acc: 0.698 - ETA: 7s - loss: 0.5765 - acc: 0.698 - ETA: 7s - loss: 0.5764 - acc: 0.698 - ETA: 7s - loss: 0.5765 - acc: 0.698 - ETA: 7s - loss: 0.5763 - acc: 0.698 - ETA: 7s - loss: 0.5763 - acc: 0.698 - ETA: 7s - loss: 0.5762 - acc: 0.698 - ETA: 7s - loss: 0.5760 - acc: 0.698 - ETA: 7s - loss: 0.5768 - acc: 0.698 - ETA: 7s - loss: 0.5771 - acc: 0.697 - ETA: 6s - loss: 0.5774 - acc: 0.697 - ETA: 6s - loss: 0.5778 - acc: 0.696 - ETA: 6s - loss: 0.5775 - acc: 0.697 - ETA: 6s - loss: 0.5776 - acc: 0.696 - ETA: 6s - loss: 0.5776 - acc: 0.696 - ETA: 6s - loss: 0.5774 - acc: 0.697 - ETA: 6s - loss: 0.5774 - acc: 0.697 - ETA: 6s - loss: 0.5776 - acc: 0.697 - ETA: 6s - loss: 0.5778 - acc: 0.696 - ETA: 6s - loss: 0.5775 - acc: 0.696 - ETA: 6s - loss: 0.5774 - acc: 0.697 - ETA: 6s - loss: 0.5774 - acc: 0.696 - ETA: 6s - loss: 0.5773 - acc: 0.697 - ETA: 5s - loss: 0.5771 - acc: 0.697 - ETA: 5s - loss: 0.5771 - acc: 0.697 - ETA: 5s - loss: 0.5771 - acc: 0.697 - ETA: 5s - loss: 0.5765 - acc: 0.697 - ETA: 5s - loss: 0.5764 - acc: 0.697 - ETA: 5s - loss: 0.5765 - acc: 0.697 - ETA: 5s - loss: 0.5764 - acc: 0.697 - ETA: 5s - loss: 0.5765 - acc: 0.697 - ETA: 5s - loss: 0.5768 - acc: 0.697 - ETA: 5s - loss: 0.5769 - acc: 0.697 - ETA: 5s - loss: 0.5771 - acc: 0.696 - ETA: 5s - loss: 0.5771 - acc: 0.696 - ETA: 5s - loss: 0.5770 - acc: 0.696 - ETA: 4s - loss: 0.5770 - acc: 0.696 - ETA: 4s - loss: 0.5770 - acc: 0.696 - ETA: 4s - loss: 0.5769 - acc: 0.696 - ETA: 4s - loss: 0.5766 - acc: 0.697 - ETA: 4s - loss: 0.5765 - acc: 0.697 - ETA: 4s - loss: 0.5764 - acc: 0.697 - ETA: 4s - loss: 0.5763 - acc: 0.697 - ETA: 4s - loss: 0.5766 - acc: 0.697 - ETA: 4s - loss: 0.5768 - acc: 0.696 - ETA: 4s - loss: 0.5769 - acc: 0.696 - ETA: 4s - loss: 0.5768 - acc: 0.696 - ETA: 4s - loss: 0.5770 - acc: 0.696 - ETA: 4s - loss: 0.5773 - acc: 0.696 - ETA: 3s - loss: 0.5774 - acc: 0.696 - ETA: 3s - loss: 0.5775 - acc: 0.696 - ETA: 3s - loss: 0.5775 - acc: 0.696 - ETA: 3s - loss: 0.5773 - acc: 0.696 - ETA: 3s - loss: 0.5774 - acc: 0.696 - ETA: 3s - loss: 0.5773 - acc: 0.696 - ETA: 3s - loss: 0.5773 - acc: 0.696 - ETA: 3s - loss: 0.5774 - acc: 0.696 - ETA: 3s - loss: 0.5772 - acc: 0.696 - ETA: 3s - loss: 0.5772 - acc: 0.696 - ETA: 3s - loss: 0.5772 - acc: 0.696 - ETA: 3s - loss: 0.5771 - acc: 0.696 - ETA: 3s - loss: 0.5768 - acc: 0.696 - ETA: 2s - loss: 0.5768 - acc: 0.696 - ETA: 2s - loss: 0.5767 - acc: 0.696 - ETA: 2s - loss: 0.5770 - acc: 0.696 - ETA: 2s - loss: 0.5769 - acc: 0.696 - ETA: 2s - loss: 0.5769 - acc: 0.696 - ETA: 2s - loss: 0.5769 - acc: 0.696 - ETA: 2s - loss: 0.5770 - acc: 0.696 - ETA: 2s - loss: 0.5771 - acc: 0.696 - ETA: 2s - loss: 0.5769 - acc: 0.696 - ETA: 2s - loss: 0.5769 - acc: 0.696 - ETA: 2s - loss: 0.5769 - acc: 0.696 - ETA: 2s - loss: 0.5774 - acc: 0.696 - ETA: 1s - loss: 0.5776 - acc: 0.696 - ETA: 1s - loss: 0.5774 - acc: 0.696 - ETA: 1s - loss: 0.5773 - acc: 0.696 - ETA: 1s - loss: 0.5772 - acc: 0.696 - ETA: 1s - loss: 0.5773 - acc: 0.696 - ETA: 1s - loss: 0.5776 - acc: 0.696 - ETA: 1s - loss: 0.5775 - acc: 0.696 - ETA: 1s - loss: 0.5775 - acc: 0.696 - ETA: 1s - loss: 0.5773 - acc: 0.696 - ETA: 1s - loss: 0.5774 - acc: 0.696 - ETA: 1s - loss: 0.5776 - acc: 0.696 - ETA: 1s - loss: 0.5776 - acc: 0.696 - ETA: 1s - loss: 0.5775 - acc: 0.696 - ETA: 0s - loss: 0.5775 - acc: 0.696 - ETA: 0s - loss: 0.5776 - acc: 0.696 - ETA: 0s - loss: 0.5773 - acc: 0.696 - ETA: 0s - loss: 0.5774 - acc: 0.696 - ETA: 0s - loss: 0.5773 - acc: 0.696 - ETA: 0s - loss: 0.5772 - acc: 0.696 - ETA: 0s - loss: 0.5772 - acc: 0.696 - ETA: 0s - loss: 0.5771 - acc: 0.696 - ETA: 0s - loss: 0.5773 - acc: 0.696 - ETA: 0s - loss: 0.5774 - acc: 0.696 - ETA: 0s - loss: 0.5773 - acc: 0.696 - ETA: 0s - loss: 0.5772 - acc: 0.696 - 25s 809us/step - loss: 0.5771 - acc: 0.6966 - val_loss: 0.5719 - val_acc: 0.6957\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21500/31500 [===================>..........] - ETA: 20s - loss: 0.5315 - acc: 0.78 - ETA: 23s - loss: 0.5270 - acc: 0.75 - ETA: 22s - loss: 0.5467 - acc: 0.74 - ETA: 22s - loss: 0.5509 - acc: 0.73 - ETA: 22s - loss: 0.5553 - acc: 0.72 - ETA: 23s - loss: 0.5478 - acc: 0.72 - ETA: 23s - loss: 0.5570 - acc: 0.72 - ETA: 23s - loss: 0.5557 - acc: 0.72 - ETA: 22s - loss: 0.5631 - acc: 0.71 - ETA: 22s - loss: 0.5703 - acc: 0.71 - ETA: 22s - loss: 0.5650 - acc: 0.72 - ETA: 22s - loss: 0.5720 - acc: 0.71 - ETA: 22s - loss: 0.5751 - acc: 0.70 - ETA: 22s - loss: 0.5720 - acc: 0.70 - ETA: 22s - loss: 0.5748 - acc: 0.70 - ETA: 22s - loss: 0.5805 - acc: 0.69 - ETA: 22s - loss: 0.5768 - acc: 0.70 - ETA: 22s - loss: 0.5758 - acc: 0.70 - ETA: 22s - loss: 0.5750 - acc: 0.70 - ETA: 22s - loss: 0.5747 - acc: 0.70 - ETA: 21s - loss: 0.5789 - acc: 0.69 - ETA: 21s - loss: 0.5802 - acc: 0.69 - ETA: 21s - loss: 0.5792 - acc: 0.69 - ETA: 21s - loss: 0.5801 - acc: 0.69 - ETA: 21s - loss: 0.5788 - acc: 0.69 - ETA: 21s - loss: 0.5816 - acc: 0.69 - ETA: 21s - loss: 0.5823 - acc: 0.69 - ETA: 21s - loss: 0.5840 - acc: 0.68 - ETA: 21s - loss: 0.5846 - acc: 0.68 - ETA: 21s - loss: 0.5844 - acc: 0.68 - ETA: 21s - loss: 0.5846 - acc: 0.68 - ETA: 21s - loss: 0.5814 - acc: 0.68 - ETA: 21s - loss: 0.5805 - acc: 0.68 - ETA: 21s - loss: 0.5784 - acc: 0.69 - ETA: 21s - loss: 0.5798 - acc: 0.68 - ETA: 21s - loss: 0.5801 - acc: 0.68 - ETA: 21s - loss: 0.5795 - acc: 0.68 - ETA: 21s - loss: 0.5803 - acc: 0.68 - ETA: 21s - loss: 0.5779 - acc: 0.69 - ETA: 21s - loss: 0.5767 - acc: 0.69 - ETA: 21s - loss: 0.5739 - acc: 0.69 - ETA: 21s - loss: 0.5735 - acc: 0.69 - ETA: 21s - loss: 0.5739 - acc: 0.69 - ETA: 21s - loss: 0.5735 - acc: 0.69 - ETA: 21s - loss: 0.5715 - acc: 0.69 - ETA: 21s - loss: 0.5707 - acc: 0.69 - ETA: 21s - loss: 0.5721 - acc: 0.69 - ETA: 21s - loss: 0.5723 - acc: 0.69 - ETA: 21s - loss: 0.5720 - acc: 0.69 - ETA: 21s - loss: 0.5715 - acc: 0.69 - ETA: 21s - loss: 0.5716 - acc: 0.69 - ETA: 21s - loss: 0.5721 - acc: 0.69 - ETA: 21s - loss: 0.5703 - acc: 0.69 - ETA: 21s - loss: 0.5705 - acc: 0.69 - ETA: 21s - loss: 0.5692 - acc: 0.69 - ETA: 21s - loss: 0.5701 - acc: 0.69 - ETA: 20s - loss: 0.5705 - acc: 0.69 - ETA: 20s - loss: 0.5708 - acc: 0.69 - ETA: 20s - loss: 0.5715 - acc: 0.69 - ETA: 20s - loss: 0.5722 - acc: 0.69 - ETA: 20s - loss: 0.5717 - acc: 0.69 - ETA: 20s - loss: 0.5722 - acc: 0.69 - ETA: 20s - loss: 0.5735 - acc: 0.69 - ETA: 20s - loss: 0.5741 - acc: 0.69 - ETA: 20s - loss: 0.5728 - acc: 0.69 - ETA: 20s - loss: 0.5727 - acc: 0.69 - ETA: 20s - loss: 0.5725 - acc: 0.69 - ETA: 20s - loss: 0.5722 - acc: 0.69 - ETA: 20s - loss: 0.5724 - acc: 0.69 - ETA: 20s - loss: 0.5727 - acc: 0.69 - ETA: 20s - loss: 0.5730 - acc: 0.69 - ETA: 19s - loss: 0.5734 - acc: 0.69 - ETA: 19s - loss: 0.5729 - acc: 0.69 - ETA: 19s - loss: 0.5730 - acc: 0.69 - ETA: 19s - loss: 0.5732 - acc: 0.69 - ETA: 19s - loss: 0.5723 - acc: 0.69 - ETA: 19s - loss: 0.5721 - acc: 0.69 - ETA: 19s - loss: 0.5730 - acc: 0.69 - ETA: 19s - loss: 0.5722 - acc: 0.69 - ETA: 19s - loss: 0.5726 - acc: 0.69 - ETA: 19s - loss: 0.5726 - acc: 0.69 - ETA: 19s - loss: 0.5719 - acc: 0.69 - ETA: 18s - loss: 0.5718 - acc: 0.69 - ETA: 18s - loss: 0.5709 - acc: 0.69 - ETA: 18s - loss: 0.5697 - acc: 0.69 - ETA: 18s - loss: 0.5691 - acc: 0.69 - ETA: 18s - loss: 0.5686 - acc: 0.69 - ETA: 18s - loss: 0.5683 - acc: 0.69 - ETA: 18s - loss: 0.5676 - acc: 0.69 - ETA: 18s - loss: 0.5677 - acc: 0.69 - ETA: 18s - loss: 0.5681 - acc: 0.69 - ETA: 18s - loss: 0.5688 - acc: 0.69 - ETA: 18s - loss: 0.5686 - acc: 0.69 - ETA: 17s - loss: 0.5680 - acc: 0.69 - ETA: 17s - loss: 0.5684 - acc: 0.69 - ETA: 17s - loss: 0.5690 - acc: 0.69 - ETA: 17s - loss: 0.5692 - acc: 0.69 - ETA: 17s - loss: 0.5692 - acc: 0.69 - ETA: 17s - loss: 0.5685 - acc: 0.69 - ETA: 17s - loss: 0.5690 - acc: 0.69 - ETA: 17s - loss: 0.5687 - acc: 0.69 - ETA: 17s - loss: 0.5686 - acc: 0.69 - ETA: 17s - loss: 0.5689 - acc: 0.69 - ETA: 17s - loss: 0.5686 - acc: 0.69 - ETA: 16s - loss: 0.5682 - acc: 0.69 - ETA: 16s - loss: 0.5684 - acc: 0.69 - ETA: 16s - loss: 0.5684 - acc: 0.69 - ETA: 16s - loss: 0.5685 - acc: 0.69 - ETA: 16s - loss: 0.5691 - acc: 0.69 - ETA: 16s - loss: 0.5689 - acc: 0.69 - ETA: 16s - loss: 0.5692 - acc: 0.69 - ETA: 16s - loss: 0.5697 - acc: 0.69 - ETA: 16s - loss: 0.5691 - acc: 0.69 - ETA: 16s - loss: 0.5692 - acc: 0.69 - ETA: 16s - loss: 0.5693 - acc: 0.69 - ETA: 16s - loss: 0.5692 - acc: 0.69 - ETA: 15s - loss: 0.5695 - acc: 0.69 - ETA: 15s - loss: 0.5693 - acc: 0.69 - ETA: 15s - loss: 0.5688 - acc: 0.70 - ETA: 15s - loss: 0.5695 - acc: 0.69 - ETA: 15s - loss: 0.5698 - acc: 0.69 - ETA: 15s - loss: 0.5693 - acc: 0.69 - ETA: 15s - loss: 0.5693 - acc: 0.69 - ETA: 15s - loss: 0.5692 - acc: 0.69 - ETA: 15s - loss: 0.5697 - acc: 0.69 - ETA: 15s - loss: 0.5697 - acc: 0.69 - ETA: 15s - loss: 0.5699 - acc: 0.69 - ETA: 14s - loss: 0.5699 - acc: 0.69 - ETA: 14s - loss: 0.5698 - acc: 0.69 - ETA: 14s - loss: 0.5697 - acc: 0.69 - ETA: 14s - loss: 0.5696 - acc: 0.69 - ETA: 14s - loss: 0.5695 - acc: 0.69 - ETA: 14s - loss: 0.5694 - acc: 0.69 - ETA: 14s - loss: 0.5696 - acc: 0.69 - ETA: 14s - loss: 0.5692 - acc: 0.70 - ETA: 14s - loss: 0.5692 - acc: 0.70 - ETA: 14s - loss: 0.5688 - acc: 0.70 - ETA: 14s - loss: 0.5687 - acc: 0.70 - ETA: 14s - loss: 0.5684 - acc: 0.70 - ETA: 13s - loss: 0.5688 - acc: 0.70 - ETA: 13s - loss: 0.5684 - acc: 0.70 - ETA: 13s - loss: 0.5686 - acc: 0.70 - ETA: 13s - loss: 0.5683 - acc: 0.70 - ETA: 13s - loss: 0.5682 - acc: 0.70 - ETA: 13s - loss: 0.5687 - acc: 0.70 - ETA: 13s - loss: 0.5687 - acc: 0.70 - ETA: 13s - loss: 0.5692 - acc: 0.70 - ETA: 13s - loss: 0.5688 - acc: 0.70 - ETA: 13s - loss: 0.5690 - acc: 0.69 - ETA: 13s - loss: 0.5688 - acc: 0.70 - ETA: 13s - loss: 0.5689 - acc: 0.70 - ETA: 12s - loss: 0.5690 - acc: 0.70 - ETA: 12s - loss: 0.5692 - acc: 0.69 - ETA: 12s - loss: 0.5684 - acc: 0.70 - ETA: 12s - loss: 0.5681 - acc: 0.70 - ETA: 12s - loss: 0.5681 - acc: 0.70 - ETA: 12s - loss: 0.5679 - acc: 0.70 - ETA: 12s - loss: 0.5686 - acc: 0.70 - ETA: 12s - loss: 0.5685 - acc: 0.70 - ETA: 12s - loss: 0.5684 - acc: 0.70 - ETA: 12s - loss: 0.5682 - acc: 0.70 - ETA: 12s - loss: 0.5679 - acc: 0.70 - ETA: 12s - loss: 0.5681 - acc: 0.70 - ETA: 11s - loss: 0.5682 - acc: 0.70 - ETA: 11s - loss: 0.5682 - acc: 0.70 - ETA: 11s - loss: 0.5678 - acc: 0.70 - ETA: 11s - loss: 0.5676 - acc: 0.70 - ETA: 11s - loss: 0.5678 - acc: 0.70 - ETA: 11s - loss: 0.5676 - acc: 0.70 - ETA: 11s - loss: 0.5675 - acc: 0.70 - ETA: 11s - loss: 0.5675 - acc: 0.70 - ETA: 11s - loss: 0.5674 - acc: 0.70 - ETA: 11s - loss: 0.5669 - acc: 0.70 - ETA: 11s - loss: 0.5672 - acc: 0.70 - ETA: 11s - loss: 0.5673 - acc: 0.70 - ETA: 10s - loss: 0.5669 - acc: 0.70 - ETA: 10s - loss: 0.5672 - acc: 0.70 - ETA: 10s - loss: 0.5674 - acc: 0.70 - ETA: 10s - loss: 0.5674 - acc: 0.70 - ETA: 10s - loss: 0.5677 - acc: 0.70 - ETA: 10s - loss: 0.5676 - acc: 0.70 - ETA: 10s - loss: 0.5673 - acc: 0.70 - ETA: 10s - loss: 0.5670 - acc: 0.70 - ETA: 10s - loss: 0.5670 - acc: 0.70 - ETA: 10s - loss: 0.5669 - acc: 0.70 - ETA: 10s - loss: 0.5667 - acc: 0.70 - ETA: 10s - loss: 0.5667 - acc: 0.70 - ETA: 9s - loss: 0.5665 - acc: 0.7041 - ETA: 9s - loss: 0.5665 - acc: 0.704 - ETA: 9s - loss: 0.5663 - acc: 0.704 - ETA: 9s - loss: 0.5663 - acc: 0.704 - ETA: 9s - loss: 0.5664 - acc: 0.704 - ETA: 9s - loss: 0.5662 - acc: 0.704 - ETA: 9s - loss: 0.5661 - acc: 0.704 - ETA: 9s - loss: 0.5663 - acc: 0.704 - ETA: 9s - loss: 0.5657 - acc: 0.704 - ETA: 9s - loss: 0.5657 - acc: 0.705 - ETA: 9s - loss: 0.5660 - acc: 0.704 - ETA: 9s - loss: 0.5657 - acc: 0.704 - ETA: 9s - loss: 0.5657 - acc: 0.704 - ETA: 8s - loss: 0.5656 - acc: 0.704 - ETA: 8s - loss: 0.5659 - acc: 0.704 - ETA: 8s - loss: 0.5659 - acc: 0.704 - ETA: 8s - loss: 0.5658 - acc: 0.704 - ETA: 8s - loss: 0.5657 - acc: 0.704 - ETA: 8s - loss: 0.5656 - acc: 0.704 - ETA: 8s - loss: 0.5658 - acc: 0.704 - ETA: 8s - loss: 0.5661 - acc: 0.704 - ETA: 8s - loss: 0.5659 - acc: 0.704 - ETA: 8s - loss: 0.5655 - acc: 0.704 - ETA: 8s - loss: 0.5652 - acc: 0.704 - ETA: 8s - loss: 0.5653 - acc: 0.705 - ETA: 7s - loss: 0.5658 - acc: 0.704 - ETA: 7s - loss: 0.5658 - acc: 0.704 - ETA: 7s - loss: 0.5659 - acc: 0.7045"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31500/31500 [==============================] - ETA: 7s - loss: 0.5660 - acc: 0.704 - ETA: 7s - loss: 0.5658 - acc: 0.704 - ETA: 7s - loss: 0.5658 - acc: 0.704 - ETA: 7s - loss: 0.5657 - acc: 0.704 - ETA: 7s - loss: 0.5656 - acc: 0.704 - ETA: 7s - loss: 0.5656 - acc: 0.704 - ETA: 7s - loss: 0.5659 - acc: 0.704 - ETA: 7s - loss: 0.5656 - acc: 0.704 - ETA: 7s - loss: 0.5653 - acc: 0.704 - ETA: 7s - loss: 0.5652 - acc: 0.704 - ETA: 6s - loss: 0.5648 - acc: 0.705 - ETA: 6s - loss: 0.5650 - acc: 0.705 - ETA: 6s - loss: 0.5650 - acc: 0.705 - ETA: 6s - loss: 0.5651 - acc: 0.704 - ETA: 6s - loss: 0.5651 - acc: 0.705 - ETA: 6s - loss: 0.5652 - acc: 0.704 - ETA: 6s - loss: 0.5652 - acc: 0.704 - ETA: 6s - loss: 0.5651 - acc: 0.705 - ETA: 6s - loss: 0.5652 - acc: 0.704 - ETA: 6s - loss: 0.5651 - acc: 0.704 - ETA: 6s - loss: 0.5649 - acc: 0.704 - ETA: 6s - loss: 0.5648 - acc: 0.705 - ETA: 5s - loss: 0.5646 - acc: 0.705 - ETA: 5s - loss: 0.5644 - acc: 0.705 - ETA: 5s - loss: 0.5641 - acc: 0.706 - ETA: 5s - loss: 0.5638 - acc: 0.706 - ETA: 5s - loss: 0.5634 - acc: 0.706 - ETA: 5s - loss: 0.5633 - acc: 0.706 - ETA: 5s - loss: 0.5629 - acc: 0.706 - ETA: 5s - loss: 0.5628 - acc: 0.706 - ETA: 5s - loss: 0.5627 - acc: 0.707 - ETA: 5s - loss: 0.5624 - acc: 0.707 - ETA: 5s - loss: 0.5626 - acc: 0.707 - ETA: 5s - loss: 0.5625 - acc: 0.707 - ETA: 5s - loss: 0.5627 - acc: 0.707 - ETA: 4s - loss: 0.5625 - acc: 0.707 - ETA: 4s - loss: 0.5626 - acc: 0.707 - ETA: 4s - loss: 0.5629 - acc: 0.707 - ETA: 4s - loss: 0.5628 - acc: 0.707 - ETA: 4s - loss: 0.5628 - acc: 0.707 - ETA: 4s - loss: 0.5630 - acc: 0.707 - ETA: 4s - loss: 0.5628 - acc: 0.707 - ETA: 4s - loss: 0.5626 - acc: 0.707 - ETA: 4s - loss: 0.5627 - acc: 0.707 - ETA: 4s - loss: 0.5627 - acc: 0.707 - ETA: 4s - loss: 0.5626 - acc: 0.707 - ETA: 4s - loss: 0.5625 - acc: 0.707 - ETA: 4s - loss: 0.5626 - acc: 0.707 - ETA: 3s - loss: 0.5628 - acc: 0.707 - ETA: 3s - loss: 0.5627 - acc: 0.707 - ETA: 3s - loss: 0.5625 - acc: 0.707 - ETA: 3s - loss: 0.5624 - acc: 0.707 - ETA: 3s - loss: 0.5628 - acc: 0.707 - ETA: 3s - loss: 0.5629 - acc: 0.707 - ETA: 3s - loss: 0.5632 - acc: 0.706 - ETA: 3s - loss: 0.5632 - acc: 0.706 - ETA: 3s - loss: 0.5633 - acc: 0.706 - ETA: 3s - loss: 0.5633 - acc: 0.706 - ETA: 3s - loss: 0.5634 - acc: 0.706 - ETA: 3s - loss: 0.5634 - acc: 0.706 - ETA: 3s - loss: 0.5633 - acc: 0.707 - ETA: 2s - loss: 0.5633 - acc: 0.707 - ETA: 2s - loss: 0.5631 - acc: 0.707 - ETA: 2s - loss: 0.5631 - acc: 0.707 - ETA: 2s - loss: 0.5631 - acc: 0.707 - ETA: 2s - loss: 0.5631 - acc: 0.707 - ETA: 2s - loss: 0.5632 - acc: 0.707 - ETA: 2s - loss: 0.5632 - acc: 0.707 - ETA: 2s - loss: 0.5629 - acc: 0.707 - ETA: 2s - loss: 0.5627 - acc: 0.707 - ETA: 2s - loss: 0.5626 - acc: 0.707 - ETA: 2s - loss: 0.5628 - acc: 0.707 - ETA: 2s - loss: 0.5629 - acc: 0.707 - ETA: 2s - loss: 0.5632 - acc: 0.707 - ETA: 1s - loss: 0.5631 - acc: 0.707 - ETA: 1s - loss: 0.5629 - acc: 0.707 - ETA: 1s - loss: 0.5629 - acc: 0.707 - ETA: 1s - loss: 0.5628 - acc: 0.708 - ETA: 1s - loss: 0.5630 - acc: 0.707 - ETA: 1s - loss: 0.5630 - acc: 0.707 - ETA: 1s - loss: 0.5631 - acc: 0.707 - ETA: 1s - loss: 0.5628 - acc: 0.707 - ETA: 1s - loss: 0.5629 - acc: 0.707 - ETA: 1s - loss: 0.5629 - acc: 0.707 - ETA: 1s - loss: 0.5627 - acc: 0.707 - ETA: 1s - loss: 0.5628 - acc: 0.707 - ETA: 1s - loss: 0.5628 - acc: 0.707 - ETA: 0s - loss: 0.5626 - acc: 0.707 - ETA: 0s - loss: 0.5625 - acc: 0.707 - ETA: 0s - loss: 0.5626 - acc: 0.707 - ETA: 0s - loss: 0.5626 - acc: 0.707 - ETA: 0s - loss: 0.5628 - acc: 0.707 - ETA: 0s - loss: 0.5625 - acc: 0.707 - ETA: 0s - loss: 0.5624 - acc: 0.708 - ETA: 0s - loss: 0.5625 - acc: 0.707 - ETA: 0s - loss: 0.5625 - acc: 0.708 - ETA: 0s - loss: 0.5626 - acc: 0.708 - ETA: 0s - loss: 0.5622 - acc: 0.708 - ETA: 0s - loss: 0.5623 - acc: 0.708 - 25s 795us/step - loss: 0.5622 - acc: 0.7081 - val_loss: 0.5567 - val_acc: 0.7077\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21500/31500 [===================>..........] - ETA: 21s - loss: 0.5444 - acc: 0.74 - ETA: 21s - loss: 0.5575 - acc: 0.70 - ETA: 21s - loss: 0.5623 - acc: 0.70 - ETA: 21s - loss: 0.5584 - acc: 0.71 - ETA: 22s - loss: 0.5649 - acc: 0.69 - ETA: 22s - loss: 0.5727 - acc: 0.69 - ETA: 22s - loss: 0.5611 - acc: 0.70 - ETA: 22s - loss: 0.5574 - acc: 0.70 - ETA: 22s - loss: 0.5535 - acc: 0.71 - ETA: 22s - loss: 0.5507 - acc: 0.71 - ETA: 22s - loss: 0.5510 - acc: 0.71 - ETA: 22s - loss: 0.5507 - acc: 0.71 - ETA: 22s - loss: 0.5463 - acc: 0.72 - ETA: 22s - loss: 0.5492 - acc: 0.72 - ETA: 22s - loss: 0.5491 - acc: 0.72 - ETA: 22s - loss: 0.5455 - acc: 0.72 - ETA: 21s - loss: 0.5421 - acc: 0.72 - ETA: 21s - loss: 0.5416 - acc: 0.72 - ETA: 21s - loss: 0.5463 - acc: 0.72 - ETA: 21s - loss: 0.5398 - acc: 0.73 - ETA: 21s - loss: 0.5453 - acc: 0.73 - ETA: 21s - loss: 0.5436 - acc: 0.73 - ETA: 21s - loss: 0.5431 - acc: 0.73 - ETA: 21s - loss: 0.5429 - acc: 0.73 - ETA: 21s - loss: 0.5470 - acc: 0.72 - ETA: 21s - loss: 0.5490 - acc: 0.72 - ETA: 21s - loss: 0.5520 - acc: 0.72 - ETA: 21s - loss: 0.5540 - acc: 0.72 - ETA: 21s - loss: 0.5531 - acc: 0.72 - ETA: 21s - loss: 0.5542 - acc: 0.71 - ETA: 21s - loss: 0.5567 - acc: 0.71 - ETA: 21s - loss: 0.5573 - acc: 0.71 - ETA: 21s - loss: 0.5561 - acc: 0.71 - ETA: 21s - loss: 0.5585 - acc: 0.71 - ETA: 20s - loss: 0.5577 - acc: 0.71 - ETA: 20s - loss: 0.5574 - acc: 0.71 - ETA: 20s - loss: 0.5569 - acc: 0.71 - ETA: 20s - loss: 0.5578 - acc: 0.71 - ETA: 20s - loss: 0.5585 - acc: 0.71 - ETA: 20s - loss: 0.5599 - acc: 0.71 - ETA: 20s - loss: 0.5587 - acc: 0.71 - ETA: 20s - loss: 0.5603 - acc: 0.71 - ETA: 20s - loss: 0.5604 - acc: 0.71 - ETA: 20s - loss: 0.5598 - acc: 0.71 - ETA: 20s - loss: 0.5588 - acc: 0.71 - ETA: 20s - loss: 0.5589 - acc: 0.71 - ETA: 20s - loss: 0.5593 - acc: 0.71 - ETA: 20s - loss: 0.5590 - acc: 0.71 - ETA: 20s - loss: 0.5595 - acc: 0.71 - ETA: 20s - loss: 0.5584 - acc: 0.71 - ETA: 20s - loss: 0.5588 - acc: 0.71 - ETA: 20s - loss: 0.5597 - acc: 0.71 - ETA: 20s - loss: 0.5606 - acc: 0.71 - ETA: 20s - loss: 0.5611 - acc: 0.70 - ETA: 19s - loss: 0.5615 - acc: 0.70 - ETA: 19s - loss: 0.5614 - acc: 0.70 - ETA: 19s - loss: 0.5627 - acc: 0.70 - ETA: 19s - loss: 0.5633 - acc: 0.70 - ETA: 19s - loss: 0.5632 - acc: 0.70 - ETA: 19s - loss: 0.5647 - acc: 0.70 - ETA: 19s - loss: 0.5650 - acc: 0.70 - ETA: 19s - loss: 0.5659 - acc: 0.70 - ETA: 19s - loss: 0.5657 - acc: 0.70 - ETA: 19s - loss: 0.5655 - acc: 0.70 - ETA: 19s - loss: 0.5654 - acc: 0.70 - ETA: 19s - loss: 0.5642 - acc: 0.70 - ETA: 19s - loss: 0.5638 - acc: 0.70 - ETA: 19s - loss: 0.5635 - acc: 0.70 - ETA: 19s - loss: 0.5639 - acc: 0.70 - ETA: 19s - loss: 0.5631 - acc: 0.70 - ETA: 18s - loss: 0.5626 - acc: 0.70 - ETA: 18s - loss: 0.5622 - acc: 0.70 - ETA: 18s - loss: 0.5623 - acc: 0.70 - ETA: 18s - loss: 0.5626 - acc: 0.70 - ETA: 18s - loss: 0.5635 - acc: 0.70 - ETA: 18s - loss: 0.5629 - acc: 0.70 - ETA: 18s - loss: 0.5627 - acc: 0.70 - ETA: 18s - loss: 0.5618 - acc: 0.70 - ETA: 18s - loss: 0.5610 - acc: 0.70 - ETA: 18s - loss: 0.5605 - acc: 0.70 - ETA: 18s - loss: 0.5606 - acc: 0.70 - ETA: 18s - loss: 0.5606 - acc: 0.70 - ETA: 18s - loss: 0.5598 - acc: 0.70 - ETA: 17s - loss: 0.5597 - acc: 0.70 - ETA: 17s - loss: 0.5594 - acc: 0.70 - ETA: 17s - loss: 0.5593 - acc: 0.71 - ETA: 17s - loss: 0.5587 - acc: 0.71 - ETA: 17s - loss: 0.5586 - acc: 0.71 - ETA: 17s - loss: 0.5584 - acc: 0.71 - ETA: 17s - loss: 0.5586 - acc: 0.71 - ETA: 17s - loss: 0.5589 - acc: 0.71 - ETA: 17s - loss: 0.5591 - acc: 0.71 - ETA: 17s - loss: 0.5596 - acc: 0.71 - ETA: 17s - loss: 0.5602 - acc: 0.71 - ETA: 17s - loss: 0.5601 - acc: 0.71 - ETA: 16s - loss: 0.5597 - acc: 0.71 - ETA: 16s - loss: 0.5594 - acc: 0.71 - ETA: 16s - loss: 0.5596 - acc: 0.71 - ETA: 16s - loss: 0.5596 - acc: 0.71 - ETA: 16s - loss: 0.5597 - acc: 0.71 - ETA: 16s - loss: 0.5597 - acc: 0.71 - ETA: 16s - loss: 0.5600 - acc: 0.71 - ETA: 16s - loss: 0.5610 - acc: 0.71 - ETA: 16s - loss: 0.5617 - acc: 0.71 - ETA: 16s - loss: 0.5614 - acc: 0.71 - ETA: 16s - loss: 0.5621 - acc: 0.70 - ETA: 16s - loss: 0.5618 - acc: 0.70 - ETA: 16s - loss: 0.5621 - acc: 0.71 - ETA: 16s - loss: 0.5625 - acc: 0.70 - ETA: 16s - loss: 0.5624 - acc: 0.71 - ETA: 15s - loss: 0.5624 - acc: 0.71 - ETA: 15s - loss: 0.5622 - acc: 0.71 - ETA: 15s - loss: 0.5626 - acc: 0.70 - ETA: 15s - loss: 0.5638 - acc: 0.70 - ETA: 15s - loss: 0.5641 - acc: 0.70 - ETA: 15s - loss: 0.5634 - acc: 0.70 - ETA: 15s - loss: 0.5633 - acc: 0.70 - ETA: 15s - loss: 0.5630 - acc: 0.70 - ETA: 15s - loss: 0.5627 - acc: 0.70 - ETA: 15s - loss: 0.5640 - acc: 0.70 - ETA: 15s - loss: 0.5631 - acc: 0.70 - ETA: 15s - loss: 0.5631 - acc: 0.70 - ETA: 14s - loss: 0.5628 - acc: 0.70 - ETA: 14s - loss: 0.5628 - acc: 0.70 - ETA: 14s - loss: 0.5621 - acc: 0.71 - ETA: 14s - loss: 0.5622 - acc: 0.71 - ETA: 14s - loss: 0.5630 - acc: 0.70 - ETA: 14s - loss: 0.5625 - acc: 0.71 - ETA: 14s - loss: 0.5620 - acc: 0.71 - ETA: 14s - loss: 0.5615 - acc: 0.71 - ETA: 14s - loss: 0.5616 - acc: 0.71 - ETA: 14s - loss: 0.5615 - acc: 0.71 - ETA: 14s - loss: 0.5613 - acc: 0.71 - ETA: 14s - loss: 0.5621 - acc: 0.71 - ETA: 14s - loss: 0.5623 - acc: 0.70 - ETA: 13s - loss: 0.5624 - acc: 0.70 - ETA: 13s - loss: 0.5626 - acc: 0.70 - ETA: 13s - loss: 0.5626 - acc: 0.70 - ETA: 13s - loss: 0.5625 - acc: 0.70 - ETA: 13s - loss: 0.5632 - acc: 0.70 - ETA: 13s - loss: 0.5633 - acc: 0.70 - ETA: 13s - loss: 0.5630 - acc: 0.70 - ETA: 13s - loss: 0.5630 - acc: 0.70 - ETA: 13s - loss: 0.5628 - acc: 0.70 - ETA: 13s - loss: 0.5632 - acc: 0.70 - ETA: 13s - loss: 0.5633 - acc: 0.70 - ETA: 13s - loss: 0.5631 - acc: 0.70 - ETA: 13s - loss: 0.5635 - acc: 0.70 - ETA: 13s - loss: 0.5634 - acc: 0.70 - ETA: 12s - loss: 0.5633 - acc: 0.70 - ETA: 12s - loss: 0.5627 - acc: 0.71 - ETA: 12s - loss: 0.5628 - acc: 0.70 - ETA: 12s - loss: 0.5625 - acc: 0.71 - ETA: 12s - loss: 0.5628 - acc: 0.70 - ETA: 12s - loss: 0.5628 - acc: 0.70 - ETA: 12s - loss: 0.5632 - acc: 0.70 - ETA: 12s - loss: 0.5630 - acc: 0.70 - ETA: 12s - loss: 0.5637 - acc: 0.70 - ETA: 12s - loss: 0.5638 - acc: 0.70 - ETA: 12s - loss: 0.5638 - acc: 0.70 - ETA: 12s - loss: 0.5637 - acc: 0.70 - ETA: 12s - loss: 0.5633 - acc: 0.70 - ETA: 11s - loss: 0.5636 - acc: 0.70 - ETA: 11s - loss: 0.5634 - acc: 0.70 - ETA: 11s - loss: 0.5636 - acc: 0.70 - ETA: 11s - loss: 0.5635 - acc: 0.70 - ETA: 11s - loss: 0.5636 - acc: 0.70 - ETA: 11s - loss: 0.5638 - acc: 0.70 - ETA: 11s - loss: 0.5639 - acc: 0.70 - ETA: 11s - loss: 0.5637 - acc: 0.70 - ETA: 11s - loss: 0.5640 - acc: 0.70 - ETA: 11s - loss: 0.5640 - acc: 0.70 - ETA: 11s - loss: 0.5644 - acc: 0.70 - ETA: 11s - loss: 0.5646 - acc: 0.70 - ETA: 11s - loss: 0.5641 - acc: 0.70 - ETA: 11s - loss: 0.5637 - acc: 0.70 - ETA: 10s - loss: 0.5634 - acc: 0.70 - ETA: 10s - loss: 0.5632 - acc: 0.70 - ETA: 10s - loss: 0.5634 - acc: 0.70 - ETA: 10s - loss: 0.5635 - acc: 0.70 - ETA: 10s - loss: 0.5633 - acc: 0.70 - ETA: 10s - loss: 0.5629 - acc: 0.70 - ETA: 10s - loss: 0.5630 - acc: 0.71 - ETA: 10s - loss: 0.5632 - acc: 0.70 - ETA: 10s - loss: 0.5633 - acc: 0.70 - ETA: 10s - loss: 0.5636 - acc: 0.70 - ETA: 10s - loss: 0.5631 - acc: 0.71 - ETA: 10s - loss: 0.5631 - acc: 0.71 - ETA: 10s - loss: 0.5635 - acc: 0.70 - ETA: 9s - loss: 0.5632 - acc: 0.7097 - ETA: 9s - loss: 0.5637 - acc: 0.709 - ETA: 9s - loss: 0.5641 - acc: 0.708 - ETA: 9s - loss: 0.5643 - acc: 0.708 - ETA: 9s - loss: 0.5642 - acc: 0.708 - ETA: 9s - loss: 0.5642 - acc: 0.708 - ETA: 9s - loss: 0.5641 - acc: 0.708 - ETA: 9s - loss: 0.5640 - acc: 0.708 - ETA: 9s - loss: 0.5644 - acc: 0.708 - ETA: 9s - loss: 0.5646 - acc: 0.708 - ETA: 9s - loss: 0.5645 - acc: 0.708 - ETA: 9s - loss: 0.5647 - acc: 0.708 - ETA: 8s - loss: 0.5645 - acc: 0.708 - ETA: 8s - loss: 0.5647 - acc: 0.708 - ETA: 8s - loss: 0.5648 - acc: 0.708 - ETA: 8s - loss: 0.5645 - acc: 0.708 - ETA: 8s - loss: 0.5645 - acc: 0.708 - ETA: 8s - loss: 0.5647 - acc: 0.708 - ETA: 8s - loss: 0.5644 - acc: 0.708 - ETA: 8s - loss: 0.5642 - acc: 0.708 - ETA: 8s - loss: 0.5641 - acc: 0.708 - ETA: 8s - loss: 0.5638 - acc: 0.709 - ETA: 8s - loss: 0.5635 - acc: 0.709 - ETA: 8s - loss: 0.5638 - acc: 0.709 - ETA: 7s - loss: 0.5636 - acc: 0.709 - ETA: 7s - loss: 0.5639 - acc: 0.7092"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31500/31500 [==============================] - ETA: 7s - loss: 0.5641 - acc: 0.709 - ETA: 7s - loss: 0.5641 - acc: 0.709 - ETA: 7s - loss: 0.5642 - acc: 0.709 - ETA: 7s - loss: 0.5641 - acc: 0.709 - ETA: 7s - loss: 0.5643 - acc: 0.709 - ETA: 7s - loss: 0.5642 - acc: 0.709 - ETA: 7s - loss: 0.5640 - acc: 0.709 - ETA: 7s - loss: 0.5643 - acc: 0.708 - ETA: 7s - loss: 0.5644 - acc: 0.708 - ETA: 7s - loss: 0.5644 - acc: 0.708 - ETA: 7s - loss: 0.5640 - acc: 0.708 - ETA: 6s - loss: 0.5639 - acc: 0.708 - ETA: 6s - loss: 0.5641 - acc: 0.708 - ETA: 6s - loss: 0.5644 - acc: 0.708 - ETA: 6s - loss: 0.5644 - acc: 0.708 - ETA: 6s - loss: 0.5645 - acc: 0.708 - ETA: 6s - loss: 0.5646 - acc: 0.708 - ETA: 6s - loss: 0.5648 - acc: 0.708 - ETA: 6s - loss: 0.5647 - acc: 0.708 - ETA: 6s - loss: 0.5644 - acc: 0.709 - ETA: 6s - loss: 0.5642 - acc: 0.709 - ETA: 6s - loss: 0.5641 - acc: 0.709 - ETA: 6s - loss: 0.5640 - acc: 0.709 - ETA: 6s - loss: 0.5638 - acc: 0.709 - ETA: 5s - loss: 0.5637 - acc: 0.709 - ETA: 5s - loss: 0.5636 - acc: 0.709 - ETA: 5s - loss: 0.5634 - acc: 0.709 - ETA: 5s - loss: 0.5635 - acc: 0.709 - ETA: 5s - loss: 0.5635 - acc: 0.709 - ETA: 5s - loss: 0.5636 - acc: 0.709 - ETA: 5s - loss: 0.5635 - acc: 0.709 - ETA: 5s - loss: 0.5635 - acc: 0.709 - ETA: 5s - loss: 0.5637 - acc: 0.709 - ETA: 5s - loss: 0.5638 - acc: 0.709 - ETA: 5s - loss: 0.5636 - acc: 0.709 - ETA: 5s - loss: 0.5636 - acc: 0.709 - ETA: 5s - loss: 0.5632 - acc: 0.709 - ETA: 4s - loss: 0.5633 - acc: 0.709 - ETA: 4s - loss: 0.5634 - acc: 0.709 - ETA: 4s - loss: 0.5636 - acc: 0.709 - ETA: 4s - loss: 0.5638 - acc: 0.709 - ETA: 4s - loss: 0.5637 - acc: 0.709 - ETA: 4s - loss: 0.5640 - acc: 0.709 - ETA: 4s - loss: 0.5639 - acc: 0.709 - ETA: 4s - loss: 0.5638 - acc: 0.708 - ETA: 4s - loss: 0.5637 - acc: 0.709 - ETA: 4s - loss: 0.5638 - acc: 0.709 - ETA: 4s - loss: 0.5634 - acc: 0.709 - ETA: 4s - loss: 0.5632 - acc: 0.709 - ETA: 3s - loss: 0.5631 - acc: 0.709 - ETA: 3s - loss: 0.5631 - acc: 0.709 - ETA: 3s - loss: 0.5632 - acc: 0.709 - ETA: 3s - loss: 0.5636 - acc: 0.709 - ETA: 3s - loss: 0.5637 - acc: 0.709 - ETA: 3s - loss: 0.5633 - acc: 0.709 - ETA: 3s - loss: 0.5631 - acc: 0.710 - ETA: 3s - loss: 0.5635 - acc: 0.709 - ETA: 3s - loss: 0.5635 - acc: 0.709 - ETA: 3s - loss: 0.5633 - acc: 0.709 - ETA: 3s - loss: 0.5633 - acc: 0.709 - ETA: 3s - loss: 0.5630 - acc: 0.709 - ETA: 3s - loss: 0.5629 - acc: 0.709 - ETA: 2s - loss: 0.5630 - acc: 0.709 - ETA: 2s - loss: 0.5627 - acc: 0.710 - ETA: 2s - loss: 0.5627 - acc: 0.710 - ETA: 2s - loss: 0.5627 - acc: 0.710 - ETA: 2s - loss: 0.5624 - acc: 0.710 - ETA: 2s - loss: 0.5626 - acc: 0.710 - ETA: 2s - loss: 0.5623 - acc: 0.710 - ETA: 2s - loss: 0.5626 - acc: 0.710 - ETA: 2s - loss: 0.5627 - acc: 0.710 - ETA: 2s - loss: 0.5626 - acc: 0.710 - ETA: 2s - loss: 0.5625 - acc: 0.710 - ETA: 2s - loss: 0.5626 - acc: 0.710 - ETA: 1s - loss: 0.5626 - acc: 0.710 - ETA: 1s - loss: 0.5626 - acc: 0.710 - ETA: 1s - loss: 0.5630 - acc: 0.709 - ETA: 1s - loss: 0.5631 - acc: 0.709 - ETA: 1s - loss: 0.5630 - acc: 0.710 - ETA: 1s - loss: 0.5632 - acc: 0.710 - ETA: 1s - loss: 0.5632 - acc: 0.710 - ETA: 1s - loss: 0.5631 - acc: 0.710 - ETA: 1s - loss: 0.5633 - acc: 0.709 - ETA: 1s - loss: 0.5634 - acc: 0.709 - ETA: 1s - loss: 0.5633 - acc: 0.710 - ETA: 1s - loss: 0.5634 - acc: 0.710 - ETA: 1s - loss: 0.5636 - acc: 0.709 - ETA: 0s - loss: 0.5637 - acc: 0.709 - ETA: 0s - loss: 0.5637 - acc: 0.709 - ETA: 0s - loss: 0.5635 - acc: 0.709 - ETA: 0s - loss: 0.5636 - acc: 0.709 - ETA: 0s - loss: 0.5637 - acc: 0.709 - ETA: 0s - loss: 0.5636 - acc: 0.709 - ETA: 0s - loss: 0.5631 - acc: 0.709 - ETA: 0s - loss: 0.5632 - acc: 0.709 - ETA: 0s - loss: 0.5632 - acc: 0.709 - ETA: 0s - loss: 0.5631 - acc: 0.709 - ETA: 0s - loss: 0.5630 - acc: 0.709 - ETA: 0s - loss: 0.5632 - acc: 0.709 - 26s 812us/step - loss: 0.5631 - acc: 0.7094 - val_loss: 0.5458 - val_acc: 0.7191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14b9baf3dd8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train DNN model on GloVe training features\n",
    "batch_size = 100\n",
    "glove_dnn.fit(train_glove_features, y_train, epochs=5, batch_size=batch_size, \n",
    "              shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RUIZ\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# get predictions on test reviews\n",
    "y_pred = glove_dnn.predict_classes(test_glove_features)\n",
    "predictions = le.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7284\n",
      "Precision: 0.7309\n",
      "Recall: 0.7284\n",
      "F1 Score: 0.7276\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.71      0.78      0.74      7510\n",
      "   negative       0.75      0.68      0.71      7490\n",
      "\n",
      "avg / total       0.73      0.73      0.73     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       5866     1644\n",
      "        negative       2430     5060\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n",
    "                                      classes=['positive', 'negative'])\n",
    "\n",
    "#We obtained an overall model accuracy and F1-score of 85% with the GloVe features, which is still good\n",
    "#but not better than what we obtained using our word2vec features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
